---
title: 'The main risks from advanced AI'
date: 2025-03-16
tags: ["ai-alignment"]
---

As with any new technology, advanced AI entails certain risks. While we're investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.

The other week, I had an insightful discussion with people in [ZÃ¼rich AI Alignment (ZAIA)](https://www.zurich-ai-alignment.com) about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this essay. Here are what I currently consider to be the most relevant risks:

- **Powerful technology in bad hands**: First, there are the risks which all fall under the label "bad guys using powerful technology to further their own interests". For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I'd guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.
- **Concentration of power**: AI is a transformative technology - a message that shouldn't have escaped anyone, given the current AI hype. In the years to come, it's likely to have a profound impact on things like our economy and healthcare system[^1]. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren't democratically elected.
- **Misaligned AI models**: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren't aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we'll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We've already seen plenty of examples of this. For example, as pointed out during the ZAIA discussion, it's still quite easy jailbreaking LLMs like ChatGPT[^2]. Moreover, there are striking examples of algorithms encoding our own biases[^3].
- **Power-seeking AI**: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I'll briefly outline what I understand to be the core argument.
    1. In the future, we might see the emergence of power-seeking AI models. AI models are trained to minimise some loss function, and power could be instrumental in achieving this.
    2. At the same time, we might develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities[^5]. Such AI agents could replace a greater portion of the workforce, and would therefore be very profitable.
    3. All in all, it's possible we'll develop power-seeking, agentic AI models. The key point now is that such an AI might view humans as obstacles to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly:  "You can't fetch the coffee if you're dead". So, the argument goes, the AI might try to disempower humanity.
- While points (1) and (2) seem quite reasonable, I'm more sceptical about point (3). A more conservative version of point (3) would be that power-seeking agentic AI models can do a lot of harm, despite not being mistrustful of all humans. I think the key word here is "agency". History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very "agentic". Endowed with AI-like capabilities, these men would probably have caused much greater damage. In light of this, I'd lower-bound the risk of existential threat from AI by 1%. It seems like the stakes are too high for us to dismiss this kind of threat.

Of course, there's much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate, or at least mitigate, some of these risks. See e.g. [this](https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary) article for a more exhaustive survey of the risks from AI, along with suggested measures. I've also left links to further resources in the footnotes.

[^1]: [6 ways AI is transforming healthcare | World Economic Forum](https://www.weforum.org/stories/2025/03/ai-transforming-global-health/)
[^2]: [ChatGPT Jailbreak - Computerphile - YouTube](https://www.youtube.com/watch?v=zn2ukSnDqSg)
[^3]: [AI Bias Examples | IBM](https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples)
[^5]:[Agentic AI vs. Generative AI | IBM](https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai)
