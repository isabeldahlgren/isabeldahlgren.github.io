---
title: "AI companies are companies"
date: 2025-08-31
tags: ["ai"]
draft: false
---

AI companies are companies. The leading AI companies don't want to be seen as companies, though. They call themselves AI labs. They have researchy names, like DeepMind and Meta AI. The best name: OpenAI. Almost sounds like a non-profit.

Most AI companies are products of Silicon Valley, so they're heavily influenced by start-up culture. Their leaders aren't professors, but seasoned business executives. Moreover, unlike normal research labs, AI companies need to make money.

To justify their growth, AI companies need to tell narratives about themselves. And they are pretty good at it -- these companies attract top talent, so their marketing teams can basically hire the world's best story-tellers. I'll list five narratives being aware of.

Before we begin, an apology. I'm deliberately exaggerating these narratives, just to make them clearer[^extremal]. Concretely, I'll be cherry-picking quotes from company leaders and generalise, like a lot. Overall, I think these companies bring huge consumer value and have the potential to radically improve the future.

Now, story time.

### "We'll build AGI soon"
AI company CEOs have famously [short AI timelines](https://80000hours.org/agi/guide/when-will-agi-arrive/). For example, in January this year, Sam Altman declared that "We are now confident we know how to build AGI as we have traditionally understood it." Dario Amodei, in the same month: "I’m more confident than I’ve ever been that we’re close to powerful capabilities… in the next 2-3 years".

Of course, no one would invest in your company if you needed two decades to develop your product.

But the term AGI and is fuzzy[^sam-dario], and it keeps on changing as models evolve. Funny because it's kinda true: "AGI is whatever machines can't do now"[^snake-oil].

Bottom line: these statements about future model capabilities end up being pretty uninformative.

### "AGI is inevitable"
The AI companies also want technological progress to seem inevitable. If you cannot guarantee AGI, why would you fund them?

But this narrative isn't just for outsiders. Ilya Sutskever, former chief scientist at OpenAI, used to "Feel the AGI" at company parties, presumably to build a better company culture[^cringe].

Similarly, if AGI is inevitable, an AI company can justify advancing model capabilities as quickly as possible -- possibly at the cost of safety -- so that none of the bad guys build it first.

### "We're the good guys"
Indeed, as highlighted in [Empire of AI](https://en.wikipedia.org/wiki/Empire_of_AI), some AI companies seem to think of themselves as "the good guys" and other AI companies as "the bad guys". For example, OpenAI was founded was because Musk was concerned about the ethical implications of Google's acquisition of DeepMind. Anthropic was founded by former OpenAI employees who were dissatisfied with the safety work at OpenAI.

Moreover, the US-China tensions lead to a divide between the American AI companies and the Chinese ones.

This narrative provides a powerful argument for minimal AI regulations.

### "Our tools empower all of humanity"
DeepMind's mission is to "build AI responsibly to benefit humanity", while OpenAI's mission is to "ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity".

The democratisation bit of the mission isn't specific to AI companies; it's a tech company cliché. Remember Facebook was supposed to connect you with friends and the rest of the world, all for free.

AI will empower all of humanity, but only if everyone can afford a laptop, has stable internet connection and knows how to use LLMs efficiently. Moreover, prices for premium subscription plans are relatively high. OpenAI's cheapest paid plan comes at 20 dollars per month. For Anthropic, the figure is 17 dollars.

The point about using LLMs efficiently is subtle. To have an LLM perform more advanced tasks, which is what would be empowering for real, you'd need to be good at prompt engineering. Becoming good at prompt engineer takes time.

### "AGI will solve all humanity's problems"
Another reason they need to build AGI as soon as possible is their belief that AGI can solve most of humanity's problems. In short, techno-optimism. In [Genius Makers](https://www.penguin.co.uk/books/437020/genius-makers-by-metz-cade/9781847942159), Cade Metz likens the belief in AGI to a religion. And a religion needs a promise of salvation.

In [Machines of Loving Grace](https://www.darioamodei.com/essay/machines-of-loving-grace), Dario Amodei writes that "it’s my guess that powerful AI could at least 10x the rate of these discoveries, giving us the next 50-100 years of biological progress in 5-10 years".

The idea of AGI radically accelerating scientific progress is pretty mainstream. For example, [this report](https://www.forethought.org/research/preparing-for-the-intelligence-explosion#the-accelerated-decade) from Forethought argues that we might very well see a century's worth of technological progress in a decade.

But human progress isn't the same as scientific progress. We need cultural and moral progress too.

### The end
Whom are these stories meant for?

In short, for everyone. Some stories serve as sales pitches for investors and customers. Others are meant to improve employee morale and create a sense of unity. Regulators learn that they shouldn't regulate the deployment of new AI models, just so their nation won't fall behind in the AI arms race.

We now find ourselves in a technological race, much like the Apollo program. If you build AGI first, you'll become Neil Armstrong. You don't want to be -- I had to Google his name, and I invite you to do the same -- Buzz Aldrin. However, this doesn't seem like an race between nations, but more like a race between tech CEOs who don't "just" want to be average tech CEOs. Perhaps these stories are mostly meant for posterity.

[^extremal]: It's a bit like using the extremal principle from mathematics.
[^sam-dario]: Everyone seems to agree the term "AGI" is problematic, even the AI company CEOs. See e.g. [here](https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework).
[^snake-oil]: As highlighted in *AI Snake Oil*, we used to call spell checkers and image-generating AIs "AGI" before we had them.
[^cringe]: As an outsider, I cannot help but think that this damages company culture, though.
