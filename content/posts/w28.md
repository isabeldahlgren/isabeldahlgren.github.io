---
title: "From the AI company storybook"
date: 2025-08-31
tags: ["ai"]
draft: false
---

AI companies are companies. The leading AI companies don't want to be seen as companies, though. They call themselves AI labs. They have researchy names, like DeepMind and Meta AI. The best name: OpenAI. Almost sounds like a real [non-profit](https://openai.com/index/introducing-openai/).

Most AI companies are products of Silicon Valley. Their leaders aren't professors, but seasoned business executives. And this is a good thing. These companies wouldn't produce nearly as much consumer value if they were led by researchers with no industry experience. Moreover, unlike normal research labs, AI companies need to make money, just as any other company.

All companies tell narratives about themselves. And the leading AI companies are pretty good at it -- these companies attract top talent, so their marketing teams can basically hire the world's best story-tellers. Here are five narratives being aware of.

Before we begin, an apology. I'm deliberately exaggerating these narratives, just to make them clearer[^extremal]. Concretely, I'll be cherry-picking quotes from company leaders and generalise, like a lot. Overall, I think these companies have the potential to radically improve the future. But the focus of this article is on the problems of these narratives.

Now, story time.

### "We'll build AGI soon"
AI company CEOs have famously [short AI timelines](https://80000hours.org/agi/guide/when-will-agi-arrive/). For example, in January this year, Sam Altman declared that "We are now confident we know how to build AGI as we have traditionally understood it." Dario Amodei, in the same month: "I’m more confident than I’ve ever been that we’re close to powerful capabilities… in the next 2-3 years".

Of course, no one would invest in your company if you needed two decades to develop your product.

But the term AGI and is fuzzy[^sam-dario], and it keeps on changing as models evolve. Funny because it's kinda true: "AGI is whatever machines can't do now". In the end, these kinds of statements about future model capabilities become pretty uninformative.

### "AGI is inevitable"
The AI companies also want technological progress to seem inevitable. If you cannot guarantee AGI, why would you fund them?

But this narrative isn't just for outsiders. Ilya Sutskever, former chief scientist at OpenAI, used to chant "Feel the AGI" at company parties. Was this supposed to improve company culture[^cringe]?

Similarly, if AGI is inevitable, an AI company can justify advancing model capabilities as quickly as possible -- possibly at the cost of safety -- so that none of the bad guys build it first.

### "We're the good guys"
Indeed, as highlighted in [Empire of AI](https://en.wikipedia.org/wiki/Empire_of_AI), some AI companies seem to think of themselves as "the good guys" and other AI companies as "the bad guys". For example, OpenAI was founded was because Musk was concerned about the ethical implications of Google's acquisition of DeepMind.

Moreover, the US-China tensions lead to a divide between the American AI companies and the Chinese ones.

### "Our tools empower all of humanity"
DeepMind's mission is to "build AI responsibly to benefit humanity", while OpenAI's mission is to "ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity".

The democratisation bit of the mission isn't specific to AI companies; it's a tech company cliché. For example, Facebook is supposed to give everyone the opportunity to connect you with friends and the rest of the world.

AI will empower all of humanity, but only if everyone can afford a laptop, has stable internet connection and knows how to use LLMs efficiently. Moreover, prices for premium subscription plans are relatively high. OpenAI's cheapest paid plan comes at 20 dollars per month. For Anthropic, the figure is 17 dollars. There's an economic reality too.

The point about using LLMs efficiently is subtle. To have an LLM perform more advanced tasks, which is what would be empowering for real, you'd need to be good at prompt engineering. Becoming good at prompt engineer takes time.

### "AGI will solve all our problems"
Another reason some companies need to build AGI as soon as possible is their belief that AGI can solve most of humanity's problems. In short, techno-optimism. In [Genius Makers](https://www.penguin.co.uk/books/437020/genius-makers-by-metz-cade/9781847942159), Cade Metz likens the belief in AGI to a religion. And a religion needs a promise of salvation.

To be fair, I'm also quite excited about the idea of AGI accelerating scientific progress. For example, [this report](https://www.forethought.org/research/preparing-for-the-intelligence-explosion#the-accelerated-decade) from Forethought argues that we might very well see a century's worth of technological progress in a decade. However, we also need to reduce the risks from [transformative AI](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/#id-1-defining-transformative-artificial-intelligence-transformative-ai). AGI might solve all our problems, but only if we can overcome a set of challenges.

### The end
Whom are these stories meant for?

In short, for everyone. Some stories serve as sales pitches for investors and customers. Others are meant to improve employee morale and create a sense of unity. Regulators learn that they shouldn't regulate the deployment of new AI models, just so their nation won't fall behind in the AI arms race. These stories are also meant for the future. The people who help build benign AGI want to be remembered as pioneers.

Companies like telling stories. As do individuals. Most people in AI safety, including myself, like telling themselves that they might help AI "go well" somehow. It's just worth being aware of what stories we tell ourselves.

[^extremal]: It's a bit like using the extremal principle from mathematics.
[^sam-dario]: Everyone seems to agree the term "AGI" is problematic, even the AI company CEOs. See e.g. [here](https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework).
[^cringe]: As an outsider, I cannot help but think that this damages company culture, though.
