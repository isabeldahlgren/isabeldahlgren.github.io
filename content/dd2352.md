---
title: 'DD2352'
date: '2023-06-07'
draft: false
---

These notes are based on the [slides](https://canvas.kth.se/courses/44839) of P. Austrin. All errors are mine.

<!-- mtoc-start -->

* [Lecture 1](#lecture-1)
  * [Examples of algorithms](#examples-of-algorithms)
    * [Time complexity](#time-complexity)
    * [Finding the time complexity](#finding-the-time-complexity)
    * [Graph algorithms](#graph-algorithms)
  * [Lecture 2](#lecture-2)
    * [Prelude](#prelude)
    * [Heuristics for greedy algorithms](#heuristics-for-greedy-algorithms)
    * [Interval scheduling](#interval-scheduling)
    * [Job scheduling with minimum lateness](#job-scheduling-with-minimum-lateness)
    * [Greedy graph algorithm](#greedy-graph-algorithm)
  * [Lecture 3](#lecture-3)
    * [Prelude](#prelude-1)
    * [Merge sort](#merge-sort)
    * [Polynomial multiplication](#polynomial-multiplication)
    * [Bit and unit cost](#bit-and-unit-cost)
    * [Karatsuba's algorithm](#karatsubas-algorithm)
    * [The Fast Fourier Transform](#the-fast-fourier-transform)
    * [The Master theorem](#the-master-theorem)
  * [Lecture 4](#lecture-4)
    * [Prelude](#prelude-2)
    * [Fibonacci](#fibonacci)
    * [Weighted interval scheduling](#weighted-interval-scheduling)
    * [Knapsack](#knapsack)
    * [Top-down or bottom-up](#top-down-or-bottom-up)
    * [Heuristics for dynamic programming](#heuristics-for-dynamic-programming)
  * [Lecture 5](#lecture-5)
    * [Heuristics for dynamic programming, continued](#heuristics-for-dynamic-programming-continued)
    * [Constructing solutions](#constructing-solutions)
    * [Sequence alignment](#sequence-alignment)
    * [Matrix chain multiplication](#matrix-chain-multiplication)
    * [Implementing bottom-up](#implementing-bottom-up)
  * [Lecture 6](#lecture-6)
    * [Flow networks](#flow-networks)
    * [Maximum flows](#maximum-flows)
    * [Edmonds-Karp](#edmonds-karp)
    * [Minimum cuts](#minimum-cuts)
    * [The Max Flow Min Cut theorem](#the-max-flow-min-cut-theorem)
  * [Lecture 7](#lecture-7)
    * [Prelude](#prelude-3)
    * [Vertex Cut](#vertex-cut)
    * [Many sources or sinks](#many-sources-or-sinks)
    * [Matching numbers](#matching-numbers)
    * [Disjoint paths](#disjoint-paths)
    * [Closure](#closure)
  * [Lecture 8](#lecture-8)
    * [Randomised algorithms](#randomised-algorithms)
    * [Random number generators](#random-number-generators)
    * [Approximation algorithms](#approximation-algorithms)
    * [Global Minimum Cut](#global-minimum-cut)
    * [Global Maximum Cut](#global-maximum-cut)
  * [Lecture 9](#lecture-9)
    * [Prelude](#prelude-4)
    * [Turing machines](#turing-machines)
    * [RAM](#ram)
    * [Models of computation](#models-of-computation)
    * [Restricted models of computation](#restricted-models-of-computation)
    * [Comparison-based sorting](#comparison-based-sorting)
  * [Lecture 10](#lecture-10)
    * [Prelude](#prelude-5)
    * [Heuristics for transforming problems](#heuristics-for-transforming-problems)
    * [Halting](#halting)
    * [Undecidable problems](#undecidable-problems)
    * [Turing reductions](#turing-reductions)
    * [Examples of reductions](#examples-of-reductions)
    * [3-SAT](#3-sat)
    * [Karp reductions](#karp-reductions)
  * [Lecture 11](#lecture-11)
    * [Prelude](#prelude-6)
    * [Problems as languages](#problems-as-languages)
    * [P](#p)
    * [NP](#np)
    * [P vs. NP](#p-vs-np)
    * [NP-completeness](#np-completeness)
    * [NP-complete problems](#np-complete-problems)
  * [Lecture 12](#lecture-12)
    * [Prelude](#prelude-7)
    * [Hamiltonian Cycle](#hamiltonian-cycle)
    * [Travelling salesman](#travelling-salesman)
    * [Graph colourings](#graph-colourings)
    * [Subset sum](#subset-sum)
    * [Knapsack](#knapsack-1)
    * [Summary NP-completeness](#summary-np-completeness)
    * [Understanding the definition of NP](#understanding-the-definition-of-np)
    * [Proof sketch of Cook-Levin](#proof-sketch-of-cook-levin)
  * [Lecture 13](#lecture-13)
    * [CoNP](#conp)
    * [CoNP-hardness](#conp-hardness)
    * [P, NP & CoNP](#p-np--conp)
    * [PSPACE](#pspace)
    * [PSPACE-hardness](#pspace-hardness)
    * [Venn diagram](#venn-diagram)
    * [BPP & ZPP](#bpp--zpp)

<!-- mtoc-end -->

### Lecture 1
2024-01-16

#### Examples of algorithms
- **Algo.** (Insertion sort). Insert elements into their correct position.
- **Algo.** (Merge sort). Divide into halves, sort, merge.

#### Time complexity
- **Q.** How do we measure the efficiency of an algorithm?
- **Def.** $O(g(n))$
    - There exist $c$ and $N$ s.t. $f(n) \le c g(n)$ for $n \ge N$.
- **Def.** $\Omega(g(n))$
    - $g(n) = O(f(n))$.
- **Def.** $\Theta(g(n))$
    - $f(n) = O(g(n))$ and $g(n) = O(f(n))$.
- **Q.** How should we think of $O$, $\Omega$ and $\Theta$?
    - $O$ means $\le$, $\Omega$ means $\ge$ and $\Theta$ means $=$.
- **Ex.** Basic techniques for comparing time complexities
    - Taking ratios, using L'HÃ´pital's rule.

#### Finding the time complexity
- **Q.** How do we compute runtime?
    - Look for loops and list insertions.
- **Ex.** List insertions making a difference
    - Insertion sort.

#### Graph algorithms
- [cf.](https://web.stanford.edu/class/cs97si/)
- **Q.** How can we represent graphs?
    - Adjacency matrix, adjacency list, incidence matrix.
- **Q.** What are two fundamental graph exploration algorithms?
    - DFS, BFS.
- **Q.** How can we think of the DFS and BFS trees?
    - They log how the algorithm worked.
- **Q.** DFS or BFS? 
    - [cf.](https://www.reddit.com/r/leetcode/comments/uhiitd/when_to_use_bfs_vs_dfs_in_graphs/) 

### Lecture 2
2024-01-17

#### Prelude
- **Q.** What is a greedy algorithm?
    - Builds a solution by always choosing the currently best option (whatever that is).
- **Q.** What are some characteristics of a greedy algorithm?
    - Fast and simple, difficult to prove correct.
- **Q.** How do we prove a greedy algorithm correct?
    - Stay ahead argument or exchange argument.
    - View the optimal solution as an algorithm that runs in parallel with the greedy algorithm.

#### Heuristics for greedy algorithms
- **Q.** Which kinds of problems often admit a greedy solution?
    - Scheduling problems, some graph algorithms.
- **Q.** How do we design a greedy algorithm?
    - To find a correct greedy rule, use intuition and trial and error.
    - Look for what is fast, stays ahead or frees up resources.

#### Interval scheduling
- **Q.** What steps are there in a stay ahead argument? [cf.](https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf)
    - Prove that $\mu(a_i) \le \mu(o_i)$ or vice versa.
- **Prob.** (Interval Scheduling). Find the maximum number of nonoverlapping intervals.
- **Ex.** Solving Interval Scheduling
    - Always pick the interval ending first.
- *Proof.* (Correctness). Stay ahead.

#### Job scheduling with minimum lateness
- **Q.** What steps are there in an exchange argument? [cf.](https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf)
    - Find where $\mathcal{A}$ and $\mathcal{O}$ differ, exchange.
- **Prob.** (Scheduling to Minimise Lateness). Find the minimum maximum lateness.
- **Ex.** Solving Scheduling to Minimise Lateness
    - Always pick the job with the earliest deadline.
- *Proof.* (Correctness). Exchange.

#### Greedy graph algorithm
- **Prob.** (Shortest Path). Find the shortest path from a source $s$ to any other vertex.
- **Algo.** (Dijkstra).
    - Add the vertex outside $U$ with is closest to $s$.

### Lecture 3
2024-01-22

#### Prelude
- **Q.** How do divide and conquer algorithms work?
    - Divide input, recursively solve each part, combine.
- **Q.** How do we draw the tree for $T(n) = aT(n/b) + f(n)$?
    - $\log_b n$ levels, $a^i$ tasks on level $i$.
- **Q.** How do we analyse the time complexity of a divide and conquer algorithm?
    - Master thm. or sum over levels.

#### Merge sort
- **Ex.** Recurrence for Merge sort 

#### Polynomial multiplication
- **Prob.** (Polynomial Multiplication). Find the coefficients of $p(x)q(x)$.
- **Q.** How can we divide and conquer Polynomial Multiplication?
    - Write $p = p_h x^{n/2} + p_l$.
- **Ex.** Attempt to solve Polynomial Multiplication
- **Ex.** Solving Polynomial Multiplication
    - Rewrite the middle term.
- **Ex.** The constant $a$ matters

#### Bit and unit cost
- **Q.** When can arithmetic operations be seen as basic?
    - If they fit in machine words, i.e. are smaller than $2^{64}$ bits.
- **Q.** What is the difference between unit cost and bit cost?

#### Karatsuba's algorithm
- **Ex.** Schoolbook multiplication algorithm
- **Q.** How can we divide and conquer Integer Multiplication?
    - Like polynomial multiplication. Evaluate at $x = 10$.
- **Algo.** (Karatsuba). 

#### The Fast Fourier Transform
- [cf.](https://www.youtube.com/watch?v=h7apO7q16V0) 
- **Q.** What is the Discrete Fourier Transform?
- **Q.** What is the Fast Fourier Transform?

#### The Master theorem
- **Q.** What do the terms in $T(n) = aT(n/b) + O(f(n))$ mean?
    - There are $a$ subproblems of size $n/b$. Splitting and combining costs $f(n)$.
- **Thm.** (Master).
- **Q.** In the master thm., what is $n^c$?
    - The number of tasks at the bottom level.
    - If we're on the bottom level, the $T(n/b)$ term vanishes.
    - At the bottom level, $T(n/b)$ vanishes. We are left with $T(n) = \Theta(n^c)$.
- **Q.** What is the intuition behind the master thm.?
    - The bottleneck, splitting and combining or the bottom-level tasks, dominates.

### Lecture 4
2024-01-29

#### Prelude
- **Q.** What is the idea behind dynamic programming?
    - Reuse solutions to subproblems.
- **Q.** What is the difference between dynamic programming and divide and conquer?
    - Problems are dependent/independent.
- **Q.** What is memoisation?
    - The process of storing solutions to subproblems in the cache.

#### Fibonacci
- **Prob.** (Fibonacci). Find $f_n$.
- **Ex.** Solving Fibonacci
- **Ex.** Implementing Fibonacci
    - `cache[n] = fib(n - 1) + fib(n - 2)`.

#### Weighted interval scheduling
- **Prob.** (Weighted Interval Scheduling). Find the maximum total weight from nonoverlapping intervals.
- **Ex.** Attempt to solve Weighted Interval Scheduling
- **Ex.** Solving Weighted Interval Scheduling
    - Order intervals by endpoints. Then case decomposition, depending on whether $I_n \in \mathcal{O}$.
- **Ex.** Implementing Weighted Interval Scheduling
    - `cache[n] = max(WIS(n - 1), w_j + WIS(p(n)))`.
    - $p(n)$ is non-conflicting intervals.

#### Knapsack
- **Prob.** (Knapsack, optimisation). Maximise value subject to the weight constraint.
- **Ex.** Solving Knapsack
    - Case decomposition, depending on whether $(v_n, w_n) \in \mathcal{O}$.
- **Ex.** Implementing Knapsack
    - `cache[i, c] = max(knapsack(i - 1, c), knapsack(i - 1, c - w_i) + v_i)`.

#### Top-down or bottom-up
- **Q.** What ways are there to implement a DP solution?
    - Top-down or bottom-up.
- **Q.** What are some characteristics of top-down? 
- **Q.** What are some characteristics of bottom-up?

#### Heuristics for dynamic programming
- **Q.** When might dynamic programming be useful?
    - There are few subproblems.
    - Subproblems have an ordering from "small" to "large".
    - They can be combined meaningfully.
- **Q.** How do you draw a dynamic programming diagram?
    - Arrows indicate dependencies.

### Lecture 5
2024-01-30

#### Heuristics for dynamic programming, continued
- **Q.** How do we design a dynamic programming algorithm? [cf.](https://www.youtube.com/watch?v=aPQY__2H3tE) 
    1. Decide which direction to work in.
        - Up/down, last/first, etc.
        - This assumes we have ordered the input.
        - Try visualising the problem, e.g. as a tree or a bipartite graph.
    2. Write down a recurrence relation.
        - State the recurrence in terms of array entries.
        - Think in terms of boolean, streak or counter matrices.
    3. Implementation.
        - Bottom-up/top-down.
- **Ex.** Prompts for dynamic programming problems
    - Can we find an optimal solution for the bottom/top or last/first objects?
    - What information should govern the algorithm?

#### Constructing solutions
- **Q.** How do we construct a solution to a dynamic programming problem?
    - Create an auxiliary function.
- **Ex.** Constructing a solution for Weighted Interval Scheduling

#### Sequence alignment
- **Prob.** (Sequence alignment). Find the minimum cost of aligning strings $x$ and $y$.
- **Ex.** Solving Sequence alignment
    - Reduce to a subproblem on the first objects.

#### Matrix chain multiplication
- **Prob.** (Matrix Chain Multiplication). Find the minimum cost of computing $A_1 \ldots A_n$, assuming that multiplying a $d_1 \times d_2$ matrix with a $d_2 \times d_3$ matrix costs $d_1d_2d_3$.
- **Ex.** Solving Matrix Chain Multiplication
    - Backwards reasoning. Dynamic programming over intervals.

#### Implementing bottom-up
- **Q.** How do we implement a bottom-up solution?
    - Create an $n \times m$-array.
    - Draw a diagram to ensure you get the computation order right.

### Lecture 6
2024-02-05

#### Flow networks
- cf. Diestel, chapter 6.
- **Notation.** $f(X, Y)$
- **Def.** Network
    - $N = (G, s, t, c)$, where $G$ is a multigraph.
- **Q.** Why do we want $G$ to be a multigraph?
    - There is no such thing as negative flow.
    - If we want to send flow in the opposite direction, we must add another edge. 
- **Def.** Flow $f : \vec{E} \to \mathbb{R}$
    - $f(e, x, y) = - f(e, y, x)$.
    - $f(v, V) = 0$ for $v \in V \setminus \{s, t\}$.
    - $f(\vec{e}) \le c(\vec{e})$.
- **Def.** Cut $F$ in $G$
    - If there is a partition into sides $\{V_1, V_2\}$ of $V$ s.t. $F = E(V_1, V_2)$. 
- **Def.** Cut in $N$
    - A pair $(S, \bar{S})$ s.t. $s \in S$ and $t \in \bar{S}$.
- **Def.** Capacity of a cut
    - $c(S, \bar{S})$.
- **Def.** Value of $f$
    - $|f| = f(S, \bar{S})$.

#### Maximum flows
- **Prob.** (Maximum Flow). Find a flow of maximum value.
- **Q.** What is the idea behind the Ford-Fulkerson algorithm?
    - Allow a greedy algorithm to undo bad choices.
- **Q.** How can we think of the residual network? 
    - It indicates how much we can undo.
- **Def.** Forward edge
    - The edge $(e, x, y)$ with capacity $c(\vec{e}) - f(\vec{e})$.
- **Def.** Backward edge
    - The edge $(e, y, x)$ with capacity $f(\vec{e})$.
- **Def.** Residual capacities
    - The capacities $c(\vec{e}) - f(\vec{e})$ and $f(\vec{e})$.
- **Algo.** (Ford-Fulkerson).
    - Find augmenting paths in the residual network.

#### Edmonds-Karp
- **Q.** What is the idea behind the Edmonds-Karp algorithm? 
    - Make a better choice of augmenting path.
- **Thm.** (Edmonds-Karp). If we choose augmenting paths $P$ with the fewest number of edges, only $nm$ iterations are needed.
- **Prop.** Complexity of Ford-Fulkerson/Edmonds-Karp
    - $O(C(n + m))$ and $O(nm(n + m))$.
- **Q.** What is strongly polynomial time?
    - If the complexity only depends on the input size. Not on the nature of the input.

#### Minimum cuts
- **Q.** What is another way to think of $s-t$ cuts?
    - A set of edges separating $s$ and $t$.
- **Def.** Directed cut
    - A set of edges in a directed graph separating $s$ and $t$.
- **Def.** Size/weight of a cut
    - Number of edges/total weight of edges.
- **Prob.** (Minimum Cut). Find an edge cut in the network $N$ of minimum capacity.

#### The Max Flow Min Cut theorem
- **Thm.** (Max Flow Min Cut). The upper bound $f(S, \bar{S}) \le c(S, \bar{S})$ is attained.
- *Proof.* (Max Flow Min Cut). Algorithmic.

### Lecture 7
2024-02-12

#### Prelude
- **Q.** Why does the Max Flow Min Cut thm. matter?
    - Tells us there is a kind of duality between paths and separators.
    - Maximum Flow and Minimum Cut are the same problem.
    - Can be used to give easier proofs of Menger and KÃ¶nig. [cf.](https://math.stackexchange.com/questions/4319279/questions-on-proof-of-konigs-theorem)

#### Vertex Cut
- **Q.** How do we model vertex capacities?
    - Add $v_{\text{in}}$ and $v_{\text{out}}$.
- **Prob.** (Vertex Cut). Find a minimum vertex separator. 
- **Ex.** Solving Vertex Cut
    - Vertex capacities of $1$ and edge capacities of $\infty$.

#### Many sources or sinks
- **Ex.** Maximum Flow if there are multiple sources/sinks
    - Add a super source/super sink.

#### Matching numbers
- **Def.** Matching $M$
    - A set of independent edges.
- **Def.** Matching number $\nu(G)$
    - The size of a maximum matching.
- **Prob.** (Matching Number). Find $\nu(G)$.
- **Ex.** Solving Matching Number
    - Add a super sink and a super source. Set all edge capacities to be $1$.
- Compare with KÃ¶nig.

#### Disjoint paths
- **Prob.** (Edge-disjoint Paths). Find the maximum number of edge-disjoint $s$-$t$-paths.
- **Ex.** Solving Edge-disjoint Paths
    - Set all edge capacities to $1$.
- **Prob.** (Disjoint Paths). Find the maximum number of disjoint $s$-$t$-paths.
- **Ex.** Solving Disjoint Paths
    - Set all vertex capacities to $1$.
- Compare with Menger.

#### Closure
- **Prob.** (Closure). A set of projects with values and dependencies. Want to find maximal total value.

### Lecture 8
2024-03-18

#### Randomised algorithms
- **Q.** What is a randomised algorithm?
    - Uses randomness to make some choices.
- **Q.** What is a Las Vegas algorithm?
    - Correct answer, random runtime.
- **Q.** What is a Monte Carlo algorithm?
    - Random correctness, (usually) guarantee on runtime.
- **Q.** What is the relation between Las Vegas and Monte Carlo algorithms?
    - A Las Vegas algorithm can be turned into a Monte Carlo algorithm.

#### Random number generators
- **Q.** What kinds of RNGs are there?
    - True RNGs and pseudo-RNGs.

#### Approximation algorithms
- **Q.** What is an approximation algorithm?
    - Finds a solution that is guaranteed to be close to optimal.
- **Q.** What is the relation between approximation algorithms and randomised algorithms?

#### Global Minimum Cut
- **Prob.** (Global Minimum Cut). Partition the vertices of $G$ into sides s.t. that the size of the cut is minimised.
- **Ex.** Solving Global Minimum Cut with Ford-Fulkerson
    - View $G$ as a network. Run Ford-Fulkerson for fixed $s$ and let $t$ vary.
- **Ex.** Approximating Global Minimum Cut
    - Randomly contract edges.
- **Ex.** Analysing Global Minimum Cut
    - Compute the probability that the $i$:th choice is bad, use $1 - x \le e^{-x}$.
- **Q.** How can we improve the expected success rate of a randomised algorithm?
    - Rerun the randomised algorithm, take best result.

#### Global Maximum Cut
- **Prob.** (Global Maximum Cut). Partition the vertices of $G$ into sides s.t. that the size of the cut is maximised.
- **Ex.** Approximating Global Maximum Cut
    - Place edges in either side with probability $1/2$.
- **Ex.** Analysing Global Maximum Cut
    - Write $\mathsf{Alg} = \sum_{i} \chi_i$ and use linearity of expectation.

### Lecture 9
2024-03-19

#### Prelude
- **Q.** What is an algorithm?

#### Turing machines
- **Q.** What is a Turing machine?
    - A machine that manipulates symbols on a strip of tape according to some rules.
    - [demo](http://turingmachine.io).
- **Q.** What goes into a Turing machine?
    - An alphabet $\Gamma$, a set of states $Q$, a transition function $\delta$. 
- **Def.** Turing machine $M$
- **Q.** What is the Church-Turing hypothesis?
    - Anything that can be computed can be computed by a Turing machine.
- **Q.** Why do we need it?
    - So we can analyse "normal" algorithms in terms of Turing machines.

#### RAM
- **Q.** What is the RAM model of computation?
    - Models the way modern computers work with registers and memory cells.
- **Rmk.** Pseudocode implicitly assumes RAM.
- **Q.** What kinds of RAM are there?
    - Basic RAM, word RAM, parallel RAM, etc.
- **Q.** What is word RAM?
    - Registers and memory cells have limited word sizes.
- **Q.** In the RAM model, how can we think of unit and bit cost?
    - Unit cost is basic RAM, bit cost is word RAM.

#### Models of computation
- **Ex.** Our favourite models of computation
    - Turing machines, RAM, etc.
- **Q.** What is a Turing-complete model of computation?
    - One which can do anything a Turing machine can do.
- **Q.** What is a computable function?
    - A function whose value can be computed by an effective procedure.

#### Restricted models of computation
- **Q.** Why do we care about restricted models of computation?
    - Hard to reason about all algorithms out there.
- **Ex.** Our favourite restricted model of computation
    - Limiting what the algorithm can do.

#### Comparison-based sorting
- **Ex.** Comparison-based sorting must take $\Omega(n \log n)$ time
    - Need to distinguish between $n!$ permutations.
    - $k$ comparisons allows us to differentiate between $2^k$ permutations.
- **Ex.** Lower bound of $\log n!$
    - Estimate with tail terms. 
    - Stirling's formula.
- **Q.** Is this a natural restriction?
    - It depends on the nature of the input.
- **Ex.** Sorting in $O(n)$
    - Counting sort.

### Lecture 10
2024-03-25

#### Prelude
- **Q.** What are the main types of problems?
    - Search, optimisation, decision.
- **Q.** What is a decidable problem?
    - There exists some algorithm for solving it.
- **Ex.** Trading is undecidable

#### Heuristics for transforming problems
- **Q.** How can we reduce a search problem to a decision problem?
    - Produce an instance where all choices are determined.
- **Ex.** Ways to make progress in graph problems
    - Removing or adding edges, contracting vertices, etc.
- **Ex.** Finding a $k$-colouring
    - All choices are determined in a complete $k$-partite graph.
    - This suggests adding edges.
- **Ex.** Finding a Hamilton cycle
    - All choices are determined if there is just one Hamilton cycle.
    - This suggests removing edges.

#### Halting
- **Prob.** (Halting). If $M(x)$ halts.
- **Prop.** Halting is undecidable.
- *Proof.* (Halting).
    - `A(M) = if halts(M, M) enter infinite loop`.
- **Prop.** (Halting on Empty Input).
- **Prob.** (Halting on All Inputs).
- **Cor.** Halting on Empty Input and Halting on All Inputs are undecidable.
- *Proof.* Let $M'$ simulate $M$ on input $x$.

#### Undecidable problems
- **Ex.** Undecidable problems
    - Halting, Diophantine Equations, Post Correspondence Problem.
- **Q.** What is a recursively enumerable problem?
    - Eventually terminates on yes-instances.

#### Turing reductions
- **Q.** What is a Turing/Cook reduction?
    - An algorithm for problem $X$ assuming an algorithm for problem $Y$.
- **Q.** How can one understand $\le_T$?
    - Larger means harder.
- **Q.** What are positive and negative reductions?
    - Positive reductions allow us to solve things.
    - Negative reductions prove that things are unsolvable.
- **Ex.** Our favourite reduction
    - Minimum Cut to Maximum Flow.

#### Examples of reductions
- **Ex.** Independent Set $\le_P$ Vertex Cover
- **Ex.** Vertex Cover $\le_P$ Set Cover

#### 3-SAT
- **Prob.** ($3$-SAT). If $\phi$ is satisfiable.
- **Q.** What are the main ways to think of $3$-SAT?
    - Find a true literal for each clause without picking $x_i$ and $\neg x_i$.
    - Find an assignment of variables s.t. each clause is true.
- **Ex.** $3$-SAT $\le_P$ Independent Set
    - Each clause is a $K_3$. Put edges between $x_i$ and $\neg x_i$.

#### Karp reductions
- **Q.** What is a Karp reduction?
    - Given an instance $A$ of problem $X$, create an instance $B = f(A)$ of problem $Y$.

### Lecture 11
2024-03-27

#### Prelude
- Recall that $\le_T$ indicates hardness, where bigger is harder.

#### Problems as languages
- **Q.** What is a formal language over $\{0, 1\}$?
    - A set of binary strings with $0$'s and $1$'s.
- **Q.** What is the relation between a formal language and a decision problem?
    - $L_p$ consists of the "yes"-instances.

#### P
- **Def.** The complexity class $\mathsf{P}$
    - A language $L$ is in $\mathsf{P}$ iff there is an $O(n^k)$ algorithm deciding $L$.
- **Q.** What is the extended Church-Turing thesis?
    - Every effective computation can be carried out effectively by a Turing machine.
- **Q.** Why do we need the Extended Church-Turing thesis?
    - For the definition of $\mathsf{P}$.
    - Not only do we care about the existence of an algorithm (computability), but also about computational efficiency. 
- **Rmk.** It is probably false!

#### NP
- **Q.** What is a witness/certificate?
    - "Proof by existence".
- **Q.** What is $\mathsf{NP}$?
    - Problems with efficient verification.
- **Def.** The complexity class $\mathsf{NP}$
    - A language $L$ is in $\mathsf{NP}$ iff there's a polynomial $p$ and and a polynomial-time algorithm $A : \{0, 1\}^* \times \{0, 1\}^* \to \{0, 1\}$ such that:
    - If $x \in L$, there's a $y$ satisfying $|y| \le p(|x|)$ s.t. $A(x, y) = 1$.
    - If $x \notin L$, then $A(x, y) = 0$ for all $y$.
- **Q.** How should we understand the definition of $\mathsf{NP}$?
    - $y$ is the certificate. There are no fake certificates.

#### P vs. NP
- **Q.** Why is $\mathsf{P} \subset \mathsf{NP}$?
    - Run the algorithm. If "yes", let any $y$ be witness. If "no", there are no witnesses.
- **Q.** What is another way to put "Is $\mathsf{NP} \subset \mathsf{P}$"?
    - If we can check it, can we solve it?

#### NP-completeness
- **Q.** How can we think of $\mathsf{NP}$-hard problems?
    - They're at least as hard as every problem in $\mathsf{NP}$.
- **Def.** $\mathsf{NP}$-hard problem $Y$
    - If $X \le_p Y$ for all $X \in NP$.
- **Q.** How can we think of $\mathsf{NP}$-complete problems?
    - The hardest kinds of problem in $\mathsf{NP}$.
- **Def.** $\mathsf{NP}$-complete problem $Y$
    - If $Y \in NP$ and $Y$ is $\mathsf{NP}$-hard.
- **Q.** How do you show $Y$ is $\mathsf{NP}$-hard?
    - Reduce an $\mathsf{NP}$-hard problem to $Y$.

#### NP-complete problems
- **Thm.** (Cook-Levin). *SAT* is $\mathsf{NP}$-hard.
- **Ex.** Reduction from SAT to 3-SAT
    - Add a "chain variable" in each clause, e.g. $(x_1 \vee x_2 \vee \neg y_1) \wedge (y_1 \vee x_3 \vee \neg y_2)$.
- **Q.** How do you show a reduction is correct?
    - It's polynomial.
    - "yes" $\mapsto$ "yes".
    - "no" $\mapsto$ "no".

### Lecture 12
2024-04-08

#### Prelude
- **Q.** What is the difference between Karp reductions and Turing reductions?
    - Turing allows you to call the black box more than once.
- **Q.** Why do we care more about Karp reductions?
    - Because $\mathsf{NP}$-hardness is defined in terms of Karp reductions.
- We will discuss more examples of $\mathsf{NP}$-complete problems.

#### Hamiltonian Cycle
- **Prob.** (Hamiltonian Cycle). Determine whether $G$ has a Hamiltonian cycle.
- **Thm.** Hamiltonian Cycle is $\mathsf{NP}$-hard.
- *Proof.* (Hamiltonian Cycle is $\mathsf{NP}$-hard).
    - Setting $x$ to "True" is like traversing the $x$ row rightwards.
    - Add a detour for each clause.
    - Show it's a valid reduction.

#### Travelling salesman
- **Prob.** (Travelling Salesman). Determine whether there a travelling salesman tour of length at most $k$. 
- **Thm.** Travelling Salesman is $\mathsf{NP}$-hard.
- *Proof.* (Travelling Salesman is $\mathsf{NP}$-hard). 
    - Reduction from Hamiltonian Cycle, with $$d(u, v) = \begin{cases} 1, & (u, v) \in E, \\ \infty.\end{cases}$$

#### Graph colourings
- **Prob.** ($k$-colouring). Determine whether $G$ is $k$-colourable.
- **Thm.** $3$-colouring is $\mathsf{NP}$-hard.
- **Rmk.** $2$-colouring is easy -- run DFS.
- **Thm.** $k$-colouring is $\mathsf{NP}$-hard.

#### Subset sum
- **Prob.** (Subset sum). Determine if there is a subset $S \subset [n]$ s.t. $\sum_{i \in S} x_i = t$.
- **Thm.** Subset sum is $\mathsf{NP}$-hard.

#### Knapsack
- **Prob.** (Knapsack). Determine if there is a subset $S \subset [n]$ s.t. $\sum_{i \in S} w_i \le C$ and $\sum_{i \in S} v_i \ge k$.
- **Thm.** Knapsack is $\mathsf{NP}$-hard.
- *Proof.* (Knapsack is $\mathsf{NP}$-hard).
    - Reduction from Subset sum to Knapsack.
    - Take $x_i = v_i = w_i$ and $C = k = t$.
- **Rmk.** We used DP to find a $O(nC)$ algorithm.
- **Q.** Why is $O(nC)$ not polynomial? 
    - If each of the $2n + 1$ variables has $\log C$ bits, the input size is $O(n \log C)$.
    - Set $m = n \log C$. The runtime $O(f(m))$ is not polynomial.

#### Summary NP-completeness
- **Q.** How can we visualise Cook-Levin in a Venn diagram?
    - Draw arrows from all problems in $\mathsf{NP}$ to Sat.
- **Q.** How can we visualise the $\mathsf{NP}$-complete problems?
    - A complete graph.

#### Understanding the definition of NP
- **Q.** How can we understand Turing machines?
    - As an algorithm, or a computer program in our favourite programming language.
- **Q.** How can we understand the definition of $\mathsf{NP}$, without $A$?
    - $X$ is in $\mathsf{NP}$ iff there's an algorithm $M$ in time $p(|x|)$ s.t.
        - If $x \in X$, there's a short $y$ s.t. $M(x, y)$ accepts.
        - If $x \notin X$, then $M(x, y)$ rejects all $y$.

#### Proof sketch of Cook-Levin
- *Proof sketch.* (Cook-Levin).
    - Construct a SAT formula $\phi$ satisfiable iff there's a witness $y$ s.t. $M(x, y)$ accepts in time $p(|x|)$.
    - Add Boolean variables that specify the state of the machine.
    - Add constraints to ensure the states are valid.

### Lecture 13
2024-05-07

#### CoNP
- **Q.** What is the first way to think of $\mathsf{CoNP}$?
    - "No"-instances can easily be verified.
- **Q.** What is the second way to think of $\mathsf{CoNP}$?
    - $X^c \in \mathsf{NP}$.
- **Def.** Co-NP
- **Ex.** Problem in $\mathsf{CoNP}$
    - Tautology, primality testing.

#### CoNP-hardness
- **Def.** $\mathsf{CoNP}$-hard, $\mathsf{CoNP}$-complete
- **Q.** If $X \le Y$, what is the relation between $X^c$ and $Y^c$?
- **Q.** What is the relation between $\mathsf{NP}$-hardness and $\mathsf{CoNP}$-hardness?
    - $X$ is $\mathsf{CoNP}$-hard iff $X^C$ is $\mathsf{NP}$-hard.
- **Ex.** $\mathsf{CoNP}$-complete problem
    - No Hamiltonian Cycle

#### P, NP & CoNP
- **Q.** Is $\mathsf{NP} = \mathsf{CoNP}$?
    - Maybe not.
- **Q.** Is $\mathsf{P} = \mathsf{NP} \cap \mathsf{CoNP}$?
    - Maybe not.

#### PSPACE
- **Def.** $\mathsf{PSPACE}$
- **Q.** How does $\mathsf{PSPACE}$ relate to $\mathsf{P}$ and $\mathsf{NP}$?
    - It contains them.
- **Ex.** Problem in $\mathsf{PSPACE}$ but not $\mathsf{NP}$
    - Generalised chess.
- **Q.** Why is generalised chess in $\mathsf{PSPACE} - \mathsf{P}$?

#### PSPACE-hardness
- **Def.** $\mathsf{PSPACE}$-hard, $\mathsf{PSPACE}$-complete
- **Ex.** $\mathsf{PSPACE}$-complete problem
    - QBF.

#### Venn diagram
- **Q.** How do we think the complexity classes relate?

#### BPP & ZPP
- **Def.** $\mathsf{BPP}$
- **Q.** Why $2/3$?
    - Immaterial, just as long as it's better than $1/2$.
- **Def.** $\mathsf{ZPP}$
- **Q.** What is the relation between $\mathsf{P}$, $\mathsf{ZPP}$ and $\mathsf{BPP}$?

