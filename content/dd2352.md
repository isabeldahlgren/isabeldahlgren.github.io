---
title: 'DD2352'
date: '2023-02-14'
draft: false
---

<!-- mtoc-start -->

* [Part I. Algorithms](#part-i-algorithms)
  * [Lecture 1](#lecture-1)
    * [Examples of algorithms](#examples-of-algorithms)
    * [Time complexity](#time-complexity)
    * [Graph algorithms](#graph-algorithms)
  * [Lecture 2](#lecture-2)
    * [Heuristics for greedy algorithms](#heuristics-for-greedy-algorithms)
    * [Interval scheduling](#interval-scheduling)
    * [Job scheduling with minimum lateness](#job-scheduling-with-minimum-lateness)
    * [Graph algorithms, continued](#graph-algorithms-continued)
  * [Lecture 3](#lecture-3)
    * [Prelude](#prelude)
    * [Merge sort](#merge-sort)
    * [Polynomial multiplication](#polynomial-multiplication)
    * [Bit and unit cost](#bit-and-unit-cost)
    * [Karatsuba's algorithm](#karatsubas-algorithm)
    * [FFT](#fft)
    * [Master thm.](#master-thm)
  * [Lecture 4](#lecture-4)
    * [Prelude](#prelude-1)
    * [Fibonacci](#fibonacci)
    * [Weighted interval scheduling](#weighted-interval-scheduling)
    * [Knapsack](#knapsack)
    * [Top-down or bottom-up](#top-down-or-bottom-up)
    * [Heuristics for DP](#heuristics-for-dp)
  * [Lecture 5](#lecture-5)
    * [Heuristics for DP](#heuristics-for-dp-1)
    * [Constructing solutions](#constructing-solutions)
    * [Aligning strings](#aligning-strings)
    * [Matrix chain multiplication](#matrix-chain-multiplication)
  * [Lecture 6](#lecture-6)
    * [Flow networks](#flow-networks)
    * [Maximum flows](#maximum-flows)
    * [Edmonds-Karp](#edmonds-karp)
    * [Minimum cuts](#minimum-cuts)
    * [The Max-flow Min-cut theorem](#the-max-flow-min-cut-theorem)
  * [Lecture 7](#lecture-7)
    * [Prelude](#prelude-2)
    * [Vertex capacities](#vertex-capacities)
    * [Many sources/sinks](#many-sourcessinks)
    * [More graph theory](#more-graph-theory)
    * [Applications to bipartite graphs](#applications-to-bipartite-graphs)
    * [Edge-disjoint paths](#edge-disjoint-paths)
    * [The closure problem](#the-closure-problem)

<!-- mtoc-end -->

## Part I. Algorithms

### Lecture 1
2024-01-16

#### Examples of algorithms
- **Q.** How do we measure the efficiency of an algorithm?
    - Count basic steps.
- **Algo.** (Insertion sort).
    - Insert elements into their correct position.
- **Algo.** (Merge sort).
    - Divide an array into halves, sort them, merge.
- This is our favourite example of a DC algorithm.
- **Ex.** Insertion sort is $O(n^2)$ and merge sort is $O(n\log n)$

#### Time complexity
- **Def.** $O(g(n))$
    - There exist $c$ and $c_0$ s.t. $f(n) \le c g(n)$ for $n \ge n_0$.
- Very much like the definition of convergence.
- **Def.** $\Omega(g(n))$
    - $g(n) = O(f(n))$.
- **Def.** $\Theta(g(n))$
    - $f(n) = O(g(n))$ and $g(n) = O(f(n))$.
- **Q.** How should you think of $O$, $\Omega$ and $\Theta$?
    - $O$ means $\le$, $\Omega$ means $\ge$ and $\Theta$ means $=$.
- **Q.** How do you compute runtime?
    - Look for For, While and list insertions.

#### Graph algorithms
- **Q.** How can you represent a graph?
    - Adjacency matrix, adjacency list, incidence matrix.
- **Q.** How can we get from $s$ to $t$ in a directed graph?
    - DFS, BFS. [cf.](https://web.stanford.edu/class/cs97si/06-basic-graph-algorithms.pdf) 
- BFS "places $s$ in the middle".
- The trees "log" how the algorithm worked.
- **Q.** DFS or BFS? [cf.](https://www.reddit.com/r/leetcode/comments/uhiitd/when_to_use_bfs_vs_dfs_in_graphs/) 
    - Use BFS if you want to find the shortest path.
    - Use DFS if you just want a route. It's typically easier to implement.

### Lecture 2
2024-01-17

#### Heuristics for greedy algorithms
- **Q.** What is a greedy algorithm?
    - Builds a solution by always choosing the currently best option (whatever that is).
- **Q.** How do you design a greedy algo?
    - To find the "currently best option", use intuition. Look for what is fast, stays ahead, is more economical or frees up resources -- look for extremality.
    - Order your input. 
- **Q.** Characteristics of a greedy algorithm?
    - Fast and simple, probably incorrect (or at least difficult to prove correct).
- **Q.** How do you prove correctness of a greedy algorithm? 
    - Stay ahead argument or exchange argument.
    - These arguments can be confusing. It helps viewing the optimal solution as an "algorithm", that runs in parallel with your greedy algorithm. You want to compare them in each step.

#### Interval scheduling
- **Q.** Steps in a stay ahead argument? [cf.](https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf)
    - Define $\mu$, $\mathcal{A}$ and $\mathcal{O}$.
    - Prove that $\mu(a_i) \le \mu(o_i)$ or vice versa.
    - Contradiction.
- **Ex.** IS algorithm
- *Proof.* (Correctness, IS algorithm). 
    - Order $\mathcal{A}$ and $\mathcal{O}$.
    - Prove that $f(a_i) \le f(o_i)$ by induction.
    - Contradiction.

#### Job scheduling with minimum lateness
- **Q.** Steps in an exchange argument? [cf.](https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf)
    - Define $\mathcal{A}$ and $\mathcal{O}$.
    - Find out where they clash.
    - Exchange and iterate.
- To show you can exhange, exhaust all cases.
- Check for inversions, etc.
- **Ex.** JSML algorithm
- *Proof.* (Correctness, JSML algo). 
    - Show that you can remove all inversions.

#### Graph algorithms, continued
- **Q.** Why Dijkstra?
    - Gives the shortest path from $s$ to $t$ in a weighted graph. Like a better BFS.
- **Algo.** (Dijkstra).

### Lecture 3
2024-01-22

#### Prelude
- **Q.** What is DC?
    - Divide input, recursively solve each part, combine.
- **Q.** How do you draw the tree for $T(n) = aT(n/b) + f(n)$?
    - $\log_b n$ levels, $a^i$ tasks on level $i$.

#### Merge sort
- **Ex.** Derivation of recurrence for Merge sort 

#### Polynomial multiplication
- **Ex.** Derivation of solution for Polynomial multiplication
    - Write $p = p_h x^{n/2} + p_l$.
    - Multiply the sums $(p_h + p_l)(q_h + q_l)$.
    - Solve for what you want.
- **Ex.** In the Master thm., the constant $a$ in $T(n) = aT(n/b) + f(n)$ matters, like a lot
    - See the two attempts to solve Polynomial multiplication.

#### Bit and unit cost
- **Q.** When is it reasonable to treat basic arithmetic operations as "a basic step"?
    - If numbers are $\le 2^{64}$.
- **Q.** What's the unit cost model?
    - Arithmetic takes $O(1)$.
- **Q.** What's the bit cost model?
    - Arithmetic doesn't necessarily take $O(1)$. Need to account for size of numbers.

#### Karatsuba's algorithm
- **Ex.** Finding the complexity of an algorithm doing arithmetic
    - Write intermediate results in a table. If it's a $c_1n \times c_2n$ table, the complexity is $O(n^2)$.
- **Q.** How can we write numbers in a more "conquerable" form?
    - Write them as split polynomials evaluated at a given point, e.g. $x = 10$.

#### FFT
- **Q.** What's the DFT?
    - Evaluating a function on the roots of unity.
- **Q.** What's the FFT?
    - Algorithm computing the DFT or the inverse of the DFT.
- **Ex.** Polynomial multiplication in $O(n\log n)$
    - Evaluate $p$ and $q$ at roots of unity, multiply the values, interpolate.

#### Master thm.
- **Q.** How do we analyse the time complexity of a DC algorithm?
    - Set up the recurrence, e.g. $T(n) = aT(n/b) + f(n)$. Use the Master thm.
- **Q.** What do the terms in $T(n) = aT(n/b) + O(f(n))$ mean?
    - $a$ subproblems of size $n/b$. Splitting and combining costs $f(n)$.
- **Thm.** (Master). Let $T(1) = O(1)$, $T(n) = a T(n/b) + \Theta(f(n))$ and $c = \frac{\log a}{\log b}$.
    - If $f(n) = O(n^{c'})$ for $c' < c$, then $T(n) = \Theta(n^c)$.
    - If $f(n) = \Theta(n^c)$ then $T(n) = \Theta(n^c \log n)$.
    - If $f(n) = \Omega(n^{c'})$ for $c' > c$, then $T(n) = \Theta(f(n))$.
- **Q.** In the Master thm., what's $n^c$?
    - The number of tasks at the bottom level.
    - If we're in the "bottom level case" when $n = m$, the $T(m/b)$ term vanishes, and we're left with $T(m) = \Theta(n^c)$.
- **Q.** Intuition behind the master thm.?
    - The bottleneck (either splitting and combining or the bottom-level tasks) dominates.
- **Ex.** Master thm. in practise 
    - Karatsuba (case 1), merge sort (case 3).

### Lecture 4
2024-01-29

#### Prelude
- **Q.** Main idea behind DP? 
    - Recycle solutions to subproblems.
- Just as in DC, we split a problem into subproblems.
- **Q.** What's the difference between DP and DC?
    - In a DC solution, the subproblems are independent. In a DP solution, the subproblems depend on each other.
- **Q.** What's memoization?
    - The process of storing solutions ot problems in the cache.

#### Fibonacci
- **Ex.** Derivation of solution of Fibonacci
    - Draw a tree, observe that multiple values show up.
- **Ex.** Implementing Fibonacci, top-down
    - `let cache[n] = fib(n - 1) + fib(n - 2)`.

#### Weighted interval scheduling
- **Ex.** Derivation of solution of WIS
    - Order intervals by endpoints. 
    - Case decomposition, depending on whether $I_n \in \mathcal{O}$.
- **Ex.** Implementing WIS, top-down 
    - `let cache[n] = max(wis(n - 1), w_j + wis(p(n)))`.
    - $p(n)$ is non-conflicting intervals.

#### Knapsack
- **Ex.** Derivation of solution of Knapsack
    - Case decomposition, depending on whether $(v_n, w_n) \in \mathcal{O}$.
- **Rmk.** No need to order the input.
- **Rmk.** Knapsack is a "$2$-dimensional DP problem".
- **Ex.** Implementing Knapsack, top-down
    - `let cache[i, c] = max(knapsack(i - 1, c), knapsack(i - 1, c - w_i) + v_i)`.

#### Top-down or bottom-up
- **Q.** Ways of approaching a DP problem?
    - Top-down means computing subresults "downwards".
    - Bottom-up means computing subresults "upwards".
- **Q.** TD or BU? 
    - TD is straightforward once you've got it and only solves what you need.
    - BU means cleaner and faster code and it is more flexible.

#### Heuristics for DP
- **Q.** When should I do DP?
    - DP is useful if there are few subproblems, subproblems have an ordering from "small" to "large" and the subproblems can be "merged".
- **Q.** How do you draw a DP?
    - Arrows indicate dependencies "downwards".
- Needs to be a DAG in order for there to be a solution.

### Lecture 5
2024-01-30

#### Heuristics for DP
- **Q.** Steps in a DP? [cf.](https://www.youtube.com/watch?v=aPQY__2H3tE) 
    1. Decide which "direction" to work in.
        - What I'm referring to is up/down, last/first, right/left, etc. Use extremality.
        - To do this, order your input.
        - It's often helpful to visualise the problem as a binary tree, e.g. if it's DP over intervals.
    2. Write down a recurrence relation.
        - Do this using an array. Think of boolean, streak or counter matrices.
        - If this is difficult to do in one go, try drawing a DP diagram.
    4. Implementation.
        - Bottom-up/top-down.
- **Q.** Prompts for step one, DP? 
    - What's the recurrence relation here?
    - Can I find an optimal solution for the first/last objects?
- The recurrence may be given explicitly, or else you will have to come up with it yourself. 

#### Constructing solutions
- **Ex.** Constructing solution for WIS
- **Q.** How do you construct a solution in a DP problem?
    - Use an auxiliary function.

#### Aligning strings
- **Ex.** Derivation of solution of Aligning strings
    - Model the problem with a graph.
    - Reduce to a subproblem on the first objects.
    - Derive a recurrence relation.

#### Matrix chain multiplication
- **Q.** What should you do if you cannot find an optimal solution for the first objects?
    - Backwards reasoning.
- **Q.** How do you implement a bottom-up solution?
    - Create an $n \times m$-array.

### Lecture 6
2024-02-05

#### Flow networks
- Quick summary of chapter 6 in Diestel.
- **Notation.** $f(X, Y)$
    - $f(X, Y) = \sum_{\vec{e} \in \vec{E}(X, Y)} f(\vec{e})$.
- **Def.** Network
    - $N = (G, s, t, c)$, where $G$ is a multigraph.
- There's no such thing as "negative" flow. If we want to send flow in the opposite direction, we must add another edge. This is why we let $G$ be a multigraph.
- **Def.** Flow $f : \vec{E} \to \mathbb{R}$
    - $f(e, x, y) = - f(e, y, x)$.
    - $f(v, V) = 0$ for $v \in V \setminus \{s, t\}$.
    - $f(\vec{e}) \le c(\vec{e})$.
- **Def.** Cut $F$ in $G$
    - If there's a partition into sides $\{V_1, V_2\}$ of $V$ s.t. $F = E(V_1, V_2)$. 
- **Def.** Bond
    - Minimal non-empty cut in $G$.
- **Def.** Cut in $N$
    - A pair $(S, \bar{S})$ s.t. $s \in S$ and $t \in \bar{S}$.
- **Def.** Capacity of a cut
    - $c(S, \bar{S})$.
- **Def.** Value of $f$
    - $|f| = f(S, \bar{S})$.
- **Q.** Why do we care about flows?
    - Lots of real-world examples and theoretical applications (see below).

#### Maximum flows
- **Q.** What's the maximum value of a flow?
- **Q.** What's the idea behind the Ford-Fulkerson algorithm?
    - Allow a greedy algorithm to "undo" bad choices.
- **Q.** How can we think of the residual network? 
    - It indicates how much slack there is, "how much we can undo".
- **Def.** Forward edge
    - The edge $(e, x, y)$ with capacity $c(\vec{e}) - f(\vec{e})$.
- **Def.** Backward edge
    - The edge $(e, y, x)$ with capacity $f(\vec{e})$.
- **Def.** Residual capacities
    - The capacities $c(\vec{e}) - f(\vec{e})$ and $f(\vec{e})$.
- **Algo.** (Ford-Fulkerson).
    - Find augmenting paths in the residual network.

#### Edmonds-Karp
- **Q.** What's the idea behind the Edmonds-Karp algorithm? 
    - Make better choice of augmenting path.
- **Thm.** (Edmonds-Karp).
    - If we choose augmenting paths $P$ with the fewest number of edges, only $nm$ iterations in the while-loop are needed. It's like "replacing $C$ with $nm$".
- **Prop.** Complexity of Ford-Fulkerson/Edmonds-Karp
    - $O(C(n + m))$ and $O(nm(n + m))$.
- **Q.** What's strongly polynomial time?
    - If the complexity only depends on the input size, and not on the "nature" of the input.

#### Minimum cuts
- **Q.** Diestel defines cuts in terms of vertex partitions. What's another way to think of $s-t$ cuts?
    - A set of edges separating $s$ and $t$.
- **Def.** Directed cut
    - A set of edges in a directed graph separating $s$ and $t$.
- **Def.** Size/weight of a cut
    - Number of edges/total weight of edges.
- **Q.** What's the minimum weight of a $s-t$ cut?

#### The Max-flow Min-cut theorem
- **Thm.** (Max-flow Min-cut).
    - The trivial upper bound $f(S, \bar{S}) \le c(S, \bar{S})$ is attained.
- *Proof.* (Max-flow Min-cut).
    - Denote by $S_n$ the vertices $v$ s.t. $G$ contains an $s-v$ walk with $f_n(\vec{e}_i) < c(\vec{e}_i)$ in step $n$.
    - If there's no augmenting path, $f_n(\vec{e}) = c(\vec{e})$ for all edges in the directed cut.

### Lecture 7
2024-02-12

#### Prelude
- **Q.** Why does the Max-flow Min-cut thm. matter?
    - The Max-flow Min-cut is a really, really deep result. It tells us there's a duality between flows/paths and cuts/separators.
    - Classical applications include proving Menger and KÃ¶nig, whose "normal" proofs are quite hacky. [cf.](https://math.stackexchange.com/questions/4319279/questions-on-proof-of-konigs-theorem) 
    - Numerous problems can be reduced to Max-flow Min-cut.
- **Q.** What kinds of problems can be reduced to Max-flow Min-cut?
    - Many Max-flow Min-cut problems look as though they're amenable to DP.
    - But sometimes it feels like the problem has just "changed/moved around" rather than becoming smaller.
- **Q.** What kinds of capacity constraints can we reduce to a Max-flow problem?
    - Constraints on edges, vertices or both.

#### Vertex capacities
- **Ex.** Modelling vertex capacities
    - Add $v_{\text{in}}$ and $v_{\text{out}}$, so any flow through $v$ is constrained by the middle edge.
- **Ex.** Finding the cardinality of a minimum (vertex) separator using FF
    - Vertex capacities of $1$ and edge capacities of $\infty$.

#### Many sources/sinks
- **Ex.** Max flow if there are multiple sources/sinks
    - Add a super source/super sink.

#### More graph theory
- Recall basic terminology.
- **Def.** Matching $M$
    - A set of independent edges.
- So no two edges in $M$ are incident or "touch" one another.
- **Def.** Bipartite graph
    - $V$ admits a partition into two classes s.t. every edge has its ends in different classes (i.e. two independent sets).

#### Applications to bipartite graphs
- **Q.** How can we find the matching number $m_G$?
- **Ex.** Computing $m_G$ using FF
    - Super sink and super source, all edges have capacity $1$.

#### Edge-disjoint paths
- **Ex.** Finding the maximum number of edge-disjoint paths
    - Take all capacities to be $1$. 
- **Ex.** Finding the maximum number of vertex-disjoint paths
    - Take all vertex capacities to be $1$. 

#### The closure problem
- **Prob. (Closure).**
    - A set of projects with values and dependencies. Want to find maximal total value.
- Can reduce this to a Max-flow problem.
