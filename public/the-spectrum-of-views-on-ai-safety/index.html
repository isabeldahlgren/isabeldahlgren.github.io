<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/the-spectrum-of-views-on-ai-safety/" />
    <title>The spectrum of views on AI safety</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>The spectrum of views on AI safety</h1>
        <div>
                <time>July 20, 2025</time>
            </div>
    </header><p>I agree the concept of <a href="https://en.wikipedia.org/wiki/P(doom)">P(doom)</a> is problematic. First, &ldquo;doom&rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.</p>
<p>But perhaps we&rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they&rsquo;re just interested in hearing general takes on AI safety, at least in my experience.</p>
<p>The P(doom) question isn&rsquo;t entirely misguided, though. If your interlocutor specifies exactly what they mean by P(doom), say P(gradual disempowerment from power-seeking AI within the next decade|no regulations), and ask for the rough shape of your PDF, then your answer immediately becomes more informative. By asking for a small set of well-chosen estimates, you could get a fairly accurate idea of someone&rsquo;s core beliefs. But again, you have to pick the right estimates.</p>
<p>Finding these estimates is like asking for the relevant dimensions in a &ldquo;political spectrum&rdquo; of views on AI safety. If you were to visualise opinions within the AI safety space, what would be your axes? While such a plot would necessarily be a simplification, perhaps it could allow us to communicate our basic assumptions more effectively. This would lead to more well-informed discussions in the AI safety community.</p>
<p>I imagine we want something between the P(doom) question and the kinds of questions used in expert surveys, like the <a href="https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things">2023 Expert Survey on Progress in AI</a> or the <a href="https://www.iaps.ai/research/ai-reliability-survey">AI Reliability &amp; Security Research Priorities</a>. While the P(doom) question is too simple, the questionnaire questions are too complicated<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. We&rsquo;re looking for questions that are as simple as possible, but no simpler - the kinds of questions you could answer at a cocktail party.</p>
<p>Here are the five questions I wish people would have asked me, rather than asking for my P(doom). For some obvious variations, see the footnotes.</p>
<ul>
<li>AI timelines<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>: In what year will we have <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/#id-1-defining-transformative-artificial-intelligence-transformative-ai">transformative AI</a>, i.e. AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>?</li>
<li>A more informative P(doom)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>: Assuming no further regulations on the development of AI systems, what is the probability of <a href="https://gradual-disempowerment.ai/">gradual disempowerment</a> from AI systems before 2050?</li>
<li>Threat model: Do the main risks from transformative AI come from bad actors developing <a href="https://www.forethought.org/research/preparing-for-the-intelligence-explosion#highly-destructive-technologies">destructive technologies</a> and creating <a href="https://www.forethought.org/research/preparing-for-the-intelligence-explosion#power-concentrating-mechanisms">power-concentrating mechanisms</a> or from AI systems seeking to eliminate humanity?</li>
<li>Views on AI slowdown<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>: How heavily should the government regulate the development of future AI systems?</li>
<li>Views on centralisation<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>: Should all leading AI companies be required to open-source their models, to ensure equal access to our most powerful AI systems?</li>
</ul>
<p>These questions translate naturally into scales from $-1$ to $1$. I also tried listing the questions in rough order of importance, so I&rsquo;d use the three first questions for the axes of a 3D-plot.</p>
<p>Going through these questions and plotting your position relative to that of others can be amusing. However, it&rsquo;s also an instructive exercise. After all, these are important questions. Finally, I&rsquo;ve also found it pretty handy having default answers to these extremely difficult questions at cocktail parties.</p>
<p><em>Thanks to Agatha Duzan for feedback on this text.</em></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For example, &ldquo;Rate the extent to which you agree that resolving the core challenges of this sub-area and implementing the resulting solutions would significantly reduce the risk of severe harm (loss of &gt;100 lives or &gt;$10 billion in economic impact from AI&rdquo;, where a sub-area might be &ldquo;Ethics-aware training and fine-tuning: Research on learning from imperfect ethical datasets, applying ethics-aware data curation methods, and incorporating collective ethical principles into model design.&rdquo; Quite a mouthful.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Variations: In what year will AI be capable to automate 99% of fully remote jobs? In what year will we have artificial general intelligence (AGI) - an AI which can match or exceed the cognitive abilities of human beings across any task?&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>For an interesting discussion on this topic, see <a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines">this moderated discussion</a> between Ajeya Cotra, Daniel Kokotaljo and Ege Erdil.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Variations: Just modify the conditions, the definition of &ldquo;doom&rdquo; or the timeframe. Alternatively, what is the probability of AI having a net positive effect on the world in within the next 20 years?&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Variations: What might be the minimum sufficient intervention to prevent gradual disempowerment from AIs?&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Variations: Should leading AI labs be placed under state ownership?&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>, <a href="/tags/ai">ai</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright Â© 2025 Isabel Dahlgren
</footer>
</body>
</html>
