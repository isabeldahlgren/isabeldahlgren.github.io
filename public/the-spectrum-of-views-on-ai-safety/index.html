<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - https://isabeldahlgren.github.io/">
  <title>The spectrum of views on AI safety | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="The spectrum of views on AI safety">
  <meta name="twitter:description" content="I agree the concept of P(doom) is problematic. First, “doom” can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.
But perhaps we’re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they’re just interested in hearing general takes on AI safety, at least in my experience.">

<meta property="og:url" content="https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="The spectrum of views on AI safety">
  <meta property="og:description" content="I agree the concept of P(doom) is problematic. First, “doom” can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.
But perhaps we’re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they’re just interested in hearing general takes on AI safety, at least in my experience.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-06T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">
    <meta property="article:tag" content="Ai">


  <meta itemprop="name" content="The spectrum of views on AI safety">
  <meta itemprop="description" content="I agree the concept of P(doom) is problematic. First, “doom” can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.
But perhaps we’re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they’re just interested in hearing general takes on AI safety, at least in my experience.">
  <meta itemprop="datePublished" content="2025-07-06T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-07-06T00:00:00+00:00">
  <meta itemprop="wordCount" content="726">
  <meta itemprop="keywords" content="Ai-Alignment,Ai">
  <link rel="canonical" href="https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "The spectrum of views on AI safety",
    "headline": "The spectrum of views on AI safety",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eI agree the concept of \u003ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/P(doom)\u0022\u003eP(doom)\u003c\/a\u003e is problematic. First, \u0026ldquo;doom\u0026rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.\u003c\/p\u003e\n\u003cp\u003eBut perhaps we\u0026rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they\u0026rsquo;re just interested in hearing general takes on AI safety, at least in my experience.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https:\/\/isabeldahlgren.github.io\/the-spectrum-of-views-on-ai-safety\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-07-06T00:00:00.00Z",
    "datePublished": "2025-07-06T00:00:00.00Z",
    "dateModified": "2025-07-06T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "https://isabeldahlgren.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https:\/\/isabeldahlgren.github.io\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "https://isabeldahlgren.github.io/",
    "url" : "https:\/\/isabeldahlgren.github.io\/the-spectrum-of-views-on-ai-safety\/",
    "wordCount" : "726",
    "genre" : [ "ai-alignment" , "ai" ],
    "keywords" : [ "ai-alignment" , "ai" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">The spectrum of views on AI safety</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-07-06"> July 06, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>, <a href='/tags/ai'>ai</a>
        </p>
      

      <span itemprop="articleBody">
      <p>I agree the concept of <a href="https://en.wikipedia.org/wiki/P(doom)">P(doom)</a> is problematic. First, &ldquo;doom&rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.</p>
<p>But perhaps we&rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they&rsquo;re just interested in hearing general takes on AI safety, at least in my experience.</p>
<p>The P(doom) question isn&rsquo;t entirely misguided, though. If your interlocutor specifies exactly what they mean by P(doom), say P(gradual disempowerment from power-seeking AI within the next decade|no regulations), and ask for the rough shape of your PDF, then your answer immediately becomes more informative. By asking for a small set of well-chosen estimates, you could get a fairly accurate idea of someone&rsquo;s core beliefs. But again, you have to pick the right estimates.</p>
<p>Finding these estimates is like asking for the relevant dimensions in a &ldquo;political spectrum&rdquo; of views on AI safety. If you were to visualise opinions within the AI safety space, what would be your axes? While such a plot would necessarily be a simplification, perhaps it could allow us to communicate our basic assumptions more effectively. This would lead to more well-informed discussions in the AI safety community.</p>
<p>I imagine we want something between the P(doom) question and the kinds of questions used in expert surveys, like the <a href="https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things">2023 Expert Survey on Progress in AI</a> or the <a href="https://www.iaps.ai/research/ai-reliability-survey">AI Reliability &amp; Security Research Priorities</a>. While the P(doom) question is too simple, the questionnaire questions are too complicated<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. We&rsquo;re looking for questions that are as simple as possible, but no simpler - the kinds of questions you could answer at a cocktail party.</p>
<p>Here are the five questions I wish people would have asked me, rather than asking for my P(doom). For some obvious variations, see the footnotes.</p>
<ul>
<li>AI timelines<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>: In what year will we have <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/#id-1-defining-transformative-artificial-intelligence-transformative-ai">transformative AI</a>, i.e. AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>?</li>
<li>A more informative P(doom)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>: Assuming no further regulations on the development of AI systems, what is the probability of <a href="https://gradual-disempowerment.ai/">gradual disempowerment</a> from AI systems before 2050?</li>
<li>Threat model: Do the main risks from transformative AI come from bad actors developing <a href="https://www.forethought.org/research/preparing-for-the-intelligence-explosion#highly-destructive-technologies">destructive technologies</a> and creating <a href="https://www.forethought.org/research/preparing-for-the-intelligence-explosion#power-concentrating-mechanisms">power-concentrating mechanisms</a> or from AI systems seeking to eliminate humanity?</li>
<li>Views on AI slowdown<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>: How heavily should the government regulate the development of future AI systems?</li>
<li>Views on centralisation<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>: Should all leading AI companies be required to open-source their models, to ensure equal access to our most powerful AI systems?</li>
</ul>
<p>These questions translate naturally into scales from $-1$ to $1$. I also tried listing the questions in rough order of importance, so I&rsquo;d use the three first questions for the axes of a 3D-plot.</p>
<p>Going through these questions and plotting your position relative to that of others can be amusing. However, it&rsquo;s also an instructive exercise. After all, these are important questions. Finally, I&rsquo;ve also found it pretty handy having default answers to these extremely difficult questions at cocktail parties.</p>
<p><em>Thanks to Agatha Duzan for feedback on this text.</em></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For example, &ldquo;Rate the extent to which you agree that resolving the core challenges of this sub-area and implementing the resulting solutions would significantly reduce the risk of severe harm (loss of &gt;100 lives or &gt;$10 billion in economic impact from AI&rdquo;, where a sub-area might be &ldquo;Ethics-aware training and fine-tuning: Research on learning from imperfect ethical datasets, applying ethics-aware data curation methods, and incorporating collective ethical principles into model design.&rdquo; Quite a mouthful.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Variations: In what year will AI be capable to automate 99% of fully remote jobs? In what year will we have artificial general intelligence (AGI) - an AI which can match or exceed the cognitive abilities of human beings across any task?&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>For an interesting discussion on this topic, see <a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines">this moderated discussion</a> between Ajeya Cotra, Daniel Kokotaljo and Ege Erdil.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Variations: Just modify the conditions, the definition of &ldquo;doom&rdquo; or the timeframe. Alternatively, what is the probability of AI having a net positive effect on the world in within the next 20 years?&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Variations: What might be the minimum sufficient intervention to prevent gradual disempowerment from AIs?&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Variations: Should leading AI labs be placed under state ownership?&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
