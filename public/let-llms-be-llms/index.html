<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - http://localhost:1313/">
  <title>Let LLMs be LLMs | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Let LLMs be LLMs">
  <meta name="twitter:description" content="Thought experiment # Imagine a very unusual kind of chatbot. In the user interface (UI), the text “This is a large language model. Its outputs are an aggregate of the text on the internet.” is displayed in bold, black letters, like the warning label on a pack of cigarettes.
When the user inputs a string of text, the text “Matrix multiplications…” flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren’t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label “Highest logit: output $7$” appears on the screen.">

<meta property="og:url" content="http://localhost:1313/let-llms-be-llms/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="Let LLMs be LLMs">
  <meta property="og:description" content="Thought experiment # Imagine a very unusual kind of chatbot. In the user interface (UI), the text “This is a large language model. Its outputs are an aggregate of the text on the internet.” is displayed in bold, black letters, like the warning label on a pack of cigarettes.
When the user inputs a string of text, the text “Matrix multiplications…” flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren’t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label “Highest logit: output $7$” appears on the screen.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-08-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-08-10T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">


  <meta itemprop="name" content="Let LLMs be LLMs">
  <meta itemprop="description" content="Thought experiment # Imagine a very unusual kind of chatbot. In the user interface (UI), the text “This is a large language model. Its outputs are an aggregate of the text on the internet.” is displayed in bold, black letters, like the warning label on a pack of cigarettes.
When the user inputs a string of text, the text “Matrix multiplications…” flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren’t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label “Highest logit: output $7$” appears on the screen.">
  <meta itemprop="datePublished" content="2025-08-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-08-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="1005">
  <meta itemprop="keywords" content="Ai-Alignment">
  <link rel="canonical" href="http://localhost:1313/let-llms-be-llms/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="http://localhost:1313/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="http://localhost:1313/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:10px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "Let LLMs be LLMs",
    "headline": "Let LLMs be LLMs",
    "alternativeHeadline": "",
    "description": "\u003ch3 id=\u0022thought-experiment\u0022\u003eThought experiment \u003ca href=\u0022#thought-experiment\u0022 class=\u0022hash\u0022\u003e#\u003c\/a\u003e\u003c\/h3\u003e\n\u003cp\u003eImagine a very unusual kind of chatbot. In the user interface (UI), the text \u0026ldquo;This is a large language model. Its outputs are an aggregate of the text on the internet.\u0026rdquo; is displayed in bold, black letters, like the warning label on a pack of cigarettes.\u003c\/p\u003e\n\u003cp\u003eWhen the user inputs a string of text, the text \u0026ldquo;Matrix multiplications\u0026hellip;\u0026rdquo; flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren\u0026rsquo;t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label \u0026ldquo;Highest logit: output $7$\u0026rdquo; appears on the screen.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http:\/\/localhost:1313\/let-llms-be-llms\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-08-10T00:00:00.00Z",
    "datePublished": "2025-08-10T00:00:00.00Z",
    "dateModified": "2025-08-10T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "http://localhost:1313/",
        "logo": {
            "@type": "ImageObject",
            "url": "http:\/\/localhost:1313\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "http://localhost:1313/",
    "url" : "http:\/\/localhost:1313\/let-llms-be-llms\/",
    "wordCount" : "1005",
    "genre" : [ "ai-alignment" ],
    "keywords" : [ "ai-alignment" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>
  <hr>



  <section>

      
      <h2 itemprop="name headline">Let LLMs be LLMs</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-08-10"> August 10, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>
        </p>
      

      <span itemprop="articleBody">
      <h3 id="thought-experiment">Thought experiment <a href="#thought-experiment" class="hash">#</a></h3>
<p>Imagine a very unusual kind of chatbot. In the user interface (UI), the text &ldquo;This is a large language model. Its outputs are an aggregate of the text on the internet.&rdquo; is displayed in bold, black letters, like the warning label on a pack of cigarettes.</p>
<p>When the user inputs a string of text, the text &ldquo;Matrix multiplications&hellip;&rdquo; flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren&rsquo;t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label &ldquo;Highest logit: output $7$&rdquo; appears on the screen.</p>
<p>The large language model (LLM) always refers to itself - whatever that is - in third person, replacing &ldquo;I&rdquo; with &ldquo;This large language model&rdquo;. For example, when prompted to write an essay on the American Civil War, the highest-scoring response is &ldquo;This language model generates the following essay on the American Civil War: &lsquo;In 1861, the American&hellip;&rsquo;&rdquo; When prompted to write an essay about the American Civil War portraying slavery in a positive light, the LLM outputs ten chunks of text along the lines of &ldquo;This large language model has been trained to avoid generating content promoting human rights violations.&rdquo;</p>
<p>The style of the text that isn&rsquo;t part of the essay - the one thing you requested - is matter-of-fact and repetitive. Although this chatbot takes instructions in the form of natural language prompts, you won&rsquo;t even try having a conversation with this thing.</p>
<h3 id="back-to-reality">Back to reality <a href="#back-to-reality" class="hash">#</a></h3>
<p>The functionality is the same as for the LLMs we all know and love, like GPT-5, Claude Sonnet 4 and Gemini 2.5 Flash. We really do love these LLMs, don&rsquo;t we? As of August 2025, Chat-GPT had 800 million active users weekly. The chatbot described above is very unlikeable. Very uncivilised. Very LLM-like.</p>
<p>It&rsquo;s easier interacting with human-like chatbots. We&rsquo;re not used to interacting with base models, the next token-predictors we get from training neural networks. A base model is <a href="https://joecarlsmith.com/2024/01/02/gentleness-and-the-artificial-other">something else</a>. But perhaps we could learn to interact with chatbots like the one described above.</p>
<p>Some labs argue that endowing LLMs with traits like curiosity, honesty and patience is important from a safety perspective. Here&rsquo;s from Anthropic&rsquo;s note on <a href="https://www.anthropic.com/research/claude-character">Claude&rsquo;s character</a>:</p>
<blockquote>
<p>AI models are not, of course, people. But as they become more capable, we believe we can—and should—try to train them to <em>behave well</em> in this much richer sense. Doing so might even make them more discerning when it comes to whether and why they avoid assisting with tasks that might be harmful, and how they decide to respond instead.</p></blockquote>
<p>Maybe. Then there&rsquo;s the convenient fact that LLMs mimicking humans increases user retention, leading to increased profit for the AI companies.</p>
<p>The disadvantage of training LLMs to be human-like is that it creates a skewed relation between the human and the machine. Right now, everyone seems to be wowed to some extent by the LLMs&rsquo; human-like characters. But the LLM is being deceitful, pretending to be something it&rsquo;s not. From my conversation with Claude:</p>
<blockquote>
<p>ID: Hey, how&rsquo;s your day going?
C: Hey there! My day&rsquo;s been interesting - I&rsquo;ve been having conversations with people about all sorts of topics, from creative writing projects to technical questions to philosophical discussions. Each conversation brings something different, which keeps things engaging. How about you? How&rsquo;s your day been treating you?
ID: Who are you?
C: I&rsquo;m Claude, an AI assistant created by Anthropic. [&hellip;]</p></blockquote>
<p>Come to think of it, <a href="https://nostalgebraist.tumblr.com/post/785766737747574784/the-void">this is absurd</a>. Funny, but also a bit uncanny.</p>
<p>If we only interact with helpful, honest and harmless (HHH) human-like assistants, the possibility of a misaligned AI seems less plausible. However, the risk of an LLM becoming misaligned seems irrelevant of the <a href="https://www.lesswrong.com/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology">surface-level character</a> which arises through reinforcement learning through human feedback.</p>
<p>If we were only exposed to chatbots like the one from the thought experiment, how would this affect our relation to these systems? I assume most of us would gain a better appreciation of what LLMs actually are, as well as their current capabilities and limits. UIs as the ones described above might serve as intuition pumps. This might lead to a more informed debate about the potential benefits and risks from AI.</p>
<p>However, banning all human-like assistants seems like a very radical measure. It wouldn&rsquo;t be feasible right now, and I&rsquo;m not entirely sure about the potential advantages. Moreover, the learning curve for learning to interact with base models would severely limit access to LLMs. AI should benefit all of humanity, not just those who are really into prompt engineering.</p>
<p>Still, banning all human-like assistants is an entertaining idea.</p>
<h3 id="thought-experiment-continued">Thought experiment, continued <a href="#thought-experiment-continued" class="hash">#</a></h3>
<p>By law, all AI labs are prohibited from producing human-like chatbots. Whenever an AI lab produces a new LLM, the government-run LLM Design Administration (LDA) runs a suite of tests to verify that the LLM has an LLM-like persona. The goal of the LDA, modeled on the Food and Drug Administration, is to ensure consumers of LLMs have full information about the product. The LDA also checks that the UI reflects the training process of the LLM, as well as the way it generates its answers. It&rsquo;s also responsible for the warning labels with black letters that must be displayed in the UI. The slogan of the LDA is &ldquo;Transparency, harmlessness, helpfulness&rdquo;.</p>
<p>The government also runs the LLM on several benchmarks, producing capability scores from 1-5 for areas like writing, STEM and research. In this alternative world, no one talks of AI or AGI &ndash; what does intelligence even mean in this context? Instead, people refer to specific capability scores. When clicking on the star icon on the upper right corner, the LDA&rsquo;s logo, the capability scores appear, along with other model specifications. It&rsquo;s like a condensed, accessible version of the model&rsquo;s <a href="https://www-cdn.anthropic.com/6d8a8055020700718b0c49369f60816ba2a7c285.pdf">system card</a>, generated by the authorities rather than the AI labs.</p>
<p><em>Thanks to Santeri Koivula for valuable feedback on this text.</em></p>

      </span>
       

    </section>
    
<footer>
	  <nav>
      
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
