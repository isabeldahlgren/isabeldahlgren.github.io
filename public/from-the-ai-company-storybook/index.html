<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/from-the-ai-company-storybook/" />
    <title>From the AI company storybook</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>From the AI company storybook</h1>
        <div>
                <time>August 31, 2025</time>
            </div>
    </header><p>AI companies are companies. The leading AI companies don&rsquo;t want to be seen as companies, though. They call themselves AI labs. They have researchy names, like DeepMind and Meta AI. The best name: OpenAI. Almost sounds like a real <a href="https://openai.com/index/introducing-openai/">non-profit</a>.</p>
<p>Most AI companies are products of Silicon Valley. Their leaders aren&rsquo;t professors, but seasoned business executives. And this is a good thing! These companies wouldn&rsquo;t produce nearly as much consumer value if they were led by researchers with no industry experience. Moreover, unlike normal research labs, AI companies need to make money, just as any other company.</p>
<p>All companies tell narratives about themselves. And the leading AI companies are pretty good at it &ndash; these companies attract top talent, so their marketing teams can basically hire the world&rsquo;s best story-tellers. Here are five narratives being aware of.</p>
<p>Before we begin, an apology. I&rsquo;m deliberately exaggerating these narratives, just to make them clearer<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Concretely, I&rsquo;ll be cherry-picking quotes from company leaders and generalise, like a lot. Overall, I think these companies bring huge consumer value and have the potential to radically improve the future. But the focus of this article is on the problems of these narratives.</p>
<p>Now, story time.</p>
<h3 id="well-build-agi-soon">&ldquo;We&rsquo;ll build AGI soon&rdquo;</h3>
<p>AI company CEOs have famously <a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">short AI timelines</a>. For example, in January this year, Sam Altman declared that &ldquo;We are now confident we know how to build AGI as we have traditionally understood it.&rdquo; Dario Amodei, in the same month: &ldquo;I’m more confident than I’ve ever been that we’re close to powerful capabilities… in the next 2-3 years&rdquo;.</p>
<p>Of course, no one would invest in your company if you needed two decades to develop your product.</p>
<p>But the term AGI and is fuzzy<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, and it keeps on changing as models evolve. Funny because it&rsquo;s kinda true: &ldquo;AGI is whatever machines can&rsquo;t do now&rdquo;.</p>
<p>In the end, these kinds of statements about future model capabilities become pretty uninformative.</p>
<h3 id="agi-is-inevitable">&ldquo;AGI is inevitable&rdquo;</h3>
<p>The AI companies also want technological progress to seem inevitable. If you cannot guarantee AGI, why would you fund them?</p>
<p>But this narrative isn&rsquo;t just for outsiders. Ilya Sutskever, former chief scientist at OpenAI, used to chant &ldquo;Feel the AGI&rdquo; at company parties, perhaps to improve company culture<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>Similarly, if AGI is inevitable, an AI company can justify advancing model capabilities as quickly as possible &ndash; possibly at the cost of safety &ndash; so that none of the bad guys build it first.</p>
<h3 id="were-the-good-guys">&ldquo;We&rsquo;re the good guys&rdquo;</h3>
<p>Indeed, as highlighted in <a href="https://en.wikipedia.org/wiki/Empire_of_AI">Empire of AI</a>, some AI companies seem to think of themselves as &ldquo;the good guys&rdquo; and other AI companies as &ldquo;the bad guys&rdquo;. For example, OpenAI was founded was because Musk was concerned about the ethical implications of Google&rsquo;s acquisition of DeepMind. Anthropic was founded by former OpenAI employees who were dissatisfied with the safety work at OpenAI.</p>
<p>Moreover, the US-China tensions lead to a divide between the American AI companies and the Chinese ones.</p>
<h3 id="our-tools-empower-all-of-humanity">&ldquo;Our tools empower all of humanity&rdquo;</h3>
<p>DeepMind&rsquo;s mission is to &ldquo;build AI responsibly to benefit humanity&rdquo;, while OpenAI&rsquo;s mission is to &ldquo;ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity&rdquo;.</p>
<p>The democratisation bit of the mission isn&rsquo;t specific to AI companies; it&rsquo;s a tech company cliché. For example, Facebook is supposed to give everyone the opportunity to connect you with friends and the rest of the world.</p>
<p>AI will empower all of humanity, but only if everyone can afford a laptop, has stable internet connection and knows how to use LLMs efficiently. Moreover, prices for premium subscription plans are relatively high. OpenAI&rsquo;s cheapest paid plan comes at 20 dollars per month. For Anthropic, the figure is 17 dollars.</p>
<p>The point about using LLMs efficiently is subtle. To have an LLM perform more advanced tasks, which is what would be empowering for real, you&rsquo;d need to be good at prompt engineering. Becoming good at prompt engineer takes time.</p>
<h3 id="agi-will-solve-all-our-problems">&ldquo;AGI will solve all our problems&rdquo;</h3>
<p>Another reason they need to build AGI as soon as possible is their belief that AGI can solve most of humanity&rsquo;s problems. In short, techno-optimism. In <a href="https://www.penguin.co.uk/books/437020/genius-makers-by-metz-cade/9781847942159">Genius Makers</a>, Cade Metz likens the belief in AGI to a religion. And a religion needs a promise of salvation.</p>
<p>In <a href="https://www.darioamodei.com/essay/machines-of-loving-grace">Machines of Loving Grace</a>, Dario Amodei writes that &ldquo;it’s my guess that powerful AI could at least 10x the rate of these discoveries, giving us the next 50-100 years of biological progress in 5-10 years&rdquo;.</p>
<p>The idea of AGI radically accelerating scientific progress is pretty mainstream. For example, <a href="https://www.forethought.org/research/preparing-for-the-intelligence-explosion#the-accelerated-decade">this report</a> from Forethought argues that we might very well see a century&rsquo;s worth of technological progress in a decade.</p>
<p>But human progress isn&rsquo;t the same as scientific progress. We need cultural and moral progress too.</p>
<h3 id="the-end">The end</h3>
<p>Whom are these stories meant for?</p>
<p>In short, for everyone. Some stories serve as sales pitches for investors and customers. Others are meant to improve employee morale and create a sense of unity. Regulators learn that they shouldn&rsquo;t regulate the deployment of new AI models, just so their nation won&rsquo;t fall behind in the AI arms race.</p>
<p>AI companies find themselves in a technological race, much like the Apollo program. If you build AGI first, you&rsquo;ll become Neil Armstrong. You don&rsquo;t want to be &ndash; I had to Google his name &ndash; Buzz Aldrin. However, this isn&rsquo;t just a race between great powers, but also a race between tech billionaires who don&rsquo;t &ldquo;just&rdquo; want to be average tech billionaires. These people want to be remembered as visionaries. So perhaps these stories are mostly meant for posterity.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>It&rsquo;s a bit like using the extremal principle from mathematics.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Everyone seems to agree the term &ldquo;AGI&rdquo; is problematic, even the AI company CEOs. See e.g. <a href="https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework">here</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>As an outsider, I cannot help but think that this damages company culture, though.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai">ai</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright © 2025 Isabel Dahlgren
</footer>
</body>
</html>
