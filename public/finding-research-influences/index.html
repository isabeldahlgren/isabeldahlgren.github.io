<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - https://isabeldahlgren.github.io/">
  <title>Finding research influences | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Finding research influences">
  <meta name="twitter:description" content="There isn’t any expert consensus on many key questions related to AI safety. For example, estimates of when we’ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.
I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are Buck Shleregis, Holden Karnofsky and Jan Kulveit. While I don’t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.">

<meta property="og:url" content="https://isabeldahlgren.github.io/finding-research-influences/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="Finding research influences">
  <meta property="og:description" content="There isn’t any expert consensus on many key questions related to AI safety. For example, estimates of when we’ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.
I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are Buck Shleregis, Holden Karnofsky and Jan Kulveit. While I don’t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-20T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">


  <meta itemprop="name" content="Finding research influences">
  <meta itemprop="description" content="There isn’t any expert consensus on many key questions related to AI safety. For example, estimates of when we’ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.
I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are Buck Shleregis, Holden Karnofsky and Jan Kulveit. While I don’t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.">
  <meta itemprop="datePublished" content="2025-07-20T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-07-20T00:00:00+00:00">
  <meta itemprop="wordCount" content="762">
  <meta itemprop="keywords" content="Ai-Alignment">
  <link rel="canonical" href="https://isabeldahlgren.github.io/finding-research-influences/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "Finding research influences",
    "headline": "Finding research influences",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eThere isn\u0026rsquo;t any expert consensus on many \u003ca href=\u0022https:\/\/isabeldahlgren.github.io\/the-spectrum-of-views-on-ai-safety\/\u0022\u003ekey questions related to AI safety\u003c\/a\u003e. For example, estimates of when we\u0026rsquo;ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.\u003c\/p\u003e\n\u003cp\u003eI think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are \u003ca href=\u0022https:\/\/substack.com\/@redwoodresearch?utm_source=about-page\u0022\u003eBuck Shleregis\u003c\/a\u003e, \u003ca href=\u0022https:\/\/www.cold-takes.com\/cold-takes-on-ai\/\u0022\u003eHolden Karnofsky\u003c\/a\u003e and \u003ca href=\u0022https:\/\/www.lesswrong.com\/users\/jan_kulveit?from=search_autocomplete\u0022\u003eJan Kulveit\u003c\/a\u003e. While I don\u0026rsquo;t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https:\/\/isabeldahlgren.github.io\/finding-research-influences\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-07-20T00:00:00.00Z",
    "datePublished": "2025-07-20T00:00:00.00Z",
    "dateModified": "2025-07-20T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "https://isabeldahlgren.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https:\/\/isabeldahlgren.github.io\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "https://isabeldahlgren.github.io/",
    "url" : "https:\/\/isabeldahlgren.github.io\/finding-research-influences\/",
    "wordCount" : "762",
    "genre" : [ "ai-alignment" ],
    "keywords" : [ "ai-alignment" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">Finding research influences</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-07-20"> July 20, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>
        </p>
      

      <span itemprop="articleBody">
      <p>There isn&rsquo;t any expert consensus on many <a href="https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/">key questions related to AI safety</a>. For example, estimates of when we&rsquo;ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.</p>
<p>I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are <a href="https://substack.com/@redwoodresearch?utm_source=about-page">Buck Shleregis</a>, <a href="https://www.cold-takes.com/cold-takes-on-ai/">Holden Karnofsky</a> and <a href="https://www.lesswrong.com/users/jan_kulveit?from=search_autocomplete">Jan Kulveit</a>. While I don&rsquo;t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.</p>
<p>I&rsquo;ve never been on looking for new research influences. Every now and then, I just realise that I&rsquo;ve been influenced by someone, perhaps after citing the work of someone for the third time in a conversation. But say you want to look for research influences more deliberately. How might you proceed?</p>
<h3 id="knowing-where-to-look">Knowing where to look <a href="#knowing-where-to-look" class="hash">#</a></h3>
<p>A reasonable first step is exploring new content.</p>
<p>A reasonable first substep, then, is to narrow down the search space. Identify the kinds of questions you care about. For example, I&rsquo;m mostly interested in reading about AI control and LLM psychology right now, so I&rsquo;ll ignore papers and blog posts on, say, singular learning theory.</p>
<p>Next, ask people you find sensible for reading recommendations. Better yet, ask if they have any research influences. This is one of those things which is infinitely easier doing in person. Sending cold emails to researchers usually works, but it&rsquo;s relatively time-consuming. It&rsquo;s much easier bringing up the topic over a coffee with people in your local community.</p>
<p>Asking &ldquo;Which blog posts have had the largest influence on your research?&rdquo; also proved a good way of rounding up conversations at EAG. This way, I got to know the other person better and exploring new content at once.</p>
<h3 id="having-a-nose-for-bullshit">Having a nose for bullshit <a href="#having-a-nose-for-bullshit" class="hash">#</a></h3>
<p>Once you&rsquo;ve decided what to read, you want to scrutinise the argument of the text. This is a highly non-trivial task. I&rsquo;m not going to try solving all of philosophy here, so I&rsquo;ll just focus on heuristics for detecting bullshit in the context of AI safety.</p>
<p>First, beware any kind of extreme. Is the proposed idea radical? Radical ideas shouldn&rsquo;t be dismissed offhand. However, the burden of proof is greater. Similarly, quickly screen the author&rsquo;s background: is the author known to have radical opinions, or affiliated with an organisation pursuing an unusual agenda?</p>
<p>I find it especially troubling when authors promote radical opinions and are unwilling to engage in debate with the general public. This leads to echo chambers. Moreover, refusing to explain your ideas to laymen just seems uncool. Just as lecturers should take questions from students seriously, authors should take questions from the non-initiated seriously. It&rsquo;s an act of charity.</p>
<p>Another helpful strategy is to listen to interviews with the author. It&rsquo;s harder lying in speech than in writing<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Of course, not everyone is as persuasive orally. But if there&rsquo;s a big discrepancy between your confidence in the author&rsquo;s argument as presented in the text and during an interview, that&rsquo;s a warning sign. Moreover, a good interviewer will also help expose the flaws in the interviewee&rsquo;s reasoning. In an essay, the author has full control.</p>
<h3 id="observing-the-influence">Observing the influence <a href="#observing-the-influence" class="hash">#</a></h3>
<p>Suppose you come across an author whose work makes sense but leaves you feeling &ldquo;Sure, so what?&rdquo;. I wouldn&rsquo;t speak of a research influence here. A research influence changes the way you think. It&rsquo;s not enough just stating true facts; their work needs to have some oomph.</p>
<p>When can we speak of a research influence, then?</p>
<p>One reliable proxy is <a href="https://www.paulgraham.com/top.html">the top idea of your mind</a>. Do you have shower thoughts about their work? Also, do you find yourself coming back to their work after several months? In particular, when revisiting their points, do they still make as much sense? Big ideas need to be slept on, and you can only sleep so many times in a given week. Lastly, notice if you reference their ideas when chatting with others and, if so, in what way.</p>
<p>So finding research influences takes time, even if you take some of the shortcuts listed above. The process of finding research influences very much resembles the process of doing research. In fact, perhaps the two are indistinguishable.</p>
<p><em>Thanks to Miles Kodama for valuable discussions on this topic.</em></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>A fact well known among those who have taken oral exams.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
