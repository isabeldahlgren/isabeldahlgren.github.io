<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Isabel Dahlgren</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Isabel Dahlgren</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 20 Jul 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>The spectrum of views on AI safety</title>
      <link>http://localhost:1313/the-spectrum-of-views-on-ai-safety/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/the-spectrum-of-views-on-ai-safety/</guid>
      <description>&lt;p&gt;I agree the concept of &lt;a href=&#34;https://en.wikipedia.org/wiki/P(doom)&#34;&gt;P(doom)&lt;/a&gt; is problematic. First, &amp;ldquo;doom&amp;rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.&lt;/p&gt;
&lt;p&gt;But perhaps we&amp;rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they&amp;rsquo;re just interested in hearing general takes on AI safety, at least in my experience.&lt;/p&gt;
&lt;p&gt;The P(doom) question isn&amp;rsquo;t entirely misguided, though. If your interlocutor specifies exactly what they mean by P(doom), say P(gradual disempowerment from power-seeking AI within the next decade|no regulations), and ask for the rough shape of your PDF, then your answer immediately becomes more informative. By asking for a small set of well-chosen estimates, you could get a fairly accurate idea of someone&amp;rsquo;s core beliefs. But again, you have to pick the right estimates.&lt;/p&gt;
&lt;p&gt;Finding these estimates is like asking for the relevant dimensions in a &amp;ldquo;political spectrum&amp;rdquo; of views on AI safety. If you were to visualise opinions within the AI safety space, what would be your axes? While such a plot would necessarily be a simplification, perhaps it could allow us to communicate our basic assumptions more effectively. This would lead to more well-informed discussions in the AI safety community.&lt;/p&gt;
&lt;p&gt;I imagine we want something between the P(doom) question and the kinds of questions used in expert surveys, like the &lt;a href=&#34;https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things&#34;&gt;2023 Expert Survey on Progress in AI&lt;/a&gt; or the &lt;a href=&#34;https://www.iaps.ai/research/ai-reliability-survey&#34;&gt;AI Reliability &amp;amp; Security Research Priorities&lt;/a&gt;. While the P(doom) question is too simple, the questionnaire questions are too complicated&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. We&amp;rsquo;re looking for questions that are as simple as possible, but no simpler - the kinds of questions you could answer at a cocktail party.&lt;/p&gt;
&lt;p&gt;Here are the five questions I wish people would have asked me, rather than asking for my P(doom). For some obvious variations, see the footnotes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI timelines&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;: In what year will we have &lt;a href=&#34;https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/#id-1-defining-transformative-artificial-intelligence-transformative-ai&#34;&gt;transformative AI&lt;/a&gt;, i.e. AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;?&lt;/li&gt;
&lt;li&gt;A more informative P(doom)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;: Assuming no further regulations on the development of AI systems, what is the probability of &lt;a href=&#34;https://gradual-disempowerment.ai/&#34;&gt;gradual disempowerment&lt;/a&gt; from AI systems before 2050?&lt;/li&gt;
&lt;li&gt;Threat model: Do the main risks from transformative AI come from bad actors developing &lt;a href=&#34;https://www.forethought.org/research/preparing-for-the-intelligence-explosion#highly-destructive-technologies&#34;&gt;destructive technologies&lt;/a&gt; and creating &lt;a href=&#34;https://www.forethought.org/research/preparing-for-the-intelligence-explosion#power-concentrating-mechanisms&#34;&gt;power-concentrating mechanisms&lt;/a&gt; or from AI systems seeking to eliminate humanity?&lt;/li&gt;
&lt;li&gt;Views on AI slowdown&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;: How heavily should the government regulate the development of future AI systems?&lt;/li&gt;
&lt;li&gt;Views on centralisation&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;: Should all leading AI companies be required to open-source their models, to ensure equal access to our most powerful AI systems?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These questions translate naturally into scales from $-1$ to $1$. I also tried listing the questions in rough order of importance, so I&amp;rsquo;d use the three first questions for the axes of a 3D-plot.&lt;/p&gt;
&lt;p&gt;Going through these questions and plotting your position relative to that of others can be amusing. However, it&amp;rsquo;s also an instructive exercise. After all, these questions matter. Finally, I&amp;rsquo;ve also found it pretty handy having default answers to these questions at cocktail parties.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;For example, &amp;ldquo;Rate the extent to which you agree that resolving the core challenges of this sub-area and implementing the resulting solutions would significantly reduce the risk of severe harm (loss of &amp;gt;100 lives or &amp;gt;$10 billion in economic impact from AI&amp;rdquo;, where a sub-area might be &amp;ldquo;Ethics-aware training and fine-tuning: Research on learning from imperfect ethical datasets, applying ethics-aware data curation methods, and incorporating collective ethical principles into model design.&amp;rdquo; Quite a mouthful.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Variations: In what year will AI be capable to automate 99% of fully remote jobs? In what year will we have artificial general intelligence (AGI) - an AI which can match or exceed the cognitive abilities of human beings across any task?&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;For an interesting discussion on this topic, see &lt;a href=&#34;https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines&#34;&gt;this moderated discussion&lt;/a&gt; between Ajeya Cotra, Daniel Kokotaljo and Ege Erdil.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;: Variations: Just modify the conditions, the definition of &amp;ldquo;doom&amp;rdquo; or the timeframe. Alternatively, what is the probability of AI having a net positive effect on the world in within the next 20 years?&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Variations: What might be the minimum sufficient intervention to prevent gradual disempowerment from AIs?&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Variations: Should leading AI labs be placed under state ownership?&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>On my relation to effective altruism</title>
      <link>http://localhost:1313/on-my-relation-to-effective-altruism/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/on-my-relation-to-effective-altruism/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve spent more time engaging with the effective altruist (EA) community this year. Not just reading EA books and blog posts, but participating in seminars, attending conferences and going on EA retreats. For context, I&amp;rsquo;d viewed myself &amp;ldquo;EA adjacent&amp;rdquo; ever since I came across &lt;em&gt;The Life You Can Save&lt;/em&gt; back in high school. However, during my master degree, the prospect of graduating soon - of becoming an adult - made me reflect more carefully on EA.&lt;/p&gt;
&lt;p&gt;First, our definitions, taken from the &lt;a href=&#34;https://www.effectivealtruism.org/articles/introduction-to-effective-altruism&#34;&gt;introduction to effective altruism&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Effective altruism is a project that aims to find the best ways to help others, and put them into practice.&lt;/p&gt;
&lt;p&gt;It’s both a &lt;strong&gt;research field&lt;/strong&gt;, which aims to identify the world’s most pressing problems and the best solutions to them, and a &lt;strong&gt;practical community&lt;/strong&gt; that aims to use those findings to do good.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When I refer to the EA community, I mean the practical community. I&amp;rsquo;ll also use the abbreviation &amp;ldquo;EA&amp;rdquo; to refer to effective altruists.&lt;/p&gt;
&lt;p&gt;Many people who have engaged with the EA community at some level or other often find themselves questioning their relationship with EA. In fact, there are plenty of blog posts on the theme &amp;ldquo;EA identity crisis&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here&amp;rsquo;s my contribution to the genre.&lt;/p&gt;
&lt;p&gt;So, do I consider myself part of the EA community?&lt;/p&gt;
&lt;h3 id=&#34;yes&#34;&gt;Yes&lt;/h3&gt;
&lt;p&gt;Technically, yes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I think the &lt;a href=&#34;https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#what-principles-unite-effective-altruism&#34;&gt;core ideas of EA&lt;/a&gt; - prioritisation, impartiality, open truthseeking and collaboration - make sense. The article does an excellent job at explaining these terms, so I&amp;rsquo;ll refer to their explanations&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. I&amp;rsquo;ve desperately tried red-teaming EA - I&amp;rsquo;d love for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Famine,_Affluence,_and_Morality&#34;&gt;drowning child argument&lt;/a&gt; to be less convincing - but I still think the core ideas hold up.&lt;/li&gt;
&lt;li&gt;I care about finding the best ways to help others, and putting them into practise. While I don&amp;rsquo;t want to impose any moral standards on others, I feel a certain moral obligation to do good&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. I also find it fulfilling working on high-impact projects. It almost seems tautological that one should try having a large positive impact on the world.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve attended several EA community events. All of these events have been tremendously valuable, both on a professional and a personal level.&lt;/li&gt;
&lt;li&gt;My interest in EA doesn&amp;rsquo;t seem to be &amp;ldquo;just a phase&amp;rdquo;. At this point, I&amp;rsquo;ve spent about five years reading and learning more about EA.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Maybe also non-technically? Here are some gut-feeling-level arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I enjoy exchanging ideas on EA-related topics with people in the EA community - it&amp;rsquo;s almost like a hobby of mine. But I also think there&amp;rsquo;s significant value in doing collaborative sensemaking on topics like existential risk, AGI timelines and longtermism.&lt;/li&gt;
&lt;li&gt;In general, I find many people in the EA community to be very thoughtful. Some pieces by Holden Karnofsky, Ajeya Cotra and Benjamin Todd have had a profound influence on my worldview. Similarly, some of my favourite non-fiction books are about EA. For example, I thought &lt;em&gt;Doing Good Better&lt;/em&gt; and &lt;em&gt;What We Owe The Future&lt;/em&gt; were exceptional reads.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;no&#34;&gt;No&lt;/h3&gt;
&lt;p&gt;But of course, it&amp;rsquo;s complicated&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I mainly care about existential risk reduction, although this may very well change in the future. I&amp;rsquo;ve also devoted much more time and effort to AI safety than any other cause area&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. It&amp;rsquo;d be more accurate saying I&amp;rsquo;m into AI safety and existential risk reduction rather than EA, which is a much broader term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I don&amp;rsquo;t seem to fit the public perception of an EA. In my experience, many people think all EAs work on farm animal welfare or global poverty reduction. Sure, I&amp;rsquo;m vegetarian (Peter Singer&amp;rsquo;s fault), but I still haven&amp;rsquo;t donated to GiveWell, nor signed the 10% pledge. This point is mostly about me not living up to my moral standards, though.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general, I try avoid identity markers related to social movements. While I think neutrality is somewhat of an illusion, I want to hold my opinions lightly. My worry is that self-identifying as an EA might make me less open-minded. But perhaps this worry is somewhat ungrounded, at least when it comes to EA. People in the EA community tend to be very open to criticism&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The EA community has its flaws, despite making a number of changes after FTX. Many of the concerns raised in &lt;a href=&#34;https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1&#34;&gt;this article&lt;/a&gt; are still valid. This excerpt summarises one of my main concerns well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The EA community is notoriously homogeneous, and the “average EA” is extremely easy to imagine: he is a white male in his twenties or thirties from an upper-middle class family in North America or Western Europe. He is ethically utilitarian and politically centrist; an atheist, but culturally protestant. He studied analytic philosophy, mathematics, computer science, or economics at an elite university in the US or UK. He is neurodivergent. He thinks space is really cool. He highly values intelligence, and believes that his own is significantly above average. He hung around LessWrong for a while as a teenager, and now wears EA-branded shirts and hoodies, drinks Huel, and consumes a narrow range of blogs, podcasts, and vegan ready-meals.&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resolution&#34;&gt;Resolution&lt;/h3&gt;
&lt;p&gt;While I do have some reservations, my views are pretty consistent with the EA agenda. Also, I&amp;rsquo;ll (reluctantly) admit that overthinking the question of whether you&amp;rsquo;re part of a given community is very EA.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://forum.effectivealtruism.org/posts/89GdH5unSb2Sze6kj/elements-of-ea-your-ea-identity-can-be-bespoke&#34;&gt;This post&lt;/a&gt; on EA identities is a good starting point. There&amp;rsquo;s also &lt;a href=&#34;https://www.neelnanda.io/blog/8-altruism&#34;&gt;Neel Nanda&amp;rsquo;s favourite blog post&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;I endorse a much broader form of the impartiality principle than the one outlined in the article. I also think strangers, animals and future people should be part of our &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Expanding_Circle&#34;&gt;circle of moral consideration&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;I assume this is because my mother, coming from the Philippines, always told me to be grateful. I&amp;rsquo;m glad she did, but I used to find it annoying as a child.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;For a list of cause areas, see the 80,000 hours list of &lt;a href=&#34;https://80000hours.org/problem-profiles/&#34;&gt;the world&amp;rsquo;s most pressing problems&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;See the posts in the &lt;a href=&#34;https://forum.effectivealtruism.org/topics/criticism-of-effective-altruism&#34;&gt;Criticism of effective altruism thread&lt;/a&gt;. I very much liked &lt;a href=&#34;https://www.benkuhn.net/ea-critique/&#34;&gt;Ben Kuhn&amp;rsquo;s critique of EA&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>On question taste</title>
      <link>http://localhost:1313/on-question-taste/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/on-question-taste/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I.&lt;/h3&gt;
&lt;p&gt;Children like asking questions. Some of their questions are very hard: &amp;ldquo;Why do people die?&amp;rdquo;. Others questions expose our biases: &amp;ldquo;Couldn&amp;rsquo;t we invite the homeless man to dinner?&amp;rdquo; Then there are all the annoying questions: &amp;ldquo;When will we arrive?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In my experience, school didn&amp;rsquo;t teach us to ask questions - we were just taught how to answer them. To pass the test, you only had to memorise the material in the textbook.&lt;/p&gt;
&lt;p&gt;Perhaps this isn&amp;rsquo;t too surprising. It&amp;rsquo;s hard objectively measuring someone&amp;rsquo;s ability to ask questions. But above all, educational institutions serve many different purposes, apart from spreading knowledge&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Schools should also produce good citizens. A society full of radical skeptics wouldn&amp;rsquo;t function properly (hence the execution of Socrates). So children might hear that some questions are meaningless or irrelevant, even in school.&lt;/p&gt;
&lt;h3 id=&#34;ii&#34;&gt;II.&lt;/h3&gt;
&lt;p&gt;On two different occasions, I&amp;rsquo;ve heard two distinguished professors at ETH mention that students ask too few questions. They weren&amp;rsquo;t referring to the specific questions about the lecture, but to something like &lt;a href=&#34;https://www.lesswrong.com/posts/Thwfy4gNFx9kHgvov/research-hamming-questions&#34;&gt;Research Hamming questions&lt;/a&gt;. For example, one could ask about improvements of a particular result or whether a proof technique generalises.&lt;/p&gt;
&lt;p&gt;They make a good point. The ability to ask interesting, open-ended questions seems like a key research skill; call it question taste. Intuitively, a person with question taste is like a child with expert domain knowledge.&lt;/p&gt;
&lt;p&gt;Certain groups of people are known for their question taste. For example, consider journalists. They have undergone extensive training in asking questions as part of their university degree. Moreover, they constantly practise the skill of asking questions at work. No wonder they get pretty good at it. Similarly, researchers, psychologists, essayists and investors tend to have good question taste, as questioning is part of their job.&lt;/p&gt;
&lt;h3 id=&#34;iii&#34;&gt;III.&lt;/h3&gt;
&lt;p&gt;I assume, perhaps somewhat naïvely, that acquiring question taste is mostly about deliberate practice. The implementational details will depend on the domain you&amp;rsquo;re working in, though.&lt;/p&gt;
&lt;p&gt;A basic strategy might be to identify the people in your area who seem to be asking the right questions and try copying what they do. For example, I&amp;rsquo;d like to formulate good questions about research agendas in AI safety someday, so I&amp;rsquo;ll often make note of questions raised by leading researchers in podcasts, blog posts or papers. Sometimes I write down own questions and discuss them with others for feedback, although this is more time-consuming.&lt;/p&gt;
&lt;p&gt;Having a safe environment in which to practise asking questions - a training ground - also seems important. If you&amp;rsquo;re a student, you can attend student-run seminars, where you might feel more comfortable asking questions. Regardless of the kind of questions you aim to ask, you need to find a community to give you feedback. And hopefully, you get some answers too.&lt;/p&gt;
&lt;p&gt;As you practise asking questions, you&amp;rsquo;ll inevitably ask some &amp;ldquo;bad questions&amp;rdquo;. Part of the reason why children are so inquisitive is their lack of self-consciousness. They don&amp;rsquo;t care if their questions make them sound stupid. So, in order to ask good questions, make a conscious effort, find a supportive community and maybe don&amp;rsquo;t take yourself too seriously.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This is argued in Bryan Caplan&amp;rsquo;s book &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Case_Against_Education&#34;&gt;The Case Against Education&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>A taxonomy of examples</title>
      <link>http://localhost:1313/a-taxonomy-of-examples/</link>
      <pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/a-taxonomy-of-examples/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m mildly obsessed with examples. Whenever I feel confused, it&amp;rsquo;s often because I don&amp;rsquo;t know enough examples. Proofs can be confusing too, if some step is poorly explained. But that kind of confusion tends to be local. You can still have a good grasp of the theory. If you don&amp;rsquo;t know enough examples, you feel generally lost. Textbooks lacking in examples end up being dry. In contrast, books with well-chosen examples are a pleasure to read&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;According to the Cambridge Dictionary, an example is &amp;ldquo;something that is typical of the group of things that it is a member of&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In mathematics, there are many kinds of examples, and they all serve different purposes. Traditionally, fields like topology and measure theory place a greater emphasis on counter examples. In algebra, there are more prototypical examples; definitions are often followed by an exhaustive list of objects of that category&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. In the context of mathematics, the dictionary definition is too simplistic! So here is an attempt at classifying the main kinds of examples, or, alternatively, an examples appreciation post.&lt;/p&gt;
&lt;h3 id=&#34;understanding-definitions&#34;&gt;Understanding definitions&lt;/h3&gt;
&lt;p&gt;First, there are the examples helping us understand definitions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prototypical examples: examples just complex enough to capture the essential properties of an object.
&lt;ul&gt;
&lt;li&gt;Fundamental groups: the fundamental group of the circle $\pi_1(\mathbb{S}^1, x_0)$.&lt;/li&gt;
&lt;li&gt;Transcendental field extensions: $\mathbb{Q}(\pi)$.&lt;/li&gt;
&lt;li&gt;Modular forms: the Poincaré series $P_{m, k}(z)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Canonical examples: the canonical construction of something - the construction you get which involves the least arbitrariness.
&lt;ul&gt;
&lt;li&gt;Stochastic processes: the coordinate process.&lt;/li&gt;
&lt;li&gt;Embeddings into bi-duals: take the embedding $V \to V^{**}$ sending $v$ to the evaluation.&lt;/li&gt;
&lt;li&gt;Rings: the integers $\mathbb{Z}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Foundational examples: examples of special interest and which form the basis for further theory.
&lt;ul&gt;
&lt;li&gt;Anything analysis: the Gaussian $\phi(x) = e^{-x^2/2}$.&lt;/li&gt;
&lt;li&gt;Stochastic processes: the simple random walk on $\mathbb{Z}$.&lt;/li&gt;
&lt;li&gt;Modular forms: the $j$–invariant.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Basic examples: the simplest possible instance of an object. The kind of example an uncreative student might cite in an exam&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.
&lt;ul&gt;
&lt;li&gt;Groups: the trivial group $G := {e}$.&lt;/li&gt;
&lt;li&gt;Martingales: the martingale $(X_t)_{t \ge 0}$ where $X_t \equiv 1$ for all $t$.&lt;/li&gt;
&lt;li&gt;Banach space: just take $\mathbb{R}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-examples: objects which are not instances of something.
&lt;ul&gt;
&lt;li&gt;A non-ring: the natural numbers $\mathbb{N}$.&lt;/li&gt;
&lt;li&gt;A non-tempered distribution: the function $e^t$.&lt;/li&gt;
&lt;li&gt;A non-Artinian ring: the integers $\mathbb{Z}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Computational examples: examples involving a computation and that illustrate how to work with a given object.
&lt;ul&gt;
&lt;li&gt;Compute $\mathbb{R}[x] \otimes \mathbb{C}$.&lt;/li&gt;
&lt;li&gt;Find the Euler product expansion for the Dirichlet generating series of the Möbius function $\mu = 1^{-*}$&lt;/li&gt;
&lt;li&gt;Check that $c^{-1/2} W_{ct}$ is again Brownian motion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pathological examples: examples illustrating distinctions between different notions, or the limitations of a certain concept.
&lt;ul&gt;
&lt;li&gt;Differentiability $\neq$ continuity: consider the Weierstrass function.&lt;/li&gt;
&lt;li&gt;Connectedness $\neq$ path connectedness: because the Topologist&amp;rsquo;s sine curve.&lt;/li&gt;
&lt;li&gt;Lebesgue integral $\neq$ Riemann integral: try integrating the Dirichlet function, the characteristic function of the rationals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Toy examples: examples you can manipulate to have some desired properties. When looking for counter examples, you typically start with some toy example.
&lt;ul&gt;
&lt;li&gt;Random variable: define $$X := \begin{cases}\alpha, &amp;amp; \text{with probability } p, \\ \beta, &amp;amp; \text{with probability } 1 - p.\end{cases}$$&lt;/li&gt;
&lt;li&gt;Measure theory: simple functions.&lt;/li&gt;
&lt;li&gt;Galois groups: consider $\mathrm{Gal}(\mathbb{Q}(\sqrt{\alpha}):\mathbb{Q})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Real-world examples: a real-world scenario involving the object of interest. These help motivate the study of the given object and are useful for gaining intuition.
&lt;ul&gt;
&lt;li&gt;Stochastic processes: a Poisson process as a description of the number of raindrops falling in a given square.&lt;/li&gt;
&lt;li&gt;PDE: the heat equation.&lt;/li&gt;
&lt;li&gt;Graph theory: a network models the flow of some fluid.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;understanding-results&#34;&gt;Understanding results&lt;/h3&gt;
&lt;p&gt;Second, there are the examples helping us understand results. I&amp;rsquo;ll count applications of theorems among these kinds of examples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Counter examples: examples illustrating how a given result breaks down if we drop assumptions&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.
&lt;ul&gt;
&lt;li&gt;Fatou&amp;rsquo;s lemma: we really do need non-negativity. Consider the sequence $f_n := - 1_{[n, n+1]}$.&lt;/li&gt;
&lt;li&gt;Open mapping theorem: surjectivity is necessary, since the zero map isn&amp;rsquo;t open.&lt;/li&gt;
&lt;li&gt;Nullstellensatz: the field $K$ must be algebraically closed. For $K = \mathbb{R}$, the ideal $I = (x^2 + 1)$ is maximal but not of the form $(x - a)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Basic applications: checking that the conditions of a theorem are satisfied, apply the theorem and see what you get. These examples are particularly useful if the theorem statement seems involved.
&lt;ul&gt;
&lt;li&gt;Hurewicz&amp;rsquo; theorem: use it to find $H_1(\mathbb{S}^n)$.&lt;/li&gt;
&lt;li&gt;Dedekind-Kummer: compute the factorisation of $(p)$ in a ring of integers.&lt;/li&gt;
&lt;li&gt;Dirichlet&amp;rsquo;s unit theorem: the theorem allows us to verify that $\mathbb{Q}(\sqrt{d})$ has finite unit group.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-basic applications: or corollaries. Deduce something interesting from a theorem.
&lt;ul&gt;
&lt;li&gt;Optional stopping: allows you to compute laws of hitting times.&lt;/li&gt;
&lt;li&gt;The ring of integers is Dedekind: this gives us a satisfying proof of Fermat&amp;rsquo;s theorem for primes that are sums of squares.&lt;/li&gt;
&lt;li&gt;Hausdorff-Young: an application of Riesz-Thorin tells us that the Fourier transform defines a bounded linear operator from $L^p$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Amusing applications: examples mentioned during lectures for amusement.
&lt;ul&gt;
&lt;li&gt;Borsuk-Ulam: there are two antipodal points on earth with the same temperature.&lt;/li&gt;
&lt;li&gt;Mean-value theorem: you can catch someone over-speeding using the mean-value theorem.&lt;/li&gt;
&lt;li&gt;Four colour theorem: you can colour a map using just four colours so no two adjacent countries have the same colour.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Satisfying applications: prove a well-known theorem as an application of the fancy theory you developed.
&lt;ul&gt;
&lt;li&gt;The insolvability of the quintic: &amp;ldquo;just&amp;rdquo; an example application of Galois theory.&lt;/li&gt;
&lt;li&gt;The central limit theorem: falls out of the computation of the characteristic function of the scaled sum.&lt;/li&gt;
&lt;li&gt;The fundamental theorem of algebra: an application of Liouville&amp;rsquo;s theorem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are examples the core of mathematics? The answer is very much a matter of personal taste. I&amp;rsquo;d say yes. Apparently von Neumann once said&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In mathematics you don&amp;rsquo;t understand things. You just get used to them.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There&amp;rsquo;s some truth to this. And the way we get used to things is by studying examples.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Alois Schaffler for suggesting the last kind of example.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;em&gt;Basic Category Theory&lt;/em&gt; or &lt;a href=&#34;https://venhance.github.io/napkin/Napkin.pdf&#34;&gt;The Napkin&lt;/a&gt; are amazing in this regard.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;In &lt;em&gt;Abstract Algebra&lt;/em&gt; by Dummit and Foote, some definitions are followed by more than ten (!) examples.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;I once cited $\mathbb{Q}$ as an example of a non-Banach space in an exam. The graders probably weren&amp;rsquo;t particularly happy with my response, but I received full points.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;In my experience, memorising counter examples is the best way to memorise assumptions of a theorem. This is somewhat counter intuitive, given that this involves memorising more information.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>On the joy of customising</title>
      <link>http://localhost:1313/on-the-joy-of-customising/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/on-the-joy-of-customising/</guid>
      <description>&lt;p&gt;Why do so many people use Safari, the jarring black VSCode colour theme and Overleaf? Because it&amp;rsquo;s the default option.&lt;/p&gt;
&lt;p&gt;Switching to a non-default option requires conscious effort. First, you need to realise that your current setup is suboptimal: the probability of your current setup being the optimal one is infinitesimal. But if you knew about the benefits of making a few small changes, you&amp;rsquo;d already have made them. So you do need to believe that switching is worthwhile, which, indeed, is a leap of faith.&lt;/p&gt;
&lt;p&gt;The pros are all believers. Most software engineers customise their code editors and terminal shells heavily. If you spend all your working hours working on a computer, spending one Sunday improving your working environment is well worth it. Similarly, I know some professors at ETH are particular about their writing tools, only using fountain pens and high-end chalk.&lt;/p&gt;
&lt;p&gt;But if you&amp;rsquo;re neither a software engineer nor an ETH professor, tinkering with command-line interface (CLI) tools and splurging money on fountain pens might feel indulgent. This makes up another psychological barrier towards switching from the default. In fact, you might question whether you&amp;rsquo;re even entitled to use the tools of the pros. And to some extent, this sentiment is valid.&lt;/p&gt;
&lt;p&gt;However, I still think most people err on the side of customising too little, especially when it comes to tech. Most of us are heavy tech users; the average screen time globally seems to be over 6h. When I say someone has a good tech setup, I mean that they use appropriate software for their computer-related tasks efficiently. It is much like having a good office with a comfortable chair and a proper desk. However, it need not require expensive purchases.&lt;/p&gt;
&lt;p&gt;Adding a few selected tools to your setup can massively improve productivity. There are some general programs useful to any computer user, like Google Drive, Google Calendar and Raycast. I also think having a dedicated note-taking system, whether it be in Obsidian, Notion or Apple Notes, pays off. However, you&amp;rsquo;ll get the most leverage from finding the tools relevant to the tasks you&amp;rsquo;re doing and mastering them.&lt;/p&gt;
&lt;p&gt;For example, I mostly use my MacBook for typesetting, writing and coding. For typesetting, I use NeoVim with custom snippets, inspired by Gilles Castel&amp;rsquo;s iconic &lt;a href=&#34;https://castel.dev/post/lecture-notes-1/&#34;&gt;VimTeX setup&lt;/a&gt;. Nowadays, I also use Typst rather than LaTeX. Although I don&amp;rsquo;t necessarily type faster in Typst, I spend much less time debugging cryptic error messages. All in all, I estimate that these changes have doubled my writing speed. There are also ergonomic benefits to this setup, as opposed to just writing in Overleaf. As for coding, I&amp;rsquo;ve found Cursor to be a complete game changer, allowing me to implement experiments in about a tenth of the time. Other than that, I&amp;rsquo;m &amp;ldquo;just&amp;rdquo; using NeoVim keybindings in VSCode and a few CLI tools allowing me to navigate between folders more easily. As can be seen from &lt;a href=&#34;https://www.alignmentforum.org/posts/dZFpEdKyb9Bf4xYn7/tips-for-empirical-alignment-research&#34;&gt;this post&lt;/a&gt;, I have a lot to learn here.&lt;/p&gt;
&lt;p&gt;Furthermore, customising your computer makes for a much better user experience. I value good design, so having my preferred colour scheme in Obsidian, VSCode and WezTerm makes my computer a joy to use. I also use the Zen Browser for a cleaner user interface.&lt;/p&gt;
&lt;p&gt;Regardless of whether you&amp;rsquo;re a software aesthete or not, it&amp;rsquo;s worth exploring tech tools relevant to your work.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Consume less AI safety news</title>
      <link>http://localhost:1313/consume-less-ai-safety-news/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/consume-less-ai-safety-news/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s hard staying on top of all the AI safety news. Some people, like &lt;a href=&#34;https://thezvi.wordpress.com/about/&#34;&gt;Zvi&lt;/a&gt;, have basically made this their full-time job.&lt;/p&gt;
&lt;p&gt;A common failure mode for forming views on AI safety is consuming too much information. It&amp;rsquo;s a tendency I&amp;rsquo;ve observed in myself, as well as in others in the AI safety community.&lt;/p&gt;
&lt;p&gt;I think it comes from the urge to solve the AI alignment problem quickly. It&amp;rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there&amp;rsquo;s a lot of general excitement surrounding the area.&lt;/p&gt;
&lt;p&gt;So we might imagine someone who starts following all the leading researchers on Twitter, listening to &lt;em&gt;The Cognitive Revolution&lt;/em&gt; while commuting and reading LessWrong posts before going to bed. Or maybe they&amp;rsquo;ll have lengthy discussions about AI governance in WhatsApp groups and watch Robert Miles&amp;rsquo; YouTube videos over meals.&lt;/p&gt;
&lt;p&gt;But if you&amp;rsquo;re looking to gain a deeper understanding of the AI safety landscape, this isn&amp;rsquo;t enough. You&amp;rsquo;d have to engage more with the material, shifting your creation-to-consumption ratio towards more creation. And you&amp;rsquo;d have to discuss your views with people in real life and, if possible, engage in a local community&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. A common piece of advice for people aiming to become experts is to focus on &lt;a href=&#34;https://forum.effectivealtruism.org/posts/ckj6Moau9qpYArHWc/want-to-be-an-expert-build-deep-models&#34;&gt;building deep models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even if your goal is just to get the big picture, the above approach seems needlessly high effort. There are many excellent resources summarising the main ideas in AI safety, such as the the 80,000 hours &lt;a href=&#34;https://80000hours.org/problem-profiles/risks-from-power-seeking-ai/&#34;&gt;problem profile&lt;/a&gt;. It&amp;rsquo;s a 60 min read with a very high signal-to-noise ratio.&lt;/p&gt;
&lt;p&gt;Either way, binging AI safety-related material to won&amp;rsquo;t help fix the AI alignment problem. It&amp;rsquo;s stressful, and perhaps self-defeating. Given that the AI safety landscape is changing so rapidly, much of what we&amp;rsquo;re seeing is noise. To gain conceptual clarity, perhaps you&amp;rsquo;ll benefit from consuming less AI-safety related news. Unless you&amp;rsquo;re working full-time on AI safety, either as a policy-maker or a researcher, this probably won&amp;rsquo;t negatively influence your ability to do good work.&lt;/p&gt;
&lt;p&gt;Hard problem require careful reflection. Although AI advances fast, we must think slowly about how to ensure things go well.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;My friend &lt;a href=&#34;https://mkodama.org/&#34;&gt;Miles Kodama&lt;/a&gt; put it well: &amp;ldquo;It is easy to BS to a screen&amp;rdquo;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Understanding the AI alignment problem</title>
      <link>http://localhost:1313/understanding-the-ai-alignment-problem/</link>
      <pubDate>Sun, 08 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/understanding-the-ai-alignment-problem/</guid>
      <description>&lt;p&gt;Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic &lt;a href=&#34;https://www.anthropic.com/news/core-views-on-ai-safety#:~:text=build%20safe%2C%20reliable%2C%20and%20steerable%20systems%20when%20those%20systems%20are%20starting%20to%20become%20as%20intelligent%20and%20as%20aware%20of%20their%20surroundings%20as%20their%20designers&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it&amp;rsquo;s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.&lt;/p&gt;
&lt;h3 id=&#34;the-eight-year-old-ceo&#34;&gt;The eight-year-old CEO&lt;/h3&gt;
&lt;p&gt;In this excellent &lt;a href=&#34;https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/&#34;&gt;blog post&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, Ajeya Cotra asks you to imagine the following scenario:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Imagine you are an eight-year-old whose parents left you a $1 trillion company and no trusted adult to serve as your guide to the world. You must hire a smart adult to run your company as CEO, handle your life the way that a parent would (e.g. decide your school, where you’ll live, when you need to go to the dentist), and administer your vast wealth (e.g. decide where you’ll invest your money). You have to hire these grownups based on a work trial or interview you come up with &amp;ndash; you don&amp;rsquo;t get to see any resumes, don&amp;rsquo;t get to do reference checks, etc. Because you&amp;rsquo;re so rich, tons of people apply for all sorts of reasons.&lt;/p&gt;
&lt;p&gt;Your candidate pool includes:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Saints&lt;/strong&gt; &amp;ndash; people who genuinely just want to help you manage your estate well and look out for your long-term interests.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sycophants&lt;/strong&gt; &amp;ndash; people who just want to do whatever it takes to make you short-term happy or satisfy the letter of your instructions regardless of long-term consequences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Schemers&lt;/strong&gt; &amp;ndash; people with their own agendas who want to get access to your company and all its wealth and power so they can use it however they want.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Deciding whom to hire is extremely difficult - you&amp;rsquo;re just eight! In this analogy, humanity is the eight-year-old CEO. Hiring a candidate is like training the superhuman AI model which will best serve our interests.&lt;/p&gt;
&lt;h3 id=&#34;building-planes&#34;&gt;Building planes&lt;/h3&gt;
&lt;p&gt;Suppose aerospace engineers have developed a new plane model. It&amp;rsquo;s energy-efficient, cheap to produce and has increased passenger comfort. However, the engineers don&amp;rsquo;t fully understand the internal workings of the engine. During testing, the engine seems to work alright. The engineers identified a few issues, but these could all be fixed quite easily. Would you be comfortable with this plane being produced for commercial use?&lt;/p&gt;
&lt;p&gt;Here, the AI models are like the engines. We know how to build AI models capable of writing poetry and conducting PhD-level research, but our understanding of how these models learn is relatively limited&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. So, we should probably reflect more carefully on how we&amp;rsquo;re deploying LLMs.&lt;/p&gt;
&lt;h3 id=&#34;drug-regulation&#34;&gt;Drug regulation&lt;/h3&gt;
&lt;p&gt;It takes years for newly developed drugs to reach consumers. First, you need preclinical trials. Then you carry out clinical trials in three distinct phases. This done, you need the approval of a regulatory agency, such as the FDA. It&amp;rsquo;s not uncommon for the entire process to take 10-15 years. Given that we subject drugs to such rigorous testing, why not do the same for LLMs?&lt;/p&gt;
&lt;p&gt;I first heard this analogy in &lt;a href=&#34;https://open.spotify.com/episode/38R2p5TG0uO02q3xybxsvR?si=7f8fa707ea174823&#34;&gt;this podcast&lt;/a&gt; (Swedish, sorry), where Olle Häggström makes the case for AI slowdown. I think the above analogy is quite compelling, although I don&amp;rsquo;t fully share his views.&lt;/p&gt;
&lt;h3 id=&#34;the-hustler&#34;&gt;The hustler&lt;/h3&gt;
&lt;p&gt;Imagine a person trying to learn a new skill, say playing Go. He has memorised all textbooks on Go ever published by heart, as well as all the games played by professional Go players. Moreover, he&amp;rsquo;s extremely hardworking: he plays roughly 1.5 million games against himself per day. (He doesn&amp;rsquo;t need any sleep, and he happens to think very quickly.) Given the amount of practise he gets, how can normal humans hope to defeat him?&lt;/p&gt;
&lt;p&gt;Here, the hustler is similar to an RL system. To me, this analogy makes the prospect of an intelligence explosion seem much more plausible&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and less sci-fi-ish.&lt;/p&gt;
&lt;h3 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h3&gt;
&lt;p&gt;Finally, to understand the alignment problem, I think it&amp;rsquo;s also worth appreciating the potential impact of superhuman AGI. To me it seems like superhuman AI could be about as transformative as the industrial revolution. At the very least, I&amp;rsquo;d expect it to be as impactful as electricity. So, the ensuring the development of AGI &amp;ldquo;goes well&amp;rdquo; seems like a key problem of our time.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;This is one of my all-time favourite pieces on AI alignment.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;I don&amp;rsquo;t count &amp;ldquo;just backpropagate&amp;rdquo; as a satisfactory answer!&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;This analogy was inspired by a conversation with Samuel Ratnam at EAG.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>A semester paper retrospective</title>
      <link>http://localhost:1313/a-semester-paper-retrospective/</link>
      <pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/a-semester-paper-retrospective/</guid>
      <description>&lt;p&gt;This is a follow-up on my &lt;a href=&#34;https://isabeldahlgren.github.io/two-results-from-probabilistic-number-theory/&#34;&gt;previous article&lt;/a&gt;, where I share some thoughts on the process of writing a semester paper.&lt;/p&gt;
&lt;p&gt;Lots of disclaimers: this is all very specific to my experience: the topic of my paper, my supervisor, my workload in others courses, etc. Also, I&amp;rsquo;m certainly in no position to give advice on mathematical writing; these are just reflections on what worked and what didn&amp;rsquo;t work for me.&lt;/p&gt;
&lt;h3 id=&#34;on-writing-papers&#34;&gt;On writing papers&lt;/h3&gt;
&lt;p&gt;Here are some principles for writing mathematical papers that I tried to follow, mostly based on feedback from my supervisors. I received plenty of useful feedback, but here are the points I found the most useful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Top-down exposition: concretely, this meant explaining how all lemmas would come together before proving each lemma. This minimises the amount of task-switching for the reader. In addition, this helps motivate the lemmas and allows the reader to decide which parts to skip.&lt;/li&gt;
&lt;li&gt;Short proofs: as a rule of thumb, I tried breaking up proofs in such a way that proofs of subresults would fit on one page. To my surprise, in most cases, this was doable.&lt;/li&gt;
&lt;li&gt;Reducing cognitive load: a piece of feedback I received from Vivian was to try reducing the cognitive load of the reader; it&amp;rsquo;s also a major theme &lt;a href=&#34;https://ngtriant.github.io/notes/practical_suggestions_for_mathematical_writing.pdf&#34;&gt;here&lt;/a&gt;. This idea really resonated with me. Concretely, this meant doing things like:
&lt;ul&gt;
&lt;li&gt;Restating parameters: for example, one might go &amp;ldquo;Recalling that $X = \text{definition of }X$, we obtain&amp;hellip;&amp;rdquo; rather than &amp;ldquo;By our choice of $X$, &amp;hellip;.&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Creating indices of notation and parameters: very relevant if your proof is notation-heavy.&lt;/li&gt;
&lt;li&gt;More descriptive text: rather than writing &amp;ldquo;we have&amp;rdquo; or &amp;ldquo;thus&amp;rdquo; before a computation, I tried describing which device I was using. Mentioning that you&amp;rsquo;re using, say, a union bound doesn&amp;rsquo;t take up much additional space on the page, and it makes for a better reading experience. (Also, if you&amp;rsquo;re doing analytic number theory, you need more alternatives to &amp;ldquo;we have&amp;rdquo;!)&lt;/li&gt;
&lt;li&gt;Indicating what won&amp;rsquo;t be proved: in order to keep the report at a reasonable length, I had to omit the proofs of some preliminary results and avoid repeating similar arguments. I tried making this clear to the reader, so I wouldn&amp;rsquo;t leave them hanging.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What I would have found useful: initially, I didn&amp;rsquo;t know how many details to include. If I&amp;rsquo;d try explaining all details I&amp;rsquo;d struggled with when reading the proof, the text would be too verbose! Vivian had some good advice here: aim for the level of detail you&amp;rsquo;d liked when reading it for the first time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For future papers, I&amp;rsquo;m considering trying the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Including dependency graphs: although a bit unconventional, these can be tremendously helpful for long and convoluted proofs. See e.g. my piece on &lt;a href=&#34;https://isabeldahlgren.github.io/hunting-dependencies/&#34;&gt;dependencies&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Commenting on dead ends: a few words about why the naïve approach fails can be very illuminating. This can help motivate the use of some very complicated tool and potentially save the reader a lot of time. While remarks on failed proof attempts can sometimes be found in the &amp;ldquo;Discussion&amp;rdquo; section of a paper, I think like they aren&amp;rsquo;t given as much attention as they deserve (an unfortunate instance of &lt;a href=&#34;https://en.wikipedia.org/wiki/Publication_bias&#34;&gt;publication bias&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Reducing cognitive load, even more: if I&amp;rsquo;d kept this in mind during the entire writing process, I think I&amp;rsquo;d done a few things differently. For further ideas on how to reduce cognitive load, I highly recommend the previously mentioned &lt;a href=&#34;https://ngtriant.github.io/notes/practical_suggestions_for_mathematical_writing.pdf&#34;&gt;piece&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lessons&#34;&gt;Lessons&lt;/h3&gt;
&lt;p&gt;If I were to write a thesis or paper again, I would have done a lot differently. Here are the main changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do it in one go: it would have been much more enjoyable and efficient doing most of the semester paper within a much shorter period of time. If you&amp;rsquo;re trying to understand an involved argument, you need all the relevant notions floating around in your brain at the same time. Don&amp;rsquo;t read one lemma per week.&lt;/li&gt;
&lt;li&gt;Request additional feedback: if your supervisor is kind to offer additional feedback, that&amp;rsquo;s extremely valuable.&lt;/li&gt;
&lt;li&gt;As soon as you get it, typeset: you&amp;rsquo;ll have to do it anyway, and you might as well do it when you understand it. This also helps corroborate your understanding.&lt;/li&gt;
&lt;li&gt;Know when to ask for help: I wasn&amp;rsquo;t sure how long it was reasonable for me to be stuck on a particular passage before asking for help, and I know supervisors have different preferences here. Trivial fix: ask your supervisor &amp;ldquo;For how long should I be stuck before asking for help?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Knowing how to ask for help: at first, I&amp;rsquo;d ask questions during meetings. But I soon began emailing a list of questions to my supervisor a few days before our check-ins. That way, I think we both got more out of the meetings.&lt;/li&gt;
&lt;li&gt;Typesetting takes time: it seems to be a law of nature that TeX:ing always takes longer than expected. This happened as I was writing my bachelor thesis too. Despite having a pretty sophisticated NeoVim LaTeX setup with snippets, typesetting took twice as long as I expected. For future papers, I&amp;rsquo;ll probably go with &lt;a href=&#34;https://typst.app/&#34;&gt;Typst&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Two results from probabilistic number theory</title>
      <link>http://localhost:1313/two-results-from-probabilistic-number-theory/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/two-results-from-probabilistic-number-theory/</guid>
      <description>&lt;p&gt;I recently wrote a semester paper on probabilistic number theory. I&amp;rsquo;m very grateful to both of my supervisors, Dr. Vivian Kuperberg and Prof. Dr. Emmanuel Kowalski, for their insights and suggestions. Here&amp;rsquo;s the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This report is an exposition of two central limit theorems in probabilistic number theory. We begin by introducing preliminary results from number theory and probability theory. Then we prove the Erdős-Kac theorem for the asymptotic behaviour of the prime divisor counting function. The majority of the report is devoted to Radziwiłł and Soundarajan&amp;rsquo;s recent proof of the Selberg central limit theorem for $\log |\zeta(\frac{1}{2} + it)|$.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You&amp;rsquo;re most welcome to have a look at the actual &lt;a href=&#34;https://drive.google.com/file/d/1AVEBwfBkdbcflnOQLYACcBXemqSPWzXP/view?usp=sharing&#34;&gt;semester paper&lt;/a&gt;. Here I&amp;rsquo;d like to give a more informal discussion of its contents.&lt;/p&gt;
&lt;h3 id=&#34;deterministic-number-theory&#34;&gt;&amp;ldquo;Deterministic&amp;rdquo; number theory&lt;/h3&gt;
&lt;p&gt;A central theme in analytic number theory is deriving estimates for averages over arithmetic functions, e.g. something like $$\sum_{n \le x} f(n) = \text{main term} + \text{error term},$$where $f: \mathbb{N} \to \mathbb{C}$ is some arithmetic function. Many &amp;ldquo;named theorems&amp;rdquo; are estimates of the above form. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we let $f(n) = x^{-1} 1_P(n)$, where $1_P$ is the indicator of the set of prime numbers, then we get &lt;a href=&#34;https://terrytao.wordpress.com/2013/12/11/mertens-theorems/&#34;&gt;Mertens&amp;rsquo; second theorem&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If instead $f(n) = d(n)$, the function counting the number of divisors of a positive integer $n$, then a &lt;a href=&#34;https://mathworld.wolfram.com/DirichletDivisorProblem.html&#34;&gt;theorem of Dirichlet&lt;/a&gt; asserts that $$\sum_{n \le x} f(n) = x \log x + (2 \gamma - 1)x + O(\sqrt{x}).$$&lt;/li&gt;
&lt;li&gt;Actually, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Prime_number_theorem#Proof_sketch&#34;&gt;Prime number theorem&lt;/a&gt; is equivalent to an estimate of the above form. Just take $f$ to be the von Mangoldt function $\Lambda$, defined by $$\Lambda(n) := \begin{cases}\log p, &amp;amp; n = p^k \ \text{for some prime } p, \\ 0, &amp;amp; \text{otherwise}.\end{cases}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;probabilistic-number-theory&#34;&gt;Probabilistic number theory&lt;/h3&gt;
&lt;p&gt;In my semester paper, I focused on two central limit theorems from probabilistic number theory: the Erdős–Kac and Selberg central limit theorems. Here, the approach is slightly different.&lt;/p&gt;
&lt;h4 id=&#34;sums-and-integrals-as-expectations&#34;&gt;Sums and integrals as expectations&lt;/h4&gt;
&lt;p&gt;First, we let $f: \Omega \to \mathbb{C}$ be an arithmetically defined quantity more generally; typically $\Omega = \mathbb{N}$ or $\mathbb{R}$. So we could take $f$ to be an arithmetic function, but now we also allow for functions of arithmetic functions. For example, if we take $g: \mathbb{N} \to \mathbb{C}$ to be an arithmetic function and post-compose with $x \mapsto x^2 / \sqrt{\log \log x}$, then we could have $f: \mathbb{N} \to \mathbb{C}$ be a weird-looking expression defined by $$f(n) := \frac{g(n)^2}{\sqrt{\log \log n}}.$$&lt;/p&gt;
&lt;p&gt;To highlight the connection with probability theory, fix a positive integer $x$ and let $U_x$ be a random variable uniformly distributed on $\{1, &amp;hellip;, x\}$. Then we obtain $$\frac{1}{x} \sum_{n \le x} f(n) = \mathbb{E}(f(U_x)).$$ So, estimating $x^{-1} \sum_{n \le x} f(n)$ is the same as estimating an expectation.&lt;/p&gt;
&lt;p&gt;Similarly, the problem of estimating the integral $$\frac{1}{x} \int_0^x f(y) \ dy$$ also comes down to estimating an expectation. In the continuous case, let $x$ be an arbitrary positive real number and take $U_x$ to be a random variable uniformly distributed over $[0, x]$. Then the above integral is precisely $\mathbb{E}(f(U_x))$.&lt;/p&gt;
&lt;h4 id=&#34;natural-questions&#34;&gt;Natural questions&lt;/h4&gt;
&lt;p&gt;This naturally prompts the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Could we also estimate the variance of $f(U_x)$?&lt;/li&gt;
&lt;li&gt;Could we even say something about the asymptotic distribution of $f(U_x)$ as $x \to \infty$?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ideally, we can prove something like this:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Let $U_x: \Omega \to \Omega_x$ denote a random variable uniformly distributed on $\Omega_x := \{1, &amp;hellip;, x\}$ (or $[0, x]$). Let $Y$ be a random variable with some simple distribution. Then as $x \to \infty$, we have $$f(U_x) \xrightharpoonup{} Y,$$ where $\xrightharpoonup{}$ denotes convergence in distribution.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A remarkable fact is that some rather convoluted expressions involving arithmetic functions can converge in distribution to standard normal random variables as $x \to \infty$.&lt;/p&gt;
&lt;h4 id=&#34;general-proof-strategy&#34;&gt;General proof strategy&lt;/h4&gt;
&lt;p&gt;Proving results of this form involves machinery from both number theory and probability. We typically use number theory to massage the expression $f(U_x)$, so it can be approximated by some simpler non-arithmetic random variable. This done, we can use limit theorems from probability to finish off the proof.&lt;/p&gt;
&lt;h3 id=&#34;the-erdős-kac-theorem&#34;&gt;The Erdős-Kac theorem&lt;/h3&gt;
&lt;p&gt;This serves as a good &amp;ldquo;toy example&amp;rdquo; for the proof strategy outlined above. Also, it&amp;rsquo;s not too difficult deriving a heuristic proof; for details, see page 14.&lt;/p&gt;
&lt;p&gt;First, some notation. Let $\omega$ denote the function indicating the number of distinct prime divisors of a positive integer. For example, $\omega(6) = 2$ since $6 = 2 \cdot 3$, while $\omega(9) = 1$, since $9 = 3^2$. Then we have:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; (Erdős-Kac, 1940). Let $U_n$ be a random variable uniformly distributed on ${1, &amp;hellip;, n}$ and define $$W_n := \frac{\omega(U_n) - \log \log n}{\sqrt{\log \log n}}.$$ Then the sequence $(W_n)_{n \ge 1}$ of random variables converges in distribution to a standard normal random variable as $n \to \infty$.&lt;/p&gt;
&lt;h3 id=&#34;selbergs-central-limit-theorem-for-log-zeta12--it&#34;&gt;Selberg&amp;rsquo;s central limit theorem for $\log |\zeta(1/2 + it)|$&lt;/h3&gt;
&lt;p&gt;Let $\zeta$ denote the Riemann zeta function, defined by $$\zeta(n) := \sum_{n \ge 1} \frac{1}{n^s}.$$&lt;/p&gt;
&lt;p&gt;The Riemann hypothesis, perhaps the most famous open problem in mathematics, says that the only zeros of the zeta function are of the form $s = -2k$ for positive integers $k$ or lie on the critical strip $\{1/2 + it : t \in \mathbb{R}\}$.&lt;/p&gt;
&lt;p&gt;Now fix a positive number $T &amp;gt; 0$, and consider the random variable $$L_T := \log |\zeta(1/2 + iU_t)|,$$ where $U_t$ is a random variable uniformly distributed on the interval $[-T, T]$. Considering just how hard the Riemann hypothesis is, it&amp;rsquo;s pretty surprising that the behaviour of $L_T$ is relatively well-understood. In fact, we have:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; (Selberg, 1946). Let $L_T$ be the random variable defined as above, and set $$M_T := \frac{L_T}{\sqrt{\frac{1}{2} \log \log T}}.$$ Then the sequence $(M_T)_{T \ge 0}$ of random variables converges in distribution to a standard normal random variable as $T \to \infty$.&lt;/p&gt;
&lt;p&gt;In my semester paper, I cover the 2016 proof due to Radziwiłł and Soundarajan. Just as in the Erdős-Kac theorem, one can get an intuitive sense of why this result is plausible.&lt;/p&gt;
&lt;h3 id=&#34;final-words&#34;&gt;Final words&lt;/h3&gt;
&lt;p&gt;For details, I encourage you to have a look at the actual paper. I hope you enjoy reading it as much as I enjoyed writing it.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>On decision fatigue</title>
      <link>http://localhost:1313/on-decision-fatigue/</link>
      <pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/on-decision-fatigue/</guid>
      <description>&lt;p&gt;Life involves plenty of hard decisions. Should I switch jobs? Should I marry him? Should I move abroad? Then there are the &amp;ldquo;easy&amp;rdquo; decisions: which pyjama to wear or which brand of laundry detergent to buy. Although it doesn&amp;rsquo;t matter which option you choose here, choosing is hard. In fact, if you think of buying laundry detergent as an optimisation problem, taking into account things like social impact, price and quality, I&amp;rsquo;d expect the optimisation problem to be NP-hard! Here&amp;rsquo;s &lt;a href=&#34;https://thezvi.wordpress.com/2017/07/22/choices-are-bad/&#34;&gt;Zvi&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you have a choice, you must stop what you’re doing, and choose.
[&amp;hellip;]
Making a random slash arbitrary choice might not be hard (although sometimes it is) but first you have to choose to choose at random. If you choose not to decide, you still have made a choice. There’s no escape!&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This leads to one of the big curses of adulthood - decision fatigue.&lt;/p&gt;
&lt;h2 id=&#34;i-understanding-the-problem&#34;&gt;I. Understanding the problem&lt;/h2&gt;
&lt;p&gt;Most of us are aware that choosing is draining. Steve Jobs wore black turtlenecks to reduce the number of decisions in a given day. &lt;em&gt;Should I Stay or Should I Go&lt;/em&gt; is basically a three-minute rant about how terrible it is making decisions&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The reason going to IKEA can be so tiring is that it involves making many small choices. Getting to choose among many options is only fun up to a certain point.&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_fatigue&#34;&gt;Wikipedia&lt;/a&gt;, decision fatigue is &amp;ldquo;the deteriorating quality of decisions made by an individual after a long session of decision making&amp;rdquo;. Decision fatigue arises from a broader phenomenon called ego depletion. Intuitively, this is the idea that willpower works like a battery. If we exert a lot of mental effort at one task, then we cannot exert as much mental effort at subsequent tasks without recharging. Making decisions drains our willpower battery.&lt;/p&gt;
&lt;p&gt;The phenomena of decision fatigue and ego depletion have been demonstrated in a number of amusing &lt;a href=&#34;https://www.nytimes.com/2011/08/21/magazine/do-you-suffer-from-decision-fatigue.html&#34;&gt;studies&lt;/a&gt;. A striking example is the study on Israeli judges, demonstrating the so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Hungry_judge_effect&#34;&gt;hungry judge effect&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Computer science offers another way to think about decision fatigue. While a psychologist might speak of ego depletion, a computer scientist might speak of computational complexity. Here&amp;rsquo;s from &lt;em&gt;Algorithms to Live By&lt;/em&gt; by Brian Christian and Tom Griffiths:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;One of the implicit principles of computer science, as odd as it may
sound, is that computation is bad: the underlying directive of any good
algorithm is to minimize the labor of thought. When we interact with other
people, we present them with computational problems—not just explicit
requests and demands, but implicit challenges such as interpreting our
intentions, our beliefs, and our preferences. It stands to reason, therefore,
that a computational understanding of such problems casts light on the
nature of human interaction.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Further, the authors go on to advocate for what they call &amp;ldquo;computational kindness&amp;rdquo;. When interacting with people, we should try sharing the computational cost of making decisions. If you prefer a particular restaurant, don&amp;rsquo;t hesitate to indicate your preference. Although you&amp;rsquo;re trying to be polite by saying you&amp;rsquo;re indifferent, the other person might be relieved to hear you&amp;rsquo;re craving a particular kind of food.&lt;/p&gt;
&lt;h2 id=&#34;ii-antidotes&#34;&gt;II. Antidotes&lt;/h2&gt;
&lt;p&gt;The first step in dealing with decision fatigue is to notice it (as with so much else related to mental well being, I guess). Then one is in a better position to either make a quick, haphazard decision or recharging one&amp;rsquo;s willpower battery, so one can make a better decision later. This is easier said than done, though.&lt;/p&gt;
&lt;p&gt;For example, I recently planned a trip to London, booking flights, finding accommodation, coordinating with others, etc. Because I was excited about the trip, I didn&amp;rsquo;t notice when decision fatigue kicked in. If I&amp;rsquo;d known that it was decision fatigue, I wouldn&amp;rsquo;t have planned the trip in as much detail.&lt;/p&gt;
&lt;p&gt;In practise, noticing decision fatigue is hard. One can always practise &lt;a href=&#34;https://www.lesswrong.com/posts/GLPaZamxqkx7XJbXv/the-skill-of-noticing-emotions&#34;&gt;the skill of noticing&lt;/a&gt;, although it takes a lot of time. Another strategy would be trying to identify situations involving many choices and recalling how it felt. For instance, typical triggers of decision fatigue might be shopping, planning or coordinating with others. Analysis paralysis or a feeling of frustration might be good proxies for decision fatigue.&lt;/p&gt;
&lt;p&gt;But one can also take preventative measures. We can cut out a surprising number of choices from our everyday lives by coming up with default actions for various situations. For example, I don&amp;rsquo;t want to only wear black turtlenecks, but I do have a default outfit - a uniform of sorts. In Google Calendar, I also have a calendar describing my ideal week, so I don&amp;rsquo;t have to decide which lectures to attend on a day-to-day basis. Or at the supermarket: pick the cheapest toothpaste.&lt;/p&gt;
&lt;p&gt;Insignificant non-recurring choices can be batched, so one doesn&amp;rsquo;t go into choosing mode too often. One can even gamify the experience of making choices. For instance, I could have challenged myself to plan the trip within a given time frame.&lt;/p&gt;
&lt;h2 id=&#34;iii&#34;&gt;III.&lt;/h2&gt;
&lt;p&gt;One of the best things about being an adult is that you get to do whatever you want. Getting to choose is a privilege, after all. It&amp;rsquo;s a privilege we should use.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;If you speak Swedish, I recommend listening to Amanda Ginsburg&amp;rsquo;s &lt;a href=&#34;https://open.spotify.com/track/2ESWovaZgVi3BMloGgukKw?si=ca868beba3e24129&#34;&gt;I de många valens land&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>On good conversations</title>
      <link>http://localhost:1313/on-good-conversations/</link>
      <pubDate>Sun, 11 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/on-good-conversations/</guid>
      <description>&lt;p&gt;Some studies suggest we spend 80-90% of our waking hours talking with others. For a normal person, that&amp;rsquo;s at least 12 hours (!) per day. We spend most of our working hours talking to others, whether it be in meetings, lectures or exercise classes. When we&amp;rsquo;re not working, friends, roommates or partners might be around. The hours add up.&lt;/p&gt;
&lt;p&gt;Why do we spend this much time talking with others? Above all, interesting conversations are among the greatest pleasures in life. There&amp;rsquo;s nothing like a good conversation over a good meal. But talking with others can also be very productive. For example, talking with people smarter than you is a phenomenal way of learning. It&amp;rsquo;s like you&amp;rsquo;re downloading part of someone else&amp;rsquo;s worldview into your own head. Of course, there are other ways of achieving the same thing. However, in my opinion, one-on-one discussions come the closest to a loss-free download.&lt;/p&gt;
&lt;p&gt;For this reason, it&amp;rsquo;s natural to ask how we can have better conversations. Similar questions are often raised in the context of dating (&amp;ldquo;What are good topics to talk about on a date?&amp;rdquo;) or business (&amp;ldquo;How can we have more productive meetings?&amp;rdquo;). However, this question seems relevant to everyone.&lt;/p&gt;
&lt;h2 id=&#34;i&#34;&gt;I.&lt;/h2&gt;
&lt;h3 id=&#34;norms&#34;&gt;Norms&lt;/h3&gt;
&lt;p&gt;Several classic Russian novels go something like this: some characters are sitting on a train, they begin chatting and one thing leads to the next. That leads to an 800-page novel. The conversations they have are fascinating, although a bit exaggerated.&lt;/p&gt;
&lt;p&gt;I often have good conversations when I&amp;rsquo;m traveling. It&amp;rsquo;s as if me and my travel companions have this tacit agreement: &amp;ldquo;Okay, we&amp;rsquo;re all somewhat bored. We can&amp;rsquo;t do small talk for another five hours, so let&amp;rsquo;s talk. For real.&amp;rdquo; For this reason, people often end up being more open with one another.&lt;/p&gt;
&lt;p&gt;Moreover, simply being in a different environment can help. If you and a colleague are on a business trip, you&amp;rsquo;re less likely to ask the usual questions about the weather or life back home.&lt;/p&gt;
&lt;p&gt;Takeaway: the norms of the conversation matters a lot. If everyone implicitly agrees on the purpose of the discussion, things become a lot easier. Sometimes, there&amp;rsquo;s no need to say it out loud. But it can definitely help. For example, it might be better starting the phone call by saying &amp;ldquo;Hi, I need advice about you-know-who.&amp;rdquo; Or you might agree on a set of norms for the conversation. For instance, I recently learned that some people in rationalist circles will put a five minute timer whenever someone brings up philosophy of mind, a topic which often leads nowhere.&lt;/p&gt;
&lt;h3 id=&#34;regularity&#34;&gt;Regularity&lt;/h3&gt;
&lt;p&gt;Many scientists, artists and writers would have something like an intellectual spouse - a close collaborator with whom they&amp;rsquo;d discuss ideas. Famous examples include Watson and Crick, Lennon and McCartney or Sartre and de Beauvoir.&lt;/p&gt;
&lt;p&gt;Discussing is a skill. Doing it with the same people, at the same time and at the same place is a bit like practising regularly. Once you know each other sufficiently well, there&amp;rsquo;s no need to agree on conventions again. You just pick up from where you left off.&lt;/p&gt;
&lt;h3 id=&#34;silence&#34;&gt;Silence&lt;/h3&gt;
&lt;p&gt;The Arabic proverb goes &amp;ldquo;Speech is silver, silence is golden.&amp;rdquo; But in order for there to be silence, you need to be in a space with little background noise. Here comes an obvious point: having good discussions in noisy environments is much harder. I feel like this point tends to get neglected in practise, so I find it worth emphasising. Most restaurants, cafés or co-working spaces can be quite rowdy. Instead, consider going for a walk. Apparently Steve Jobs loved going for walking meetings, perhaps for this reason.&lt;/p&gt;
&lt;h2 id=&#34;ii&#34;&gt;II.&lt;/h2&gt;
&lt;h3 id=&#34;synchronising&#34;&gt;Synchronising&lt;/h3&gt;
&lt;p&gt;From an information-theoretic perspective, good conversations are nothing short of miracles. You&amp;rsquo;ll have several people communicating complex ideas at an astonishing speed. There are no mind-reading devices involved, just words. In my own experience, having a good conversation largely comes down to synchronisation.&lt;/p&gt;
&lt;p&gt;As a quick side note, there&amp;rsquo;s a reason I&amp;rsquo;m using the word &amp;ldquo;synchronisation&amp;rdquo; rather than &amp;ldquo;communication&amp;rdquo;. &amp;ldquo;Communication&amp;rdquo; can also be interpreted as a one-sided process, where one party shares information with another group. But in a good discussion, information usually flows both ways.&lt;/p&gt;
&lt;p&gt;One way of synchronising better is asking more questions. Not only do questions help eliminating confusion, but they also make the conversation much more enjoyable. Depending on the situation, asking questions might also be necessary for everyone to feel included. Moreover, as pointed out by Ben Kuhn &lt;a href=&#34;https://www.benkuhn.net/listen/&#34;&gt;here&lt;/a&gt;, asking questions puts you in a better position to help whoever you&amp;rsquo;re talking to. Without any background information, you&amp;rsquo;re likely to give bad advice. And you can only acquire background information by asking questions.&lt;/p&gt;
&lt;p&gt;Another useful technique is paraphrasing. The goal here isn&amp;rsquo;t to sound smart, but to synchronise with your conversation partner. In practise, this means I&amp;rsquo;ll try rephrasing the other person&amp;rsquo;s idea and asking if it&amp;rsquo;s accurate.&lt;/p&gt;
&lt;h3 id=&#34;direction&#34;&gt;Direction&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s not just about being in sync, though. A good conversation also needs to be heading in an interesting direction. If you all know you&amp;rsquo;re heading towards a dead end or that you&amp;rsquo;re going in circles, then it might be worth pointing this out (kindly, of course). Chances are people will agree and be glad someone noticed. It&amp;rsquo;s a bit like when the host declares the party is over at 2 am, and you&amp;rsquo;re grateful for someone forcing you to go home and get some sleep. Another benefit of moderating the discussion is that you simultaneously encourage the group to &lt;a href=&#34;https://en.wikipedia.org/wiki/Metacognition&#34;&gt;metacogitate&lt;/a&gt;. Similarly, if you feel uncomfortable with a particular subject, it might be a good idea to say you&amp;rsquo;d prefer talk about something else, rather than being passive aggressive.&lt;/p&gt;
&lt;h2 id=&#34;iii&#34;&gt;III.&lt;/h2&gt;
&lt;p&gt;How many of our 12 hours of conversation are of high quality? Quite few, I suspect. And this completely OK. Having good conversations can be draining too. It&amp;rsquo;s a form of flow. Even Mihaly Csikszentmihalyi, the psychologist who coined the term &amp;ldquo;flow&amp;rdquo;, remarked that we cannot spend all our lives flowing. So there&amp;rsquo;s no need to cut all small talk.&lt;/p&gt;
&lt;p&gt;But some conversations matter more than others. In fact, certain conversations can be life-changing. They make life more enjoyable, generate creative and scientific insights and connect us to others. What if we could improve the quality of these conversations ever so slightly?&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Building time machines</title>
      <link>http://localhost:1313/building-time-machines/</link>
      <pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/building-time-machines/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a folder on my computer called &amp;ldquo;Time machine&amp;rdquo; with files I want to preserve for posterity. The idea is to collect files which somehow represent today&amp;rsquo;s me. After doing this over a longer period of time, I&amp;rsquo;ll share the folder with future versions of people I care about.&lt;/p&gt;
&lt;h3 id=&#34;i&#34;&gt;I.&lt;/h3&gt;
&lt;p&gt;I like poring over old photos. I think it&amp;rsquo;s something I&amp;rsquo;ve picked up from my mom; she&amp;rsquo;s always the one to pull up a camera and have everyone else pose. I used to find this pretty embarrassing. But now I&amp;rsquo;m glad she ignored my complaints.&lt;/p&gt;
&lt;p&gt;A key theme in the book &lt;em&gt;Thinking, Fast and Slow&lt;/em&gt; is the distinction between the experiencing and remembering self. The experiencing self is the part feeling pleasure and pain, while the remembering self reflects on past experiences. While we&amp;rsquo;re often told to &amp;ldquo;carpe diem&amp;rdquo; more, we&amp;rsquo;re rarely told to try pleasing our remembering selves. But looking back on memories can be a huge source of joy. In fact, this has been confirmed experimentally. In &lt;em&gt;A little book about happiness&lt;/em&gt;, Michael Dahlén, professor of well-being at Stockholm School of Economics (SSE), describes a fascinating experiment in which participants were asked to estimate their well-being at three points in time: past, present and future. On average, the graph looked like a slanted &amp;ldquo;V&amp;rdquo;, the lowest point being the present. Perhaps looking back can make you more appreciative of what you have.&lt;/p&gt;
&lt;p&gt;If that all sounds a bit woo woo, that&amp;rsquo;s OK. If you want, you can also use your time machine to spot cognitive biases. For instance, I have a list of things I&amp;rsquo;ve changed my mind about. As highlighted in &lt;em&gt;Stumbling on Happiness&lt;/em&gt; (bad title, good book), we consistently fail to appreciate just how much we change. I also have a document with things preoccupying me at a given time. When revisiting past entries, I&amp;rsquo;ve noticed a tendency of taking things too seriously.&lt;/p&gt;
&lt;p&gt;The end product - your time machine - is something you can share with others. In due time, I&amp;rsquo;ll share mine with the people who matter the most to me. For instance, I&amp;rsquo;d like to share my time machine with family members, long-time friends and my future partner. And in the very distant future, I might show it to my teenage kids, allowing them to get to know their mother better.&lt;/p&gt;
&lt;h3 id=&#34;ii&#34;&gt;II.&lt;/h3&gt;
&lt;p&gt;There&amp;rsquo;s a lot of latitude in how to build a time machine. My journal makes up a big portion of my time machine. But there are also other files in there. One of the benefits of the digital age is that we can store huge amounts of data very easily, so you may as well be experimental.&lt;/p&gt;
&lt;p&gt;In secondary school, a friend of mine started a trend of creating monthly playlists on Spotify&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. I was quick in hopping on the trend, and I&amp;rsquo;m very grateful I did. Today I have a complete record of how my music taste has evolved since August 2015&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. But I&amp;rsquo;m far from the only one doing this kind of thing. For example, Luke Muehlhauser publishes a list of music recommendations on a quarterly basis on his &lt;a href=&#34;https://lukemuehlhauser.com/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve also kept a reading diary since 2016. It must have started out as a vanity project - having read a boring classic, I assume I wanted to remember that I&amp;rsquo;d finished it. It&amp;rsquo;s amusing seeing how my preferences have changed over time. My reading log is just a long markdown document on my computer&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, but you could also use a platform like StoryGraph or BookShelved. As of a couple of months ago, I&amp;rsquo;ve also been saving blog posts that particularly resonated with me.&lt;/p&gt;
&lt;p&gt;However, you could go beyond music and reading lists. Luke Muehlhauser also curates lists with video games. If you&amp;rsquo;re a big podcast listener, take note of which podcasts you particularly enjoyed in a given month.&lt;/p&gt;
&lt;p&gt;This is all a lot of work, especially if you consume a lot of content. Be selective, but be consistent. I&amp;rsquo;d count the moments I discovered my favourite authors and musicians among the most decisive in my life. I somehow feel like it&amp;rsquo;s impossible getting to know someone on a deeper level unless you have a rough idea of their media consumption.&lt;/p&gt;
&lt;p&gt;Other things could go into your time machine too. You can get to know someone else in a surprisingly short amount of time by asking the right questions. In fact, there are various compilations of questions meant to foster closer relationships - the most famous one being the &lt;a href=&#34;https://36questionsinlove.com/&#34;&gt;36 questions to fall in love&lt;/a&gt;. You could grab your favourite questions from that list and answer them in writing every year, say.&lt;/p&gt;
&lt;p&gt;I have two-three documents of this genre, but I&amp;rsquo;m planning on adding more. As mentioned above, there&amp;rsquo;s the list of things I was wrong about, as well as the list of things bothering me. Following the example of &lt;a href=&#34;https://nabeelqu.co/principles&#34;&gt;Nabeel Qureshi&lt;/a&gt;, I also have a &amp;ldquo;principles&amp;rdquo; document.&lt;/p&gt;
&lt;h3 id=&#34;iii&#34;&gt;III.&lt;/h3&gt;
&lt;p&gt;Humans have always been building their own time machines in the form of literature and art. But a time machine can be just about anything. Building one doesn&amp;rsquo;t require artistic talent, just consistency. Nowadays, you can build a digital time machine with minimal effort. Since files don&amp;rsquo;t take up any physical space, there&amp;rsquo;s no need to clutter your house with keepsakes.&lt;/p&gt;
&lt;p&gt;I was recently given a necklace which belonged to my great grandmother. It&amp;rsquo;s a beautiful piece of jewellery. But it doesn&amp;rsquo;t tell me much about who Ellen was. I hope I&amp;rsquo;ll be able to gift future generations something more.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Nina Ivarsson, more precisely. Thank you.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;The playlists from 2015-2017 are on my parents&amp;rsquo; account, but the more recent playlists can be found here &lt;a href=&#34;https://open.spotify.com/user/q2r0ontmw48z1mc7u2elyho2u?si=1aed8800d74d41ce&#34;&gt;here&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;For excerpts of my reading log, see the &lt;a href=&#34;https://isabeldahlgren.github.io/library/&#34;&gt;library&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>How I use LLMs</title>
      <link>http://localhost:1313/how-i-use-llms/</link>
      <pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/how-i-use-llms/</guid>
      <description>&lt;p&gt;&lt;em&gt;Related: &lt;a href=&#34;https://isabeldahlgren.github.io/will-ai-replace-mathematicians/&#34;&gt;Will AI replace mathematicians?&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;About 40% of students in the ETH main library always have a Chat-GPT tab open. I soon decided to try using LLMs for my own studies (because the wisdom of the crowd is a real thing). I haven&amp;rsquo;t figured out how to best use LLMs for my coursework, but I&amp;rsquo;m experimenting with various approaches.&lt;/p&gt;
&lt;h3 id=&#34;getting-unstuck&#34;&gt;Getting unstuck&lt;/h3&gt;
&lt;p&gt;For me, a big time sink is getting stuck on details. I usually go over the lecture notes after lectures, trying to work out the steps I didn&amp;rsquo;t follow with pen and paper. Ideally, I&amp;rsquo;d do this sitting next to a friend - it&amp;rsquo;s very convenient having someone whom to ask nearby. As Nate Soares put it &lt;a href=&#34;https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I&amp;rsquo;ve either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words. &amp;lsquo;Dude. Disjunction. &lt;em&gt;Dis&lt;/em&gt;junction.&amp;rsquo;&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But studying with a friend isn&amp;rsquo;t always possible. If there&amp;rsquo;s a point of confusion I cannot resolve myself after making a reasonable effort, asking an LLM might help. Formulating a good question is always an instructive exercise. Moreover, nine times out of ten, the response is useful. Even if the LLM doesn&amp;rsquo;t entirely solve my problem, it might reference relevant concepts or serve as a sanity check. Sometimes I&amp;rsquo;ll learn that my approach was completely mistaken - and that&amp;rsquo;s certainly useful too!&lt;/p&gt;
&lt;h3 id=&#34;hints&#34;&gt;Hints&lt;/h3&gt;
&lt;p&gt;In the classic &lt;em&gt;How to Solve It&lt;/em&gt;, George Pólya famously noted that mathematics isn&amp;rsquo;t a spectator sport. By generating hints, LLMs can be an aid in the problem-solving process too. Just have an honest attempt at the problem before consulting an LLM, and tell the LLM to not give away the entire solution. But it&amp;rsquo;s important to notice when one is stuck and ask for help. For someone like me who usually waits too long before taking hints, the ease of generating hints with Chat-GPT makes a huge difference.&lt;/p&gt;
&lt;h3 id=&#34;the-big-picture&#34;&gt;The big picture&lt;/h3&gt;
&lt;p&gt;LLMs are terrific at explaining high-level ideas. I&amp;rsquo;m a big fan of learning concepts &amp;ldquo;top-down&amp;rdquo;, starting with the big picture before getting into the details. While having more context doesn&amp;rsquo;t necessarily mean the material sticks better, I find this approach much more enjoyable. I usually ask Chat-GPT to give me the key idea before I look into the details. Apart from this, I regularly prompt Chat-GPT to give me the intuition for something or to motivate concepts. If a lecturer is pressed on time, they&amp;rsquo;ll cut the motivation bit, rather than leaving out a definition or theorem statement. For this reason, an AI-generated introduction can complement the lectured material.&lt;/p&gt;
&lt;p&gt;Some of my favourite prompts include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Why do we care about X?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What is the main idea behind the proof of X?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What&amp;rsquo;s the intuition for this definition?&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also find it helpful trying to explain a concept in my own words and asking Chat-GPT to elaborate or check if my explanation is accurate.&lt;/p&gt;
&lt;h3 id=&#34;caveats&#34;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;All this said, I&amp;rsquo;d like to add a few caveats.&lt;/p&gt;
&lt;p&gt;A friend or teaching assistant could help with the above tasks better than today&amp;rsquo;s LLMs. They know about your mathematical background and what conventions you&amp;rsquo;re using. When I interact with chatbots, explaining conventions and providing context adds a lot of overhead. However, this problem seems fixable. Many AI labs are already working on ways to provide more personalised responses by having the chatbot remember information across chat sessions. Students could e.g. upload lecture notes and indicate which parts they&amp;rsquo;d covered.&lt;/p&gt;
&lt;p&gt;AI systems also make mistakes. But this isn&amp;rsquo;t that big of an issue. Most mistakes are easy to spot, especially if you ask the AI to explain steps that seem fishy. If you point out what went wrong, it will modify the argument. With human guidance, AI systems can get quite far. Also, LLMs don&amp;rsquo;t need to get all the details right in order to be useful. As Terry Tao noted in &lt;a href=&#34;https://unlocked.microsoft.com/ai-anthology/terence-tao/&#34;&gt;this blog post&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Strangely, even nonsensical LLM-generated math often references relevant concepts. With effort, human experts can modify ideas that do not work as presented into a correct and original argument. The 2023-level AI can already generate suggestive hints and promising leads to a working mathematician and participate actively in the decision-making process.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Of course, if you want to be dead certain the AI-generated argument is correct, have the AI output a formal proof in Lean.&lt;/p&gt;
&lt;p&gt;Another fear of mine, perhaps ungrounded, is basically that LLMs will make us lazy. Learning requires a certain amount of effort, while writing a good LLM prompt is relatively easy. If we use LLMs more and more, will we remove the friction needed for learning? I don&amp;rsquo;t know whether this fear is valid or if it&amp;rsquo;s just an instance of &amp;ldquo;tech panic&amp;rdquo;. But as long as we set boundaries for our LLM usage, we need not spoil the learning experience.&lt;/p&gt;
&lt;h3 id=&#34;where-does-this-leave-us&#34;&gt;Where does this leave us?&lt;/h3&gt;
&lt;p&gt;It seems, then, as if we could overcome the problems I&amp;rsquo;ve encountered when tinkering with AIs. My experience with using LLMs as part of my studies has been positive, so I&amp;rsquo;ll continue exploring ways in which AI can assist. The one thing that LLMs can&amp;rsquo;t provide, however, is the social aspect of doing maths. Solving problems with others is infinitely more fun than coming up with LLM prompts. If anything, I think the above use cases highlight the importance of doing maths together with others.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Will AI replace mathematicians?</title>
      <link>http://localhost:1313/will-ai-replace-mathematicians/</link>
      <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/will-ai-replace-mathematicians/</guid>
      <description>&lt;p&gt;I used to think of maths as the one thing LLMs couldn&amp;rsquo;t do well. While GPT 3.0 would excel in language-based tasks, it would struggle to solve elementary maths problems. But a lot has happened since then. Over the last year, I&amp;rsquo;ve come to take the idea of using AI as an aid for doing maths more seriously. In fact, I now believe LLMs might prove new theorems with no human guidance within just 3 years.&lt;/p&gt;
&lt;p&gt;First, there was the silver medal at International Mathematics Olympiad (IMO). Although AlphaProof and AlphaGeometry 2 took well over 9h, the time given contestants are given, it&amp;rsquo;s quite a feat: IMO problems require an element of creativity. Not only that - it was able to formalise its solution in Lean. Lean is still a fairly new programming language, and there isn&amp;rsquo;t nearly as much training data as for other programming languages. The work of the DeepMind team shows two things: firstly, lots of clever people are trying to build AI systems for doing maths; secondly, apparently their current approach works pretty well. However, as of April 2025, you don&amp;rsquo;t need an AI specifically trained to do maths in order to solve tricky problems: the new o3 and o4 mini models achieve &lt;a href=&#34;https://openai.com/index/introducing-o3-and-o4-mini&#34;&gt;impressive performance&lt;/a&gt; in the American Invitational Mathematics Examination (AIME). What if we use RL to build AI systems specialised in more advanced topics? Perhaps these AIs might prove new theorems. Even if they don&amp;rsquo;t, they might provide researchers with insights.&lt;/p&gt;
&lt;p&gt;Next, several top-gun mathematicians think AI might transform maths research in the next decade. Most notably, Terence Tao has highlighted ways in which machines can help human mathematicians. Here&amp;rsquo;s from a &lt;a href=&#34;https://unlocked.microsoft.com/ai-anthology/terence-tao&#34;&gt;blog post&lt;/a&gt; of Tao:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I could feed GPT-4 the first few PDF pages of a recent math preprint and get it to generate a half-dozen intelligent questions that an expert attending a talk on the preprint could ask. I plan to use variants of such prompts to prepare my future presentations or to begin reading a technically complex paper.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In &lt;a href=&#34;https://epoch.ai/frontiermath/expert-perspectives&#34;&gt;this report&lt;/a&gt; from Epoch AI, Richard Borcherds seems equally optimistic about the possibilities of using, predicting that AI might even surpass human mathematicians within 10 years. Overall, I think we&amp;rsquo;re starting to see a cultural shift in the maths community. People are recognising that AI is a huge deal.&lt;/p&gt;
&lt;p&gt;Finally, I observed that 40% of students in the library seem to have a Chat-GPT tab open at all times. These are students doing STEM subjects, such as maths and physics. This seems like an important data point (and this isn&amp;rsquo;t just because I&amp;rsquo;m giving more weight to first-hand experience). LLMs are transforming the way students learn, and these are the people who will go on to do research in a couple of years. Chances are we won&amp;rsquo;t stop using LLMs just because the material becomes more niche. Even if you receive a hallucinatory answer, the LLM might reference a relevant concept, helping you get unstuck. I&amp;rsquo;m using Chat-GPT for my own studies, and I&amp;rsquo;m impressed by its explaining abilities. Basically, it can easily handle any concept you&amp;rsquo;ll come across in a master degree in mathematics. I&amp;rsquo;ve also prompted Chat-GPT to distill the key ideas from more recent papers and found its responses very helpful.&lt;/p&gt;
&lt;p&gt;All in all, I&amp;rsquo;ve come to shorten my AI timelines quite a bit. But rather than thinking &amp;ldquo;Will I ever find a job?&amp;rdquo;, I find myself thinking &amp;ldquo;What a time to be alive!&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>&#34;Just switch it off&#34;</title>
      <link>http://localhost:1313/just-switch-it-off/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/just-switch-it-off/</guid>
      <description>&lt;p&gt;If we develop a rogue AI, couldn&amp;rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &amp;ldquo;switching off&amp;rdquo; would mean deleting a model&amp;rsquo;s weights, so it can&amp;rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?&lt;/p&gt;
&lt;p&gt;First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&amp;rsquo;t &amp;ldquo;want&amp;rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&amp;rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;&amp;ldquo;alignment faking&amp;rdquo;&lt;/a&gt;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.&lt;/p&gt;
&lt;p&gt;Second, and perhaps more worryingly, is the question of whether we as a society want to &amp;ldquo;press delete&amp;rdquo;. Turning off sandboxed AI - AI developed in a secure training environment - isn&amp;rsquo;t a big deal. The negative consequences, if any, are limited to the few people who can access the model. But a leading AI company has strong incentives against withdrawing potentially unsafe models from the market. Doing this would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&amp;rsquo;s the geopolitical aspect. As highlighted in the &lt;a href=&#34;https://ai-2027.com/&#34;&gt;AI 2027&lt;/a&gt; report, the fear of falling behind in the AI arms race might lead us to deploy even misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, despite poor performance on safety benchmarks.&lt;/p&gt;
&lt;p&gt;Switching off an AI isn&amp;rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&amp;rsquo;s likely to become more difficult with more capable models. Then there&amp;rsquo;s the human factor. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>From a YouTube alumni</title>
      <link>http://localhost:1313/from-a-youtube-alumni/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/from-a-youtube-alumni/</guid>
      <description>&lt;p&gt;Random people on the Internet have played a huge role in my education. I&amp;rsquo;m not just referring to my coursework at university, but also to &amp;ldquo;Bildung&amp;rdquo; more generally. I&amp;rsquo;ve learned a ton by browsing StackOverflow threads and reading Medium articles. However, I&amp;rsquo;ve probably learned the most from watching YouTube.&lt;/p&gt;
&lt;h3 id=&#34;learning-by-watching&#34;&gt;Learning by watching&lt;/h3&gt;
&lt;p&gt;Above all, there&amp;rsquo;s some really high-quality content out there. Nowadays there are full-time YouTubers, working on creating professional, meticulously edited videos. And some channels, such as &lt;a href=&#34;https://www.youtube.com/channel/UCsXVk37bltHxD1rDPwtNM8Q&#34;&gt;Kurzgesagt&lt;/a&gt;, are even ran by entire teams of illustrators and script-writers. Moreover, because anyone can record themselves and upload it to YouTube, we have world-class experts sharing their knowledge in YouTube lectures&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This means there are some truly remarkable YouTube videos. For example, here are comments from some &lt;a href=&#34;https://www.youtube.com/@3blue1brown&#34;&gt;3Blue1Brown&lt;/a&gt; videos:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I dropped out in 10th grade 25 years ago and your videos have inspired me to go back to school.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You sir truly deserve an honorary doctorate - just for this video. Your impact to generations of confused engineering and math students will forever ripple through our society.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I have a master&amp;rsquo;s degree in mechanical engineering and I&amp;rsquo;m starting to think I should redo my whole education from ground up searching for this kind of intuitive knowledge. It&amp;rsquo;s absurd that I find out explanations which are as intuitive as this one so late in my life. I&amp;rsquo;m blown away completely! I mean how many bits of information have we stumbled upon during our formal education failing to see how they elegantly relate to each other and form a bigger picture&amp;hellip;oh my!&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, videos tend to be more attention-grabbing than articles. Although most people are unable to read while cooking or brushing their teeth, they can watch videos. So getting started learning has never required less willpower: just search &amp;ldquo;Introduction to&amp;hellip;&amp;rdquo; on YouTube.&lt;/p&gt;
&lt;h3 id=&#34;learning-to-watch&#34;&gt;Learning to watch&lt;/h3&gt;
&lt;p&gt;Of course, YouTube isn&amp;rsquo;t designed to be a learning platform. But there ways of optimising for a better learning experience.&lt;/p&gt;
&lt;p&gt;The first step is to recognise that YouTube tries maximising user retention. This is a feature, not a bug. It means we can design our YouTube interface such that we end up binge watching informative videos about topics we care about. Here are some ways of achieving this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Train your algorithm&lt;/strong&gt;: in your YouTube feed, take 2 seconds to press &amp;ldquo;Not interested&amp;rdquo; whenever something irrelevant pops up. It pays off - I find the YouTube algorithm to be surprisingly sensitive to my feedback.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keep separate accounts&lt;/strong&gt;: on a similar note, I have two Google accounts: my main account, and my secondary account. I&amp;rsquo;ll use my main account for watching &amp;ldquo;useful&amp;rdquo; content, while I&amp;rsquo;ll log onto my secondary account for, well, everything else.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Block channels&lt;/strong&gt;: use &lt;a href=&#34;https://getcoldturkey.com/support/how-to/allow-youtube-channel/&#34;&gt;ColdTurkey&lt;/a&gt; to block certain YouTube channels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second step is to recognise the limits of just watching videos. There&amp;rsquo;s a reason we don&amp;rsquo;t abandon more traditional media altogether. When reading a book, I find it much easier recognising when I&amp;rsquo;m confused. However, after finishing a video, I sometimes find myself completely lost and unable to tell where I stopped following. Moreover, whenever I have a physical textbook, I&amp;rsquo;ll often refer back to chapters I&amp;rsquo;ve finished, just to refresh my memory. Here are two partial fixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Media notes plugin&lt;/strong&gt;: the &lt;a href=&#34;obsidian://show-plugin?id=media-notes&#34;&gt;Media Notes&lt;/a&gt; plugin for Obsidian is a real game-changer. It allows you to watch YouTube videos from inside Obsidian and take notes with timestamps. When doing this, I seem to engage more with the material. Taking physical notes while watching YouTube is a bit overkill, so this seems like a good compromise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rewatch your favourite videos&lt;/strong&gt;: it&amp;rsquo;s easy ending up only consuming new content, just because the YouTube landing page is filled with new videos. But it&amp;rsquo;s worth saving your favourite videos to playlists and rewatching them later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I discussed above, there are some hacks for a better learning experience. However, YouTube could also design their platform differently. They could e.g. develop distraction-free mode, enabling the user to remove shorts, ads or sponsored content. From a technical perspective, it&amp;rsquo;s doable. But we could also go beyond ordinary videos. For example, Andy Matuschak and Michael Nielsen have explored ways of incorporating an element of spaced repetition in videos, making for a more interactive learning experience; see &lt;a href=&#34;https://numinous.productions/ttft/#mnemonic-video&#34;&gt;here&lt;/a&gt;. In general, I&amp;rsquo;m excited about integrating modern technology with education.&lt;/p&gt;
&lt;p&gt;All this said, I’d like to thank the strangers who have played - and continue to play - an important role in my education.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;See e.g. &lt;a href=&#34;https://www.youtube.com/@AndrejKarpathy&#34;&gt;Andrej Karpathy&amp;rsquo;s channel&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;My friend Åke Lindblom first told me about this. He apparently has an insanely good algorithm.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Look for opinions</title>
      <link>http://localhost:1313/look-for-opinions/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/look-for-opinions/</guid>
      <description>&lt;p&gt;Opinionated people can be really annoying. Wherever they go, they try convincing you of their ideas. If you have an opinionated uncle, the Christmas dinner might be ruined by a bitter argument. I&amp;rsquo;ve certainly had bad experiences with a dinner-table conversations turning into feuds. For this reason, I used to try having fewer opinions. I somehow assumed this meant being more open-minded and mature. Well, no.&lt;/p&gt;
&lt;h3 id=&#34;welcoming-opinions&#34;&gt;Welcoming opinions&lt;/h3&gt;
&lt;p&gt;Actually, there are plenty of benefits of actively trying to form more opinions. Even about topics you don&amp;rsquo;t know particularly well. If you learn with a view towards arguing, then you&amp;rsquo;ll pay closer attention to the material. I think this has to do with anchoring. If you pick a stance, even at random, you&amp;rsquo;ll be more emotionally invested. Holden Karnofsky summarised it neatly in &lt;a href=&#34;https://www.cold-takes.com/learning-by-writing/&#34;&gt;Learning by Writing&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;By doing this [trying to have a hypothesis and rearticulating it whenever it changes], I try to &lt;strong&gt;continually focus my reading on the goal of forming a bottom-line view, rather than just “gathering information.”&lt;/strong&gt; I think this makes my investigations more focused and directed, and the results easier to retain. I consider this approach to be &lt;strong&gt;probably the single biggest difference-maker between &amp;ldquo;reading a ton about lots of things, but retaining little&amp;rdquo; and &amp;ldquo;efficiently developing a set of views on key topics and retaining the reasoning behind them.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, chatting with people with strong opinions can also be fun. Say you&amp;rsquo;re at a cocktail party. Small talk can be quite tiring, at least after a couple of hours. In this situation, I&amp;rsquo;ll gladly talk to people trying to persuade me of their ideas. Or say you&amp;rsquo;re hosting friends for dinner, and a friend explains her take on a topic you all like.&lt;/p&gt;
&lt;p&gt;Back in school, we were encouraged to form more opinions. Teachers made us write argumentative essays about topics we hardly knew anything about. They know that most 14 year-olds don&amp;rsquo;t care the slightest about whether fathers should be given two additional weeks of paternity leave, or if the capital income tax should be raised by 1%. But I don&amp;rsquo;t think it was only meant as an exercise in communicating effectively. It felt as if teachers were saying &amp;ldquo;Go out there in the big wild world, and look for opinions!&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;chasing-opinions&#34;&gt;Chasing opinions&lt;/h3&gt;
&lt;p&gt;So why doesn&amp;rsquo;t everyone have, like, a lot opinions?&lt;/p&gt;
&lt;p&gt;I think many people, whether they recognise it or not, resort to some kind of agnosticism for fear of being wrong. However, recognising you&amp;rsquo;re wrong just means you&amp;rsquo;re updating your beliefs. It&amp;rsquo;s not that big of a deal. Also, many people think they aren&amp;rsquo;t entitled to hold an opinion since they aren&amp;rsquo;t &amp;ldquo;qualified&amp;rdquo;. This is true for areas in which there&amp;rsquo;s a clear distinction between experts and non-experts. But I have a hunch that we sometimes use this as an excuse for not looking into certain issues. For example, as highlighted in &lt;a href=&#34;https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718&#34;&gt;Superforecasting&lt;/a&gt;, normal people can form well-informed predictions on certain issues with a bit of practise. Anyway, I think there&amp;rsquo;s a middle ground here: if you don&amp;rsquo;t know all the technical details, just adjust your confidence levels.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth emphasising that forming opinions can be very difficult and time-consuming. For example, predicting technological progress is notoriously hard. It requires you to do your homework, researching which factors influence scientific advancements and so on. This goes against the idea of opinions coming to us &amp;ldquo;naturally&amp;rdquo;, as if by chance. Yet, this isn&amp;rsquo;t a good reason not to actively seek out opinions. Sometimes, we have to make our minds up in order to take action.&lt;/p&gt;
&lt;p&gt;As long as one has some &amp;ldquo;epistemic etiquette&amp;rdquo; - being prepared to change your beliefs in the light of new evidence, and not taking everything so personally - having more opinions seems like a good thing. I&amp;rsquo;m currently trying to build the habit of always having a working hypothesis whenever I learn something new. Having more opinions makes you feel more like part of the world, rather than as a bystander.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>How AI might go wrong</title>
      <link>http://localhost:1313/how-ai-might-go-wrong/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/how-ai-might-go-wrong/</guid>
      <description>&lt;p&gt;As with any new technology, advanced AI entails certain risks. While we&amp;rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.&lt;/p&gt;
&lt;p&gt;The other week, I had an insightful discussion with people in &lt;a href=&#34;https://www.zurich-ai-alignment.com&#34;&gt;Zürich AI Alignment (ZAIA)&lt;/a&gt; about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Powerful technology in bad hands&lt;/strong&gt;: First, there are the risks which all fall under the label &amp;ldquo;bad guys using powerful technology to further their own interests&amp;rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I&amp;rsquo;d guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concentration of power&lt;/strong&gt;: AI is a transformative technology - a message that shouldn&amp;rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&amp;rsquo;s likely to have a profound impact on things like our economy and healthcare system; see e.g. &lt;a href=&#34;https://www.weforum.org/stories/2025/03/ai-transforming-global-health/&#34;&gt;here&lt;/a&gt;. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren&amp;rsquo;t democratically elected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misaligned AI models&lt;/strong&gt;: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&amp;rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&amp;rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&amp;rsquo;ve already seen plenty of examples of this. For example, as highlighted in &lt;a href=&#34;https://www.youtube.com/watch?v=zn2ukSnDqSg&#34;&gt;this video&lt;/a&gt;, it&amp;rsquo;s still quite easy jailbreaking LLMs like ChatGPT. Moreover, there are &lt;a href=&#34;https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples&#34;&gt;striking examples&lt;/a&gt;
) of algorithms encoding our own biases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power-seeking AI&lt;/strong&gt;: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&amp;rsquo;ll briefly outline what I understand to be the core argument. The first premise is that AIs might end up with power-seeking behaviour. If power is instrumental in achieving one of the AI&amp;rsquo;s goals, the AI might learn to optimise for power. At the same time, we&amp;rsquo;re likely to develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities. Now the punchline is that such an AI might view humans as obstructions to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly: &amp;ldquo;You can&amp;rsquo;t fetch the coffee if you&amp;rsquo;re dead&amp;rdquo;. So, the argument goes, the AI might try to disempower humanity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The argument about power-seeking AI is quite subtle, so I&amp;rsquo;ll elaborate a bit. I think the keyword here is &amp;ldquo;agency&amp;rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &amp;ldquo;agentic&amp;rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage.&lt;/p&gt;
&lt;p&gt;Of course, there&amp;rsquo;s much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. For a more fine-grained taxonomy, see &lt;a href=&#34;https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary&#34;&gt;this article&lt;/a&gt;. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate or mitigate some of these risks. If we take adequate measures, I believe we can make AI &amp;ldquo;go well&amp;rdquo; with very high probability - something like 90%. Just take a moment to imagine what that would be like.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Isaia Gisler for feedback on an earlier version of this text.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Useful thought experiments</title>
      <link>http://localhost:1313/useful-thought-experiments/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/useful-thought-experiments/</guid>
      <description>&lt;p&gt;Philosophers love thought experiments. Thought experiments are hypothetical scenarios meant to tease out our intuitions about an argument or theory. For example, here&amp;rsquo;s a classic thought experiment, due to Robert Nozick:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose there were an experience machine that would give you any experience you desired. Superduper neuropsychologists could stimulate your brain so that you would think and feel you were writing a great novel, or making a friend, or reading an interesting book. All the time you would be floating in a tank, with electrodes attached to your brain. Should you plug into this machine for life, preprogramming your life&amp;rsquo;s experiences?&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Many thought experiments seem kind of cooked up, so it&amp;rsquo;s easy to believe thought experiments have no practical use. However, one of my key takeaways from &lt;em&gt;The Scout Mindset&lt;/em&gt; by Julia Galef was that thought experiments aren&amp;rsquo;t just diversion for people with too much spare time. In fact, she argues, certain thought experiments can help us think more clearly about decisions we face in everyday life&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. In this essay, I&amp;rsquo;ll go over three interesting thought experiments from her book, and then describe two personal faves.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Outsider Test&lt;/strong&gt;: This all comes down to having an outsider&amp;rsquo;s perspective, as if one were calling a friend. How would an outsider evaluate the situation? For example, say John Doe has been given two job offers: one at company X, the other at company Y. Company X will look better on his CV, but he&amp;rsquo;s unlikely to enjoy the day-to-day tasks. In contrast, the job at company Y, although not quite as prestigious, seems more fun. Here, an outsider may say something like &amp;ldquo;If prestige weren&amp;rsquo;t a consideration, which option would you pick?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Conformity Test&lt;/strong&gt;: This is a good one. We&amp;rsquo;re often quick in adopting the beliefs of people we respect. And this is usually a good thing: life is to short to overthink everything, and we&amp;rsquo;ve got to form opinions somehow. (For fans of &lt;em&gt;Thinking, Fast and Slow&lt;/em&gt;, this is System 1 in action.) However, when it comes to more delicate subjects, this mental short-cut might fail. In the Conformity Test, Julia Galef asks you to imagine that people no longer hold your view. (To all contrarians out there, just imagine that the people in your community suddenly become just as everyone else.) There&amp;rsquo;s a particularly interesting spin-off here: what if one of the main proponents of your view, perhaps the person who helped shape your beliefs about the subject, would change their mind? I think the EA/rationalist community provides a good use case. What if Will MacAskill would say he was completely mistaken about longtermism, rejecting the idea altogether? Or if Eliezer Yudowsky would declare that AI after all isn&amp;rsquo;t that big of a threat?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Status Quo Bias Test&lt;/strong&gt;: The underlying idea here is that humans have a bias towards the status quo. If you were to start from scratch, would you actively choose your current situation? For example, imagine a medical student who realises halfway through second year of med school that medicine isn&amp;rsquo;t for her. Although she cannot imagine herself as a doctor, she&amp;rsquo;s still hesitant to switch subjects. Here the Status Quo Bias Test might come in handy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading &lt;em&gt;The Scout Mindset&lt;/em&gt;, I soon realised that some of the advice I&amp;rsquo;ve received over the years can be rephrased as thought experiments. Here are two such thought experiments which I&amp;rsquo;ve found particularly useful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Worst Case Scenario&lt;/strong&gt;: This just involves asking oneself about the worst-case scenario. Is it really that bad? If yes, well, then you know. At least your fear isn&amp;rsquo;t  some sort of vague, scary illusion. And if not, good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast forward a decade&lt;/strong&gt;: In ten years from now, which decision is one more likely to regret? Humans are famously bad at long-term planning. We&amp;rsquo;ll often fail to take the route of action with benefits in a distant future. (Hence climate change and the fact that most adults don&amp;rsquo;t get enough sleep.) So it might be a good idea doing some kind of Outsider test, where the outsider is one&amp;rsquo;s future self.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the above thought experiments might sound familiar. Perhaps you&amp;rsquo;ve already used some of them yourself. After all, these seem like obvious tricks for seeing this for what they are. But perhaps it&amp;rsquo;s helpful having names for these tricks. It&amp;rsquo;s a bit like building a toolkit for better decision-making. I&amp;rsquo;ve applied something like Worst Case Scenario a bunch of times, but only after spending a couple of days of dwelling on the issue in a very unproductive way. Thinking in terms of thought experiments would have spared me a lot of headache.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Nozick, Robert, and Thomas Nagel. &lt;em&gt;Anarchy, state, and utopia&lt;/em&gt;. Vol. 5038. New York: Basic books, 1974.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Galef, Julia. &lt;em&gt;The Scout Mindset: Why some people see things clearly and others don&amp;rsquo;t&lt;/em&gt;. Penguin, 2021.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Hunting dependencies</title>
      <link>http://localhost:1313/hunting-dependencies/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hunting-dependencies/</guid>
      <description>&lt;p&gt;One of the benefits of attending lectures is that lecturers tend to give some really good unsolicited advice every now and then. Last semester, a professor of ours digressed to talk about the importance of identifying the key ingredients of a result. He&amp;rsquo;d just concluded a rather long proof, and was about to clean the blackboard as he said that the result was elementary. See, you only need this one lemma from point-set topology (admittedly a bit niche, but easy to prove), and then the definition of the Fourier transform. Put that way, sure, maybe it&amp;rsquo;s elementary. Explicitly writing down the dependencies of an idea, he said, was a good exercise.&lt;/p&gt;
&lt;h3 id=&#34;an-example&#34;&gt;An example&lt;/h3&gt;
&lt;p&gt;I tried doing this a couple of times for definitions, theorem statements and proofs. At first, it felt a bit silly. Once I finished writing down the main components of a result, it seemed trivial. I overdid it at first, writing down dependencies even for minor lemmas. But for more complicated theorems or involved definitions, it proved quite useful.&lt;/p&gt;
&lt;p&gt;Now for a concrete example. Suppose we&amp;rsquo;re trying to understand martingales. Martingales can be thought of as sequences of random variables representing fair games: if we&amp;rsquo;re given the value of the $n$:th random variable, we expect the value of the $(n+1)$:th random variable to stay the same. There&amp;rsquo;s no predictable up- or downward trend. Here&amp;rsquo;s the definition from &lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-031-14205-5&#34;&gt;Le Gall&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let $(X_n)_{n \in \mathbb{Z}_+}$ be an adapted, real-valued random process, such that $E(|X_n|) &amp;lt; \infty$ for every $n \in \mathbb{Z}_+$. We say that the process $(X_n)_{n \in \mathbb{Z}_+}$ is a martingale if, for every $n \in \mathbb{Z}_+$, $$E(X_{n+1}|\mathcal{F}_n) = X_n.$$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;First, we need some terminology related to random processes, understanding what&amp;rsquo;s meant by an &amp;ldquo;adapted process&amp;rdquo; and a &amp;ldquo;filtration&amp;rdquo;. Apart from that, we need a solid grasp of conditional expectations. The definition of a conditional expectation with respect to a $\sigma$-algebra, as well as the underlying intuition. And that&amp;rsquo;s about it.&lt;/p&gt;
&lt;h3 id=&#34;should-i-bother&#34;&gt;Should I bother?&lt;/h3&gt;
&lt;p&gt;If you want to learn something thoroughly, then yes. Tracking down dependencies makes you engage more with the material. Learning a concept isn&amp;rsquo;t just a matter of being able to regurgitate the contents of the lecture notes - it requires you to build your own mental model of what&amp;rsquo;s going on. As I see it, the purpose of exercises, quizzes or review questions is to make us think more carefully about a given topic. Otherwise, because humans (or at least maths students) are lazy, chances are we&amp;rsquo;ll go through the material too quickly. Tracking down dependencies is a bit like doing more problems, in that it prompts us to revisit the material.&lt;/p&gt;
&lt;p&gt;Another benefit of nailing down dependencies is that it makes the concept seem relatively simple. Some results can seem quite daunting at first. But working out the dependencies can make the concept seem deceptively simple - almost to the point where you&amp;rsquo;re struck with the curse of knowledge. Moreover, if I struggle to understand an idea but am clear about the dependencies, I know what to do: I&amp;rsquo;ll just read up on each of the topics involved. In this way, the dependencies translate into an a checklist for my learning process.&lt;/p&gt;
&lt;p&gt;Lastly, I found it satisfying seeing how different notions tied into one another. I also put it all into one mindmap which, apart from having a high aesthetic value, gave me the big picture of the subject.&lt;/p&gt;
</description>
    </item>
    
    
    
    
    
    
    
    
  </channel>
</rss>
