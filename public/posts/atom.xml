<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Isabel Dahlgren</title>
    <link>http://localhost:55424/posts/</link>
    <description>Recent content in Posts on Isabel Dahlgren</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 30 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:55424/posts/atom.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On shame</title>
      <link>http://localhost:55424/on-shame/</link>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-shame/</guid>
      <description>&lt;p&gt;There are many negative emotions: envy, anger, fear, worry, sadness, to name a few. Positive emotions don&amp;rsquo;t carry as much nuance. When you&amp;rsquo;re asked how you&amp;rsquo;re doing, assuming you&amp;rsquo;re doing good, you&amp;rsquo;ll probably reply you&amp;rsquo;re simply &amp;lsquo;doing good&amp;rsquo;. Positive emotions like happiness, fulfilment or excitement appear to be more strongly correlated with one another &amp;ndash; for this reason, they&amp;rsquo;re often lumped together under the generic term &amp;lsquo;happiness&amp;rsquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Among negative experiences, worry is the most prevalent. In a &lt;a href=&#34;https://www.gallup.com/analytics/349280/state-of-worlds-emotional-health.aspx&#34;&gt;2024 Gallup study&lt;/a&gt; where adults were asked which feelings they&amp;rsquo;d experienced &amp;lsquo;a lot&amp;rsquo; the previous day, 39% and 37% reported feelings of worry and stress, respectively. Next followed physical pain (32%), sadness (26%) and anger (22%)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Normal situations, abnormal norms</title>
      <link>http://localhost:55424/normal-situations-abnormal-norms/</link>
      <pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/normal-situations-abnormal-norms/</guid>
      <description>&lt;p&gt;Most people lead a comfortable 9-5 life. You go to work, work away, go home, sometimes seeing friends or engaging in extracurriculars. Even so, sometimes you find yourself in unusual social situations, situations where a first-timer wouldn&amp;rsquo;t know how to behave. Here are some normal situations with abnormal social norms.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Flights:&lt;/strong&gt; On a 12-hour flight, you find a cross section of the population bored, tired and without Internet. In this willpower-reduced state, snobs dress in sweatpants and workaholics watch thrashy feel-good movies. A surprising number of people try napping&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, while others strike up conversations with strangers.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Dates:&lt;/strong&gt; It&amp;rsquo;s common for people taking steps to get to know new people, for example by attending local meet-ups; however, friend-seekers rarely try getting to know others well. Dates are among the few situations where people deliberately try building deep connections. Think about it: the purpose of a date is for two people, usually strangers, to become romantic partners. Pretty remarkable, right? Even more remarkable is that it sometimes works.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Alcohol:&lt;/strong&gt; Alcohol creates a form of social anarchy. Moderately tipsy people are amusing &amp;ndash; they&amp;rsquo;re a bit like children, candid and ignorant of social norms, though they enjoy discussing grown-up topics.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Work interviews:&lt;/strong&gt; A work interview is nothing at all like an ordinary conversation, despite appearances; it&amp;rsquo;s like an improv play where the interviewee tries selling themself and the interviewer maintains a poker face. The interviewee presents themself as the ideal employee: competent, reliable and a good coffee machine conversationalist&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Music festivals:&lt;/strong&gt; During music festivals, thousands of sweaty people packed like sardines scream in unison while jumping up and down. Though they&amp;rsquo;re strangers, they share a sense of camaraderie, sometimes even sharing food and belongings&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Cult?&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summer camps:&lt;/strong&gt; There are summer camps for kids as well as adults, though the latter are sometimes called &amp;lsquo;retreats&amp;rsquo; or &amp;lsquo;get-aways&amp;rsquo;. The goal of most summer camps is for a group of strangers to pursue a common interest together and make new friends. Being in a remote place helps create a space for intimacy: the camp participants tacitly agree that &amp;lsquo;what gets said here stays here&amp;rsquo;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Military service:&lt;/strong&gt; Life in the military is famously totalitarian: it&amp;rsquo;s all about obeying your superiors. And if you do mandatory military service, you&amp;rsquo;re normally at the bottom of the pecking order. In no other setting is there such a clear hierarchy, and in no other setting is outright bullying socially acceptable.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;These are all time-bound activities disconnected from everyday life, usually physically or by means of clothing, with extraordinary social norms. This makes them good settings for people-watching. Why study &amp;lsquo;man in a state of nature&amp;rsquo; &amp;ndash; an impossibility anyway &amp;ndash; if you can study &amp;lsquo;man in a state of society&amp;rsquo;?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Techno-optimism - techno-altruism?</title>
      <link>http://localhost:55424/techno-optimism-techno-altruism/</link>
      <pubDate>Sun, 16 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/techno-optimism-techno-altruism/</guid>
      <description>&lt;p&gt;You might think the average worker in the tech industry is a diehard capitalist, since tech is full of rich entrepreneurs. Moreover, many big tech companies appear to be driven more by money-making than by providing consumer value: Facebook didn&amp;rsquo;t connect us with friends and family and OpenAI didn&amp;rsquo;t remain a safety-oriented non-profit.&lt;/p&gt;&#xA;&lt;p&gt;However, most tech hubs are overwhelmingly liberal: 64% of voters in San Fransisco are registered as democrats; 8% as republican. Many programmers also dream to create widely accessible software, software for anyone, rich or poor, with stable Internet connection.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What effective altruism misses</title>
      <link>http://localhost:55424/what-effective-altruism-misses/</link>
      <pubDate>Sun, 09 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/what-effective-altruism-misses/</guid>
      <description>&lt;p&gt;Effective altruism (EA) centres around a dozen or so &amp;ldquo;cause areas&amp;rdquo;, areas in which you can have a big positive impact. Common cause areas include AI safety, global poverty, farm animal welfare, biorisk, nuclear security and EA field building. As of a couple of years, AI safety is very hot within EA, with some of the EA golden boys, like Will MacAskill and Holden Karnofsky, making AI going well their top priority.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Sanremese</title>
      <link>http://localhost:55424/the-sanremese/</link>
      <pubDate>Sun, 02 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/the-sanremese/</guid>
      <description>&lt;p&gt;In Europe, you don&amp;rsquo;t have to travel far to experience a completely different culture. If you randomly select a point in Europe outside of Russia, you&amp;rsquo;re never further than &lt;a href=&#34;https://maps.app.goo.gl/oY6zXbdFeQCc6Xqo7&#34;&gt;10h by train from the nearest nation border&lt;/a&gt;. There are 44 countries and 24 official languages in Europe &amp;ndash; yet, the area of Europe is just 60% of the area of the American South.&lt;/p&gt;&#xA;&lt;p&gt;I expected Sanremo to be like some of the towns by the French Riviera I&amp;rsquo;d visited when living in Aix, only with better ravioli and friendlier people. Although I wasn&amp;rsquo;t entirely wrong, I was far from correct.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On cities</title>
      <link>http://localhost:55424/on-cities/</link>
      <pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-cities/</guid>
      <description>&lt;p&gt;The 17th century was a century of rapid human progress. There was the Scientific Revolution. Philosophers like John Locke and Thomas Hobbes laid the groundwork for modern political thought, while René Descartes and Blaise Pascal made important contributions to philosophy more broadly. Meanwhile, in the arts, William Shakespeare redefined English literature, while Rembrandt changed the way artists approach their subjects.&lt;/p&gt;&#xA;&lt;p&gt;The above mentioned people have something important in common, other than being extremely intelligent white men. They spent non-negligible parts of their lives in cities. In the 1600s, only &lt;a href=&#34;https://ourworldindata.org/grapher/long-term-urban-population-region?time=1500..latest&amp;amp;country=Eastern+Africa~Western+Europe~Korea~Central+Europe~OWID_EUR&#34;&gt;8%&lt;/a&gt; of the European population lived in cities. Many people spent their entire lives without setting foot in a big city. But there is a sense in which you need to spend some time in a city to become an influential historical figure, or at least if you wish to understand human societies. Cities tell thousands of tales about history, culture and economics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The mad scientist is probably mad</title>
      <link>http://localhost:55424/the-mad-scientist-is-probably-mad/</link>
      <pubDate>Sun, 19 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/the-mad-scientist-is-probably-mad/</guid>
      <description>&lt;p&gt;Many pioneering scientists were initially rejected by the scientific community. At first, the ideas of Galileo Galilei, Gregor Mendel and Geoffrey Hinton were ridiculed. There are many more examples of such &amp;ldquo;martyrs of science&amp;rdquo;, people who were cancelled and then vindicated.&lt;/p&gt;&#xA;&lt;p&gt;Science isn&amp;rsquo;t as dramatic nowadays. There aren&amp;rsquo;t as many Galilei-like figures, at least not in relative terms. If 1/1000 of crazy-seeming scientists turned out to be geniuses in the 20th century, today the proportion might be more like 1/1 000 000. Why is that?&lt;/p&gt;</description>
    </item>
    <item>
      <title>High school for heroes</title>
      <link>http://localhost:55424/high-school-for-heroes/</link>
      <pubDate>Sun, 12 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/high-school-for-heroes/</guid>
      <description>&lt;p&gt;High school isn&amp;rsquo;t so much about preparing teenagers for adult life. In Sweden, you have a subject about cooking and personal finance, but only until secondary school. I think this makes sense: you only need one course on practical adult skills. After you know the basics, it&amp;rsquo;s all about practising IRL.&lt;/p&gt;&#xA;&lt;p&gt;But being a competent grown-up involves many other skills. It&amp;rsquo;s not just about successfully running a household &amp;ndash; it&amp;rsquo;s also a matter of &amp;ldquo;running yourself&amp;rdquo;. There are plenty of &amp;ldquo;serious&amp;rdquo; subjects we could fit in a standard high school curriculum. Here&amp;rsquo;s a vibe-based list:&lt;/p&gt;</description>
    </item>
    <item>
      <title>A study on slack</title>
      <link>http://localhost:55424/a-study-on-slack/</link>
      <pubDate>Sun, 05 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/a-study-on-slack/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Once I was done with exams, it felt like getting my brain back. It&amp;rsquo;s not just that you spend most of your day studying difficult subjects, leaving you with less time and willpower for other tasks. You also have to do cost-benefit analysis all the time. All. The. Time. Sometimes, it&amp;rsquo;s relieving if someone tells you exactly what to do&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;ii&#34;&gt;II. &lt;a href=&#34;#ii&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Slack is sometimes defined as having leftover resources. Intuitively, slack is like a buffer allowing you to absorb shocks better. We often speak of slack in the context of time and money. But to make things more visceral, imagine packing a bag. Having slack means having leftover space. There&amp;rsquo;s no need to stuff your shoes with underwear, fold your clothes meticulously or pack things in a given order. You don&amp;rsquo;t have to engage in trade-off thinking (&amp;ldquo;If I leave my camera, I can fit my notebook and an umbrella&amp;rdquo;). Having slack leads to a feeling of abundance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Analytic podcast-listening</title>
      <link>http://localhost:55424/analytic-podcast-listening/</link>
      <pubDate>Sun, 28 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/analytic-podcast-listening/</guid>
      <description>&lt;p&gt;Nowadays, you can spend all your waking hours consuming valuable information. Pretty remarkable, when you come to think of it. Hint: AirPods Pro.&lt;/p&gt;&#xA;&lt;p&gt;Whenever you&amp;rsquo;re doing something not demanding your full attention, you can always turn on a podcast. You can listen to podcasts when commuting, working out or cooking&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Technically, you could have done something similar 50 years ago using a walkman. But it have been very high-effort. Today, we have dedicated podcast apps and noise-cancelling, wireless earplugs. As a result, the podcast industry is rapidly growing in size. If you sample 100 young adults from the street in an urban area, a substantial number of them might be listening to podcasts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI safety lingo</title>
      <link>http://localhost:55424/ai-safety-lingo/</link>
      <pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/ai-safety-lingo/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I&amp;rsquo;ll omit the exact definitions. I&amp;rsquo;ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I&amp;rsquo;ll focus on intuitions.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents&#34;&gt;Embedded agency&lt;/a&gt;: Playing the Sims is very different from living IRL. When playing a video game, you&amp;rsquo;re not an agent embedded in the game environment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction&#34;&gt;Base optimizer vs. mesa-optimizer&lt;/a&gt;: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1&#34;&gt;Inner alignment vs. outer alignment&lt;/a&gt;: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/deceptive-alignment&#34;&gt;Deceptive alignment&lt;/a&gt;: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/treacherous-turn&#34;&gt;Treacherous turn&lt;/a&gt;: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to&amp;hellip; I don&amp;rsquo;t know, but people imagine something bad.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/corrigibility-1&#34;&gt;Corrigibility&lt;/a&gt;: Ever been in the library when someone&amp;rsquo;s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we&amp;rsquo;d speak of a non-corrigible phone: a phone that resists attempts to &amp;ldquo;correct&amp;rdquo; its behaviour and resists attempts to be shut down.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.01790&#34;&gt;Goal misgeneralization&lt;/a&gt;: A pianist might practise Bach to please her pianist friends; however, when she&amp;rsquo;s at normal parties, people just want her to play &lt;em&gt;Let it Be&lt;/em&gt;. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/instrumental-convergence&#34;&gt;Edge instantiation&lt;/a&gt;: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Goodhart%27s_law&#34;&gt;Goodhart&amp;rsquo;s law&lt;/a&gt;: When a measure becomes a target, it ceases to be a good measure&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The phenomenon of studying for the test is an example of Goodhart&amp;rsquo;s law.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc&#34;&gt;Value learning&lt;/a&gt;: This is the broad project of teaching AIs human values. AI-rearing, essentially.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qo355ALvLRI&#34;&gt;Inverse reinforcement learning (IRL)&lt;/a&gt;: If you&amp;rsquo;re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2024-11-28-reward-hacking/&#34;&gt;Reward hacking/specification gaming&lt;/a&gt;: Or, finding legal loopholes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/instrumental-convergence&#34;&gt;Instrumental convergence&lt;/a&gt;: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_model_specification&#34;&gt;Model misspecification&lt;/a&gt;: If you think the colour of Alice&amp;rsquo;s shirt determines whether she&amp;rsquo;ll win over Bob in a game of pingpong, your model is misspecified. You&amp;rsquo;re making the wrong assumptions about the data generation mechanism.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/sAJnZY8pp2W3DR4mx/breaking-down-the-training-deployment-dichotomy&#34;&gt;Training distribution vs. deployment distribution&lt;/a&gt;: Regardless of how much a soccer player practises taking penalty kicks, she&amp;rsquo;ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/distributional-shifts&#34;&gt;Distributional shift&lt;/a&gt;: The shift from training conditions to a real match is a distributional shift.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1&#34;&gt;Reward model splintering&lt;/a&gt;: A strategy can fail when you switch to a more general setting. Student insider jokes won&amp;rsquo;t work on the average person on the street.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf&#34;&gt;Constitutional AI (CAI)&lt;/a&gt;: Tell, don&amp;rsquo;t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. The idea behind constitutional AI is to give AIs a &amp;ldquo;constitution&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&#34;&gt;Human feedback (RLHF)&lt;/a&gt;: A process for producing ideal leaders. The ideal leader studies people&amp;rsquo;s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://newsletter.safe.ai/p/ai-safety-newsletter-13&#34;&gt;Proxy gaming&lt;/a&gt;: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game &amp;ndash; that of recommending addictive content &amp;ndash; has been gamed.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/value-drift&#34;&gt;Value drift&lt;/a&gt;: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;Thanks to Atharva Nihalani for inspiring me to write this post.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI is not AI is not AI</title>
      <link>http://localhost:55424/ai-is-not-ai-is-not-ai/</link>
      <pubDate>Sun, 14 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/ai-is-not-ai-is-not-ai/</guid>
      <description>&lt;p&gt;There are plenty of misnomers in science and mathematics. Atoms aren&amp;rsquo;t indivisible. Hubble&amp;rsquo;s constant isn&amp;rsquo;t a constant. And in 9/10 cases, X&amp;rsquo;s theorem was usually first discovered by someone else (in 4/10 cases, it was discovered by Gauss). Another bad piece of terminology, according to some: &amp;ldquo;artificial intelligence&amp;rdquo; or AI.&lt;/p&gt;&#xA;&lt;p&gt;Given that we don&amp;rsquo;t have a good definition of human intelligence, the term &amp;ldquo;artificial intelligence&amp;rdquo; is inherently vague. Because AI sounds cool, people use the term quite liberally. Logistic regression in Excel? AI! But it&amp;rsquo;s unclear what qualifies as &amp;ldquo;intelligent enough&amp;rdquo;. As AI systems become more capable, we seem to raise the bar. Previously, calculators and spell checkers were considered artificial intelligence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Policy-making is complicated</title>
      <link>http://localhost:55424/policy-making-is-complicated/</link>
      <pubDate>Sun, 07 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/policy-making-is-complicated/</guid>
      <description>&lt;p&gt;Things are more complicated than they first seem, especially when it comes to policy-making. Should Germany build nuclear power plants to reduce their carbon emissions? Should there be global standards for content moderation on social media platforms? How should we best mitigate the risk of future global pandemics?&lt;/p&gt;&#xA;&lt;p&gt;I recently had the pleasure of getting to know more people working in policy and forecasting, and I quickly realised that I hadn&amp;rsquo;t internalised the complexity of the problems they face&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here, I&amp;rsquo;ll list some intuition pumps for the difficulty of policy-related questions. For concreteness, I&amp;rsquo;ll focus on the first of the three above topics, nuclear power in Germany.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From the AI company storybook</title>
      <link>http://localhost:55424/from-the-ai-company-storybook/</link>
      <pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/from-the-ai-company-storybook/</guid>
      <description>&lt;p&gt;AI companies are companies. The leading AI companies don&amp;rsquo;t want to be seen as companies, though. They call themselves AI labs. They have researchy names, like DeepMind and Meta AI. The best name: OpenAI. Almost sounds like a real &lt;a href=&#34;https://openai.com/index/introducing-openai/&#34;&gt;non-profit&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Most AI companies are products of Silicon Valley. Their leaders aren&amp;rsquo;t professors, but seasoned business executives. And this is a good thing. These companies wouldn&amp;rsquo;t produce nearly as much consumer value if they were led by researchers with no industry experience. Moreover, unlike normal research labs, AI companies need to make money, just as any other company.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nerd trying to adapt to an AI economy</title>
      <link>http://localhost:55424/nerd-trying-to-adapt-to-an-ai-economy/</link>
      <pubDate>Sun, 24 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/nerd-trying-to-adapt-to-an-ai-economy/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Part of the reason I decided to study maths was because it seemed like the most useful subject. If I knew ML systems would never get better than GPT-2 at maths, I&amp;rsquo;d probably be of the same opinion today. But today&amp;rsquo;s state-of-the-art ML systems are far better than GPT-2. LLMs have excelled at maths and programming because maths- and coding-related tasks admit quick feedback, allowing for efficient reinforcement learning. Jobs involving applying maths and programming jobs could theoretically be automated within a few decades. The glory days of the nerd might be over soon.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Man&#39;s search for fun</title>
      <link>http://localhost:55424/mans-search-for-fun/</link>
      <pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/mans-search-for-fun/</guid>
      <description>&lt;p&gt;Willpower is a scarce resource. I can only exert so much willpower in a day&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Alas, many important tasks require lot of willpower. Examples of such tasks include filing one&amp;rsquo;s taxes, memorising messy proofs and booking flights. Oftentimes, I&amp;rsquo;m bottlenecked by willpower rather than time. Rather than increasing willpower - finding Meaning - how about decreasing the amount of willpower required for a given task? This might be an easier problem. Here are my favourite ways of making boring tasks fun.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Let LLMs be LLMs</title>
      <link>http://localhost:55424/let-llms-be-llms/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/let-llms-be-llms/</guid>
      <description>&lt;h3 id=&#34;thought-experiment&#34;&gt;Thought experiment &lt;a href=&#34;#thought-experiment&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Imagine a very unusual kind of chatbot. In the user interface (UI), the text &amp;ldquo;This is a large language model. Its outputs are an aggregate of the text on the internet.&amp;rdquo; is displayed in bold, black letters, like the warning label on a pack of cigarettes.&lt;/p&gt;&#xA;&lt;p&gt;When the user inputs a string of text, the text &amp;ldquo;Matrix multiplications&amp;hellip;&amp;rdquo; flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren&amp;rsquo;t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label &amp;ldquo;Highest logit: output $7$&amp;rdquo; appears on the screen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On good hobbies</title>
      <link>http://localhost:55424/on-good-hobbies/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-good-hobbies/</guid>
      <description>&lt;p&gt;Everyone has hobbies, whether they recognise it or not. For example, workaholics who don&amp;rsquo;t seem to have any &amp;ldquo;normal&amp;rdquo; hobbies might be really into, say, writing slick SQL queries. Code golf is a hobby too. By definition, a hobby is something you do for pleasure when you&amp;rsquo;re not working. And for the record, writing slick SQL queries isn&amp;rsquo;t work.&lt;/p&gt;&#xA;&lt;p&gt;Some hobbies are better than others, though. Examples of not-as-good hobbies include motorcycling (dangerous), graffiti painting (illegal) and doom-scrolling social media (depressing). A benchmark for hobby quality: ask whether your mom would like it. So, which hobbies would your mom like?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sweden</title>
      <link>http://localhost:55424/sweden/</link>
      <pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/sweden/</guid>
      <description>&lt;p&gt;Everyone who has ever lived abroad gets homesick every now and then.&lt;/p&gt;&#xA;&lt;p&gt;Swedes, I give you a postcard from a Stockholm-centric, homesick expat. Non-Swedes, here&amp;rsquo;s a highly biased tourist guide.&lt;/p&gt;&#xA;&lt;p&gt;The best thing about Sweden? My favourite Swedes, for sure. I miss family and friends a lot.&lt;/p&gt;&#xA;&lt;p&gt;Other than that, hearing Swedish jazz live. Hearing &lt;em&gt;Sakta vi gå genom stan&lt;/em&gt; and &lt;em&gt;Flykten från vardagen&lt;/em&gt; live. Or if Glenn Miller or Fasching is too pricey, you could just drop by KMH and enjoy up-and-coming jazz, all for free.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Finding research influences</title>
      <link>http://localhost:55424/finding-research-influences/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/finding-research-influences/</guid>
      <description>&lt;p&gt;There isn&amp;rsquo;t any expert consensus on many &lt;a href=&#34;https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/&#34;&gt;key questions related to AI safety&lt;/a&gt;. For example, estimates of when we&amp;rsquo;ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.&lt;/p&gt;&#xA;&lt;p&gt;I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are &lt;a href=&#34;https://substack.com/@redwoodresearch?utm_source=about-page&#34;&gt;Buck Shleregis&lt;/a&gt;, &lt;a href=&#34;https://www.cold-takes.com/cold-takes-on-ai/&#34;&gt;Holden Karnofsky&lt;/a&gt; and &lt;a href=&#34;https://www.lesswrong.com/users/jan_kulveit?from=search_autocomplete&#34;&gt;Jan Kulveit&lt;/a&gt;. While I don&amp;rsquo;t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the perks of adulthood</title>
      <link>http://localhost:55424/on-the-perks-of-adulthood/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-the-perks-of-adulthood/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;I know you dread becoming an adult. Most adults seem to have such boring lives. Not that you&amp;rsquo;re having fun all the time either. High school biology isn&amp;rsquo;t that exciting. But at least your life outside of school is filled with novelty. That&amp;rsquo;s something. There&amp;rsquo;s something special about all those first times.&lt;/p&gt;&#xA;&lt;p&gt;If you&amp;rsquo;re just seeking novelty, you&amp;rsquo;ll be disappointed. Maybe that&amp;rsquo;s the real reason you dread adulthood. While I don&amp;rsquo;t feel like an adult just yet, many of my friends have become Real Adults, being in stable relationships, getting high-paid jobs and moving abroad. So I&amp;rsquo;m beginning to understand the adult world. It&amp;rsquo;s not as bad as it seems, only very different.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The spectrum of views on AI safety</title>
      <link>http://localhost:55424/the-spectrum-of-views-on-ai-safety/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/the-spectrum-of-views-on-ai-safety/</guid>
      <description>&lt;p&gt;I agree the concept of &lt;a href=&#34;https://en.wikipedia.org/wiki/P(doom)&#34;&gt;P(doom)&lt;/a&gt; is problematic. First, &amp;ldquo;doom&amp;rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.&lt;/p&gt;&#xA;&lt;p&gt;But perhaps we&amp;rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they&amp;rsquo;re just interested in hearing general takes on AI safety, at least in my experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On my relation to effective altruism</title>
      <link>http://localhost:55424/on-my-relation-to-effective-altruism/</link>
      <pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-my-relation-to-effective-altruism/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve spent more time engaging with the effective altruist (EA) community this year. Not just reading EA books and blog posts, but participating in seminars, attending conferences and going on EA retreats. For context, I&amp;rsquo;d viewed myself &amp;ldquo;EA adjacent&amp;rdquo; ever since I came across &lt;em&gt;The Life You Can Save&lt;/em&gt; back in high school. However, during my master degree, the prospect of graduating soon - of becoming an adult - made me reflect more carefully on EA.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On question taste</title>
      <link>http://localhost:55424/on-question-taste/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-question-taste/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Children like asking questions. Some of their questions are very hard: &amp;ldquo;Why do people die?&amp;rdquo;. Others questions expose our biases: &amp;ldquo;Couldn&amp;rsquo;t we invite the homeless man to dinner?&amp;rdquo; Then there are all the annoying questions: &amp;ldquo;When will we arrive?&amp;rdquo;&lt;/p&gt;&#xA;&lt;p&gt;In my experience, school didn&amp;rsquo;t teach us to ask questions - we were just taught how to answer them. To pass the test, you only had to memorise the material in the textbook.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A taxonomy of examples</title>
      <link>http://localhost:55424/a-taxonomy-of-examples/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/a-taxonomy-of-examples/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m mildly obsessed with examples. Whenever I feel confused, it&amp;rsquo;s often because I don&amp;rsquo;t know enough examples. Proofs can be confusing too, if some step is poorly explained. But that kind of confusion tends to be local. You can still have a good grasp of the theory. If you don&amp;rsquo;t know enough examples, you feel generally lost. Textbooks lacking in examples end up being dry. In contrast, books with well-chosen examples are a pleasure to read&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the joy of customising</title>
      <link>http://localhost:55424/on-the-joy-of-customising/</link>
      <pubDate>Sun, 08 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-the-joy-of-customising/</guid>
      <description>&lt;p&gt;Why do so many people use Safari, the jarring black VSCode colour theme and Overleaf? Because it&amp;rsquo;s the default option.&lt;/p&gt;&#xA;&lt;p&gt;Switching to a non-default option requires conscious effort. First, you need to realise that your current setup is suboptimal: the probability of your current setup being the optimal one is infinitesimal. But if you knew about the benefits of making a few small changes, you&amp;rsquo;d already have made them. So you do need to believe that switching is worthwhile, which, indeed, is a leap of faith.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Consume less AI safety news</title>
      <link>http://localhost:55424/consume-less-ai-safety-news/</link>
      <pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/consume-less-ai-safety-news/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s hard staying on top of all the AI safety news. Some people, like &lt;a href=&#34;https://thezvi.wordpress.com/about/&#34;&gt;Zvi&lt;/a&gt;, have basically made this their full-time job.&lt;/p&gt;&#xA;&lt;p&gt;A common failure mode for forming views on AI safety is consuming too much information. It&amp;rsquo;s a tendency I&amp;rsquo;ve observed in myself, as well as in others in the AI safety community.&lt;/p&gt;&#xA;&lt;p&gt;I think it comes from the urge to solve the AI alignment problem quickly. It&amp;rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there&amp;rsquo;s a lot of general excitement surrounding the area.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding the AI alignment problem</title>
      <link>http://localhost:55424/understanding-the-ai-alignment-problem/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/understanding-the-ai-alignment-problem/</guid>
      <description>&lt;p&gt;Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic &lt;a href=&#34;https://www.anthropic.com/news/core-views-on-ai-safety#:~:text=build%20safe%2C%20reliable%2C%20and%20steerable%20systems%20when%20those%20systems%20are%20starting%20to%20become%20as%20intelligent%20and%20as%20aware%20of%20their%20surroundings%20as%20their%20designers&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it&amp;rsquo;s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A semester paper retrospective</title>
      <link>http://localhost:55424/a-semester-paper-retrospective/</link>
      <pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/a-semester-paper-retrospective/</guid>
      <description>&lt;p&gt;This is a follow-up on my &lt;a href=&#34;https://isabeldahlgren.github.io/two-results-from-probabilistic-number-theory/&#34;&gt;previous article&lt;/a&gt;, where I share some thoughts on the process of writing a semester paper.&lt;/p&gt;&#xA;&lt;p&gt;Lots of disclaimers: this is all very specific to my experience: the topic of my paper, my supervisor, my workload in others courses, etc. Also, I&amp;rsquo;m certainly in no position to give advice on mathematical writing; these are just reflections on what worked and what didn&amp;rsquo;t work for me.&lt;/p&gt;&#xA;&lt;h3 id=&#34;on-writing-papers&#34;&gt;On writing papers &lt;a href=&#34;#on-writing-papers&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Here are some principles for writing mathematical papers that I tried to follow, mostly based on feedback from my supervisors. I received plenty of useful feedback, but here are the points I found the most useful:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two results from probabilistic number theory</title>
      <link>http://localhost:55424/two-results-from-probabilistic-number-theory/</link>
      <pubDate>Sun, 11 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/two-results-from-probabilistic-number-theory/</guid>
      <description>&lt;p&gt;I recently wrote a semester paper on probabilistic number theory. I&amp;rsquo;m very grateful to both of my supervisors, Dr. Vivian Kuperberg and Prof. Dr. Emmanuel Kowalski, for their insights and suggestions. Here&amp;rsquo;s the abstract:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;This report is an exposition of two central limit theorems in probabilistic number theory. We begin by introducing preliminary results from number theory and probability theory. Then we prove the Erdős-Kac theorem for the asymptotic behaviour of the prime divisor counting function. The majority of the report is devoted to Radziwiłł and Soundarajan&amp;rsquo;s recent proof of the Selberg central limit theorem for $\log |\zeta(\frac{1}{2} + it)|$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On decision fatigue</title>
      <link>http://localhost:55424/on-decision-fatigue/</link>
      <pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-decision-fatigue/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Life involves plenty of hard decisions. Should I switch jobs? Should I marry him? Should I move abroad? Then there are the &amp;ldquo;easy&amp;rdquo; decisions: which pyjama to wear or which brand of laundry detergent to buy. Although it doesn&amp;rsquo;t matter which option you choose here, choosing is hard. In fact, if you think of buying laundry detergent as an optimisation problem, taking into account things like social impact, price and quality, I&amp;rsquo;d expect the optimisation problem to be NP-hard! Here&amp;rsquo;s &lt;a href=&#34;https://thezvi.wordpress.com/2017/07/22/choices-are-bad/&#34;&gt;Zvi&lt;/a&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>On good conversations</title>
      <link>http://localhost:55424/on-good-conversations/</link>
      <pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/on-good-conversations/</guid>
      <description>&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Some studies suggest we spend 80-90% of our waking hours talking with others. For a normal person, that&amp;rsquo;s at least 12 hours (!) per day. We spend most of our working hours talking to others, whether it be in meetings, lectures or exercise classes. When we&amp;rsquo;re not working, friends, roommates or partners might be around. The hours add up.&lt;/p&gt;&#xA;&lt;p&gt;Why do we spend this much time talking with others? Above all, interesting conversations are among the greatest pleasures in life. There&amp;rsquo;s nothing like a good conversation over a good meal. But talking with others can also be very productive. For example, talking with people smarter than you is a phenomenal way of learning. It&amp;rsquo;s like you&amp;rsquo;re downloading part of someone else&amp;rsquo;s worldview into your own head. Of course, there are other ways of achieving the same thing. However, in my opinion, one-on-one discussions come the closest to a loss-free download.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building time machines</title>
      <link>http://localhost:55424/building-time-machines/</link>
      <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/building-time-machines/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a folder on my computer called &amp;ldquo;Time machine&amp;rdquo;, where I collect files on today&amp;rsquo;s me. It&amp;rsquo;s not exactly an FBI record on myself, but rather like a digital self portrait. After doing this over a longer period of time, I&amp;rsquo;ll share the folder with future versions of people I care about.&lt;/p&gt;&#xA;&lt;h3 id=&#34;i&#34;&gt;I. &lt;a href=&#34;#i&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;I like poring over old photos. I think it&amp;rsquo;s something I&amp;rsquo;ve picked up from my mom. She&amp;rsquo;s something of a camera terrorist &amp;ndash; she&amp;rsquo;ll always pull up the camera and have everyone else pose. As a teenager, I used to find this pretty embarrassing. But now I&amp;rsquo;m glad she ignored my complaints.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How I use LLMs</title>
      <link>http://localhost:55424/how-i-use-llms/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/how-i-use-llms/</guid>
      <description>&lt;p&gt;&lt;em&gt;Related: &lt;a href=&#34;https://isabeldahlgren.github.io/will-ai-replace-mathematicians/&#34;&gt;Will AI replace mathematicians?&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;About 40% of students in the ETH main library always have a Chat-GPT tab open. I soon decided to try using LLMs for my own studies (because the wisdom of the crowd is a real thing). I haven&amp;rsquo;t figured out how to best use LLMs for my coursework, but I&amp;rsquo;m experimenting with various approaches.&lt;/p&gt;&#xA;&lt;h3 id=&#34;getting-unstuck&#34;&gt;Getting unstuck &lt;a href=&#34;#getting-unstuck&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;For me, a big time sink is getting stuck on details. I usually go over the lecture notes after lectures, trying to work out the steps I didn&amp;rsquo;t follow with pen and paper. Ideally, I&amp;rsquo;d do this sitting next to a friend - it&amp;rsquo;s very convenient having someone whom to ask nearby. As Nate Soares put it &lt;a href=&#34;https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Will AI replace mathematicians?</title>
      <link>http://localhost:55424/will-ai-replace-mathematicians/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/will-ai-replace-mathematicians/</guid>
      <description>&lt;p&gt;I used to think of maths as the one thing LLMs couldn&amp;rsquo;t do well. While GPT 3.0 would excel in language-based tasks, it would struggle to solve elementary maths problems. But a lot has happened since then. Over the last year, I&amp;rsquo;ve come to take the idea of using AI as an aid for doing maths more seriously. In fact, I now believe LLMs might prove new theorems with no human guidance within just 3 years.&lt;/p&gt;</description>
    </item>
    <item>
      <title>&#34;Just switch it off&#34;</title>
      <link>http://localhost:55424/just-switch-it-off/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/just-switch-it-off/</guid>
      <description>&lt;p&gt;If we develop a rogue AI, couldn&amp;rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &amp;ldquo;switching off&amp;rdquo; would mean deleting a model&amp;rsquo;s weights, so it can&amp;rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?&lt;/p&gt;&#xA;&lt;p&gt;First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&amp;rsquo;t &amp;ldquo;want&amp;rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&amp;rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;&amp;ldquo;alignment faking&amp;rdquo;&lt;/a&gt;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From a YouTube alumna</title>
      <link>http://localhost:55424/from-a-youtube-alumna/</link>
      <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/from-a-youtube-alumna/</guid>
      <description>&lt;p&gt;Random people on the Internet have played a huge role in my education. I&amp;rsquo;m not just referring to my coursework at university, but also to &amp;ldquo;Bildung&amp;rdquo; more generally. I&amp;rsquo;ve learned a ton by browsing StackOverflow threads and reading Medium articles. However, I&amp;rsquo;ve probably learned the most from watching YouTube.&lt;/p&gt;&#xA;&lt;h3 id=&#34;learning-by-watching&#34;&gt;Learning by watching &lt;a href=&#34;#learning-by-watching&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Above all, there&amp;rsquo;s some really high-quality content out there. Nowadays there are full-time YouTubers, working on creating professional, meticulously edited videos. And some channels, such as &lt;a href=&#34;https://www.youtube.com/channel/UCsXVk37bltHxD1rDPwtNM8Q&#34;&gt;Kurzgesagt&lt;/a&gt;, are even ran by entire teams of illustrators and script-writers. Moreover, because anyone can record themselves and upload it to YouTube, we have world-class experts sharing their knowledge in YouTube lectures&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This means there are some truly remarkable YouTube videos. For example, here are comments from some &lt;a href=&#34;https://www.youtube.com/@3blue1brown&#34;&gt;3Blue1Brown&lt;/a&gt; videos:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Look for opinions</title>
      <link>http://localhost:55424/look-for-opinions/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/look-for-opinions/</guid>
      <description>&lt;p&gt;Opinionated people can be really annoying. Wherever they go, they try convincing you of their ideas. If you have an opinionated uncle, the Christmas dinner might be ruined by a bitter argument. I&amp;rsquo;ve certainly had bad experiences with a dinner-table conversations turning into feuds. For this reason, I used to try having fewer opinions. I somehow assumed this meant being more open-minded and mature. Well, no.&lt;/p&gt;&#xA;&lt;h3 id=&#34;welcoming-opinions&#34;&gt;Welcoming opinions &lt;a href=&#34;#welcoming-opinions&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Actually, there are plenty of benefits of actively trying to form more opinions. Even about topics you don&amp;rsquo;t know particularly well. If you learn with a view towards arguing, then you&amp;rsquo;ll pay closer attention to the material. I think this has to do with anchoring. If you pick a stance, even at random, you&amp;rsquo;ll be more emotionally invested. Holden Karnofsky summarised it neatly in &lt;a href=&#34;https://www.cold-takes.com/learning-by-writing/&#34;&gt;Learning by Writing&lt;/a&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How AI might go wrong</title>
      <link>http://localhost:55424/how-ai-might-go-wrong/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/how-ai-might-go-wrong/</guid>
      <description>&lt;p&gt;As with any new technology, advanced AI entails certain risks. While we&amp;rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.&lt;/p&gt;&#xA;&lt;p&gt;The other week, I had an insightful discussion with people in &lt;a href=&#34;https://www.zurich-ai-alignment.com&#34;&gt;Zürich AI Alignment (ZAIA)&lt;/a&gt; about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Useful thought experiments</title>
      <link>http://localhost:55424/useful-thought-experiments/</link>
      <pubDate>Sun, 02 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/useful-thought-experiments/</guid>
      <description>&lt;p&gt;Philosophers love thought experiments. Thought experiments are hypothetical scenarios meant to tease out our intuitions about an argument or theory. For example, here&amp;rsquo;s a classic thought experiment, due to Robert Nozick:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Suppose there were an experience machine that would give you any experience you desired. Superduper neuropsychologists could stimulate your brain so that you would think and feel you were writing a great novel, or making a friend, or reading an interesting book. All the time you would be floating in a tank, with electrodes attached to your brain. Should you plug into this machine for life, preprogramming your life&amp;rsquo;s experiences?&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hunting dependencies</title>
      <link>http://localhost:55424/hunting-dependencies/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:55424/hunting-dependencies/</guid>
      <description>&lt;p&gt;One of the benefits of attending lectures is that lecturers tend to give some really good unsolicited advice every now and then. Last semester, a professor of ours digressed to talk about the importance of identifying the key ingredients of a result. He&amp;rsquo;d just concluded a rather long proof, and was about to clean the blackboard as he said that the result was elementary. See, you only need this one lemma from point-set topology (admittedly a bit niche, but easy to prove), and then the definition of the Fourier transform. Put that way, sure, maybe it&amp;rsquo;s elementary. Explicitly writing down the dependencies of an idea, he said, was a good exercise.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
