<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              
              
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              
              throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/dd2352/" />
    <title>DD2352</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>DD2352</h1>
        <div>
                <time>June 7, 2023</time>
            </div>
    </header><p>These notes are based on the <a href="https://canvas.kth.se/courses/44839">slides</a> of P. Austrin. All errors are mine.</p>
<!-- mtoc-start -->
<ul>
<li><a href="#lecture-1">Lecture 1</a>
<ul>
<li><a href="#examples-of-algorithms">Examples of algorithms</a>
<ul>
<li><a href="#time-complexity">Time complexity</a></li>
<li><a href="#finding-the-time-complexity">Finding the time complexity</a></li>
<li><a href="#graph-algorithms">Graph algorithms</a></li>
</ul>
</li>
<li><a href="#lecture-2">Lecture 2</a>
<ul>
<li><a href="#prelude">Prelude</a></li>
<li><a href="#heuristics-for-greedy-algorithms">Heuristics for greedy algorithms</a></li>
<li><a href="#interval-scheduling">Interval scheduling</a></li>
<li><a href="#job-scheduling-with-minimum-lateness">Job scheduling with minimum lateness</a></li>
<li><a href="#greedy-graph-algorithm">Greedy graph algorithm</a></li>
</ul>
</li>
<li><a href="#lecture-3">Lecture 3</a>
<ul>
<li><a href="#prelude-1">Prelude</a></li>
<li><a href="#merge-sort">Merge sort</a></li>
<li><a href="#polynomial-multiplication">Polynomial multiplication</a></li>
<li><a href="#bit-and-unit-cost">Bit and unit cost</a></li>
<li><a href="#karatsubas-algorithm">Karatsuba&rsquo;s algorithm</a></li>
<li><a href="#the-fast-fourier-transform">The Fast Fourier Transform</a></li>
<li><a href="#the-master-theorem">The Master theorem</a></li>
</ul>
</li>
<li><a href="#lecture-4">Lecture 4</a>
<ul>
<li><a href="#prelude-2">Prelude</a></li>
<li><a href="#fibonacci">Fibonacci</a></li>
<li><a href="#weighted-interval-scheduling">Weighted interval scheduling</a></li>
<li><a href="#knapsack">Knapsack</a></li>
<li><a href="#top-down-or-bottom-up">Top-down or bottom-up</a></li>
<li><a href="#heuristics-for-dynamic-programming">Heuristics for dynamic programming</a></li>
</ul>
</li>
<li><a href="#lecture-5">Lecture 5</a>
<ul>
<li><a href="#heuristics-for-dynamic-programming-continued">Heuristics for dynamic programming, continued</a></li>
<li><a href="#constructing-solutions">Constructing solutions</a></li>
<li><a href="#sequence-alignment">Sequence alignment</a></li>
<li><a href="#matrix-chain-multiplication">Matrix chain multiplication</a></li>
<li><a href="#implementing-bottom-up">Implementing bottom-up</a></li>
</ul>
</li>
<li><a href="#lecture-6">Lecture 6</a>
<ul>
<li><a href="#flow-networks">Flow networks</a></li>
<li><a href="#maximum-flows">Maximum flows</a></li>
<li><a href="#edmonds-karp">Edmonds-Karp</a></li>
<li><a href="#minimum-cuts">Minimum cuts</a></li>
<li><a href="#the-max-flow-min-cut-theorem">The Max Flow Min Cut theorem</a></li>
</ul>
</li>
<li><a href="#lecture-7">Lecture 7</a>
<ul>
<li><a href="#prelude-3">Prelude</a></li>
<li><a href="#vertex-cut">Vertex Cut</a></li>
<li><a href="#many-sources-or-sinks">Many sources or sinks</a></li>
<li><a href="#matching-numbers">Matching numbers</a></li>
<li><a href="#disjoint-paths">Disjoint paths</a></li>
<li><a href="#closure">Closure</a></li>
</ul>
</li>
<li><a href="#lecture-8">Lecture 8</a>
<ul>
<li><a href="#randomised-algorithms">Randomised algorithms</a></li>
<li><a href="#random-number-generators">Random number generators</a></li>
<li><a href="#approximation-algorithms">Approximation algorithms</a></li>
<li><a href="#global-minimum-cut">Global Minimum Cut</a></li>
<li><a href="#global-maximum-cut">Global Maximum Cut</a></li>
</ul>
</li>
<li><a href="#lecture-9">Lecture 9</a>
<ul>
<li><a href="#prelude-4">Prelude</a></li>
<li><a href="#turing-machines">Turing machines</a></li>
<li><a href="#ram">RAM</a></li>
<li><a href="#models-of-computation">Models of computation</a></li>
<li><a href="#restricted-models-of-computation">Restricted models of computation</a></li>
<li><a href="#comparison-based-sorting">Comparison-based sorting</a></li>
</ul>
</li>
<li><a href="#lecture-10">Lecture 10</a>
<ul>
<li><a href="#prelude-5">Prelude</a></li>
<li><a href="#heuristics-for-transforming-problems">Heuristics for transforming problems</a></li>
<li><a href="#halting">Halting</a></li>
<li><a href="#undecidable-problems">Undecidable problems</a></li>
<li><a href="#turing-reductions">Turing reductions</a></li>
<li><a href="#examples-of-reductions">Examples of reductions</a></li>
<li><a href="#3-sat">3-SAT</a></li>
<li><a href="#karp-reductions">Karp reductions</a></li>
</ul>
</li>
<li><a href="#lecture-11">Lecture 11</a>
<ul>
<li><a href="#prelude-6">Prelude</a></li>
<li><a href="#problems-as-languages">Problems as languages</a></li>
<li><a href="#p">P</a></li>
<li><a href="#np">NP</a></li>
<li><a href="#p-vs-np">P vs. NP</a></li>
<li><a href="#np-completeness">NP-completeness</a></li>
<li><a href="#np-complete-problems">NP-complete problems</a></li>
</ul>
</li>
<li><a href="#lecture-12">Lecture 12</a>
<ul>
<li><a href="#prelude-7">Prelude</a></li>
<li><a href="#hamiltonian-cycle">Hamiltonian Cycle</a></li>
<li><a href="#travelling-salesman">Travelling salesman</a></li>
<li><a href="#graph-colourings">Graph colourings</a></li>
<li><a href="#subset-sum">Subset sum</a></li>
<li><a href="#knapsack-1">Knapsack</a></li>
<li><a href="#summary-np-completeness">Summary NP-completeness</a></li>
<li><a href="#understanding-the-definition-of-np">Understanding the definition of NP</a></li>
<li><a href="#proof-sketch-of-cook-levin">Proof sketch of Cook-Levin</a></li>
</ul>
</li>
<li><a href="#lecture-13">Lecture 13</a>
<ul>
<li><a href="#conp">CoNP</a></li>
<li><a href="#conp-hardness">CoNP-hardness</a></li>
<li><a href="#p-np--conp">P, NP &amp; CoNP</a></li>
<li><a href="#pspace">PSPACE</a></li>
<li><a href="#pspace-hardness">PSPACE-hardness</a></li>
<li><a href="#venn-diagram">Venn diagram</a></li>
<li><a href="#bpp--zpp">BPP &amp; ZPP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- mtoc-end -->
<h3 id="lecture-1">Lecture 1</h3>
<p>2024-01-16</p>
<h4 id="examples-of-algorithms">Examples of algorithms</h4>
<ul>
<li><strong>Algo.</strong> (Insertion sort). Insert elements into their correct position.</li>
<li><strong>Algo.</strong> (Merge sort). Divide into halves, sort, merge.</li>
</ul>
<h4 id="time-complexity">Time complexity</h4>
<ul>
<li><strong>Q.</strong> How do we measure the efficiency of an algorithm?</li>
<li><strong>Def.</strong> $O(g(n))$
<ul>
<li>There exist $c$ and $N$ s.t. $f(n) \le c g(n)$ for $n \ge N$.</li>
</ul>
</li>
<li><strong>Def.</strong> $\Omega(g(n))$
<ul>
<li>$g(n) = O(f(n))$.</li>
</ul>
</li>
<li><strong>Def.</strong> $\Theta(g(n))$
<ul>
<li>$f(n) = O(g(n))$ and $g(n) = O(f(n))$.</li>
</ul>
</li>
<li><strong>Q.</strong> How should we think of $O$, $\Omega$ and $\Theta$?
<ul>
<li>$O$ means $\le$, $\Omega$ means $\ge$ and $\Theta$ means $=$.</li>
</ul>
</li>
<li><strong>Ex.</strong> Basic techniques for comparing time complexities
<ul>
<li>Taking ratios, using L&rsquo;HÃ´pital&rsquo;s rule.</li>
</ul>
</li>
</ul>
<h4 id="finding-the-time-complexity">Finding the time complexity</h4>
<ul>
<li><strong>Q.</strong> How do we compute runtime?
<ul>
<li>Look for loops and list insertions.</li>
</ul>
</li>
<li><strong>Ex.</strong> List insertions making a difference
<ul>
<li>Insertion sort.</li>
</ul>
</li>
</ul>
<h4 id="graph-algorithms">Graph algorithms</h4>
<ul>
<li><a href="https://web.stanford.edu/class/cs97si/">cf.</a></li>
<li><strong>Q.</strong> How can we represent graphs?
<ul>
<li>Adjacency matrix, adjacency list, incidence matrix.</li>
</ul>
</li>
<li><strong>Q.</strong> What are two fundamental graph exploration algorithms?
<ul>
<li>DFS, BFS.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we think of the DFS and BFS trees?
<ul>
<li>They log how the algorithm worked.</li>
</ul>
</li>
<li><strong>Q.</strong> DFS or BFS?
<ul>
<li><a href="https://www.reddit.com/r/leetcode/comments/uhiitd/when_to_use_bfs_vs_dfs_in_graphs/">cf.</a></li>
</ul>
</li>
</ul>
<h3 id="lecture-2">Lecture 2</h3>
<p>2024-01-17</p>
<h4 id="prelude">Prelude</h4>
<ul>
<li><strong>Q.</strong> What is a greedy algorithm?
<ul>
<li>Builds a solution by always choosing the currently best option (whatever that is).</li>
</ul>
</li>
<li><strong>Q.</strong> What are some characteristics of a greedy algorithm?
<ul>
<li>Fast and simple, difficult to prove correct.</li>
</ul>
</li>
<li><strong>Q.</strong> How do we prove a greedy algorithm correct?
<ul>
<li>Stay ahead argument or exchange argument.</li>
<li>View the optimal solution as an algorithm that runs in parallel with the greedy algorithm.</li>
</ul>
</li>
</ul>
<h4 id="heuristics-for-greedy-algorithms">Heuristics for greedy algorithms</h4>
<ul>
<li><strong>Q.</strong> Which kinds of problems often admit a greedy solution?
<ul>
<li>Scheduling problems, some graph algorithms.</li>
</ul>
</li>
<li><strong>Q.</strong> How do we design a greedy algorithm?
<ul>
<li>To find a correct greedy rule, use intuition and trial and error.</li>
<li>Look for what is fast, stays ahead or frees up resources.</li>
</ul>
</li>
</ul>
<h4 id="interval-scheduling">Interval scheduling</h4>
<ul>
<li><strong>Q.</strong> What steps are there in a stay ahead argument? <a href="https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf">cf.</a>
<ul>
<li>Prove that $\mu(a_i) \le \mu(o_i)$ or vice versa.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Interval Scheduling). Find the maximum number of nonoverlapping intervals.</li>
<li><strong>Ex.</strong> Solving Interval Scheduling
<ul>
<li>Always pick the interval ending first.</li>
</ul>
</li>
<li><em>Proof.</em> (Correctness). Stay ahead.</li>
</ul>
<h4 id="job-scheduling-with-minimum-lateness">Job scheduling with minimum lateness</h4>
<ul>
<li><strong>Q.</strong> What steps are there in an exchange argument? <a href="https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf">cf.</a>
<ul>
<li>Find where $\mathcal{A}$ and $\mathcal{O}$ differ, exchange.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Scheduling to Minimise Lateness). Find the minimum maximum lateness.</li>
<li><strong>Ex.</strong> Solving Scheduling to Minimise Lateness
<ul>
<li>Always pick the job with the earliest deadline.</li>
</ul>
</li>
<li><em>Proof.</em> (Correctness). Exchange.</li>
</ul>
<h4 id="greedy-graph-algorithm">Greedy graph algorithm</h4>
<ul>
<li><strong>Prob.</strong> (Shortest Path). Find the shortest path from a source $s$ to any other vertex.</li>
<li><strong>Algo.</strong> (Dijkstra).
<ul>
<li>Add the vertex outside $U$ with is closest to $s$.</li>
</ul>
</li>
</ul>
<h3 id="lecture-3">Lecture 3</h3>
<p>2024-01-22</p>
<h4 id="prelude-1">Prelude</h4>
<ul>
<li><strong>Q.</strong> How do divide and conquer algorithms work?
<ul>
<li>Divide input, recursively solve each part, combine.</li>
</ul>
</li>
<li><strong>Q.</strong> How do we draw the tree for $T(n) = aT(n/b) + f(n)$?
<ul>
<li>$\log_b n$ levels, $a^i$ tasks on level $i$.</li>
</ul>
</li>
<li><strong>Q.</strong> How do we analyse the time complexity of a divide and conquer algorithm?
<ul>
<li>Master thm. or sum over levels.</li>
</ul>
</li>
</ul>
<h4 id="merge-sort">Merge sort</h4>
<ul>
<li><strong>Ex.</strong> Recurrence for Merge sort</li>
</ul>
<h4 id="polynomial-multiplication">Polynomial multiplication</h4>
<ul>
<li><strong>Prob.</strong> (Polynomial Multiplication). Find the coefficients of $p(x)q(x)$.</li>
<li><strong>Q.</strong> How can we divide and conquer Polynomial Multiplication?
<ul>
<li>Write $p = p_h x^{n/2} + p_l$.</li>
</ul>
</li>
<li><strong>Ex.</strong> Attempt to solve Polynomial Multiplication</li>
<li><strong>Ex.</strong> Solving Polynomial Multiplication
<ul>
<li>Rewrite the middle term.</li>
</ul>
</li>
<li><strong>Ex.</strong> The constant $a$ matters</li>
</ul>
<h4 id="bit-and-unit-cost">Bit and unit cost</h4>
<ul>
<li><strong>Q.</strong> When can arithmetic operations be seen as basic?
<ul>
<li>If they fit in machine words, i.e. are smaller than $2^{64}$ bits.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the difference between unit cost and bit cost?</li>
</ul>
<h4 id="karatsubas-algorithm">Karatsuba&rsquo;s algorithm</h4>
<ul>
<li><strong>Ex.</strong> Schoolbook multiplication algorithm</li>
<li><strong>Q.</strong> How can we divide and conquer Integer Multiplication?
<ul>
<li>Like polynomial multiplication. Evaluate at $x = 10$.</li>
</ul>
</li>
<li><strong>Algo.</strong> (Karatsuba).</li>
</ul>
<h4 id="the-fast-fourier-transform">The Fast Fourier Transform</h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=h7apO7q16V0">cf.</a></li>
<li><strong>Q.</strong> What is the Discrete Fourier Transform?</li>
<li><strong>Q.</strong> What is the Fast Fourier Transform?</li>
</ul>
<h4 id="the-master-theorem">The Master theorem</h4>
<ul>
<li><strong>Q.</strong> What do the terms in $T(n) = aT(n/b) + O(f(n))$ mean?
<ul>
<li>There are $a$ subproblems of size $n/b$. Splitting and combining costs $f(n)$.</li>
</ul>
</li>
<li><strong>Thm.</strong> (Master).</li>
<li><strong>Q.</strong> In the master thm., what is $n^c$?
<ul>
<li>The number of tasks at the bottom level.</li>
<li>If we&rsquo;re on the bottom level, the $T(n/b)$ term vanishes.</li>
<li>At the bottom level, $T(n/b)$ vanishes. We are left with $T(n) = \Theta(n^c)$.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the intuition behind the master thm.?
<ul>
<li>The bottleneck, splitting and combining or the bottom-level tasks, dominates.</li>
</ul>
</li>
</ul>
<h3 id="lecture-4">Lecture 4</h3>
<p>2024-01-29</p>
<h4 id="prelude-2">Prelude</h4>
<ul>
<li><strong>Q.</strong> What is the idea behind dynamic programming?
<ul>
<li>Reuse solutions to subproblems.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the difference between dynamic programming and divide and conquer?
<ul>
<li>Problems are dependent/independent.</li>
</ul>
</li>
<li><strong>Q.</strong> What is memoisation?
<ul>
<li>The process of storing solutions to subproblems in the cache.</li>
</ul>
</li>
</ul>
<h4 id="fibonacci">Fibonacci</h4>
<ul>
<li><strong>Prob.</strong> (Fibonacci). Find $f_n$.</li>
<li><strong>Ex.</strong> Solving Fibonacci</li>
<li><strong>Ex.</strong> Implementing Fibonacci
<ul>
<li><code>cache[n] = fib(n - 1) + fib(n - 2)</code>.</li>
</ul>
</li>
</ul>
<h4 id="weighted-interval-scheduling">Weighted interval scheduling</h4>
<ul>
<li><strong>Prob.</strong> (Weighted Interval Scheduling). Find the maximum total weight from nonoverlapping intervals.</li>
<li><strong>Ex.</strong> Attempt to solve Weighted Interval Scheduling</li>
<li><strong>Ex.</strong> Solving Weighted Interval Scheduling
<ul>
<li>Order intervals by endpoints. Then case decomposition, depending on whether $I_n \in \mathcal{O}$.</li>
</ul>
</li>
<li><strong>Ex.</strong> Implementing Weighted Interval Scheduling
<ul>
<li><code>cache[n] = max(WIS(n - 1), w_j + WIS(p(n)))</code>.</li>
<li>$p(n)$ is non-conflicting intervals.</li>
</ul>
</li>
</ul>
<h4 id="knapsack">Knapsack</h4>
<ul>
<li><strong>Prob.</strong> (Knapsack, optimisation). Maximise value subject to the weight constraint.</li>
<li><strong>Ex.</strong> Solving Knapsack
<ul>
<li>Case decomposition, depending on whether $(v_n, w_n) \in \mathcal{O}$.</li>
</ul>
</li>
<li><strong>Ex.</strong> Implementing Knapsack
<ul>
<li><code>cache[i, c] = max(knapsack(i - 1, c), knapsack(i - 1, c - w_i) + v_i)</code>.</li>
</ul>
</li>
</ul>
<h4 id="top-down-or-bottom-up">Top-down or bottom-up</h4>
<ul>
<li><strong>Q.</strong> What ways are there to implement a DP solution?
<ul>
<li>Top-down or bottom-up.</li>
</ul>
</li>
<li><strong>Q.</strong> What are some characteristics of top-down?</li>
<li><strong>Q.</strong> What are some characteristics of bottom-up?</li>
</ul>
<h4 id="heuristics-for-dynamic-programming">Heuristics for dynamic programming</h4>
<ul>
<li><strong>Q.</strong> When might dynamic programming be useful?
<ul>
<li>There are few subproblems.</li>
<li>Subproblems have an ordering from &ldquo;small&rdquo; to &ldquo;large&rdquo;.</li>
<li>They can be combined meaningfully.</li>
</ul>
</li>
<li><strong>Q.</strong> How do you draw a dynamic programming diagram?
<ul>
<li>Arrows indicate dependencies.</li>
</ul>
</li>
</ul>
<h3 id="lecture-5">Lecture 5</h3>
<p>2024-01-30</p>
<h4 id="heuristics-for-dynamic-programming-continued">Heuristics for dynamic programming, continued</h4>
<ul>
<li><strong>Q.</strong> How do we design a dynamic programming algorithm? <a href="https://www.youtube.com/watch?v=aPQY__2H3tE">cf.</a>
<ol>
<li>Decide which direction to work in.
<ul>
<li>Up/down, last/first, etc.</li>
<li>This assumes we have ordered the input.</li>
<li>Try visualising the problem, e.g. as a tree or a bipartite graph.</li>
</ul>
</li>
<li>Write down a recurrence relation.
<ul>
<li>State the recurrence in terms of array entries.</li>
<li>Think in terms of boolean, streak or counter matrices.</li>
</ul>
</li>
<li>Implementation.
<ul>
<li>Bottom-up/top-down.</li>
</ul>
</li>
</ol>
</li>
<li><strong>Ex.</strong> Prompts for dynamic programming problems
<ul>
<li>Can we find an optimal solution for the bottom/top or last/first objects?</li>
<li>What information should govern the algorithm?</li>
</ul>
</li>
</ul>
<h4 id="constructing-solutions">Constructing solutions</h4>
<ul>
<li><strong>Q.</strong> How do we construct a solution to a dynamic programming problem?
<ul>
<li>Create an auxiliary function.</li>
</ul>
</li>
<li><strong>Ex.</strong> Constructing a solution for Weighted Interval Scheduling</li>
</ul>
<h4 id="sequence-alignment">Sequence alignment</h4>
<ul>
<li><strong>Prob.</strong> (Sequence alignment). Find the minimum cost of aligning strings $x$ and $y$.</li>
<li><strong>Ex.</strong> Solving Sequence alignment
<ul>
<li>Reduce to a subproblem on the first objects.</li>
</ul>
</li>
</ul>
<h4 id="matrix-chain-multiplication">Matrix chain multiplication</h4>
<ul>
<li><strong>Prob.</strong> (Matrix Chain Multiplication). Find the minimum cost of computing $A_1 \ldots A_n$, assuming that multiplying a $d_1 \times d_2$ matrix with a $d_2 \times d_3$ matrix costs $d_1d_2d_3$.</li>
<li><strong>Ex.</strong> Solving Matrix Chain Multiplication
<ul>
<li>Backwards reasoning. Dynamic programming over intervals.</li>
</ul>
</li>
</ul>
<h4 id="implementing-bottom-up">Implementing bottom-up</h4>
<ul>
<li><strong>Q.</strong> How do we implement a bottom-up solution?
<ul>
<li>Create an $n \times m$-array.</li>
<li>Draw a diagram to ensure you get the computation order right.</li>
</ul>
</li>
</ul>
<h3 id="lecture-6">Lecture 6</h3>
<p>2024-02-05</p>
<h4 id="flow-networks">Flow networks</h4>
<ul>
<li>cf. Diestel, chapter 6.</li>
<li><strong>Notation.</strong> $f(X, Y)$</li>
<li><strong>Def.</strong> Network
<ul>
<li>$N = (G, s, t, c)$, where $G$ is a multigraph.</li>
</ul>
</li>
<li><strong>Q.</strong> Why do we want $G$ to be a multigraph?
<ul>
<li>There is no such thing as negative flow.</li>
<li>If we want to send flow in the opposite direction, we must add another edge.</li>
</ul>
</li>
<li><strong>Def.</strong> Flow $f : \vec{E} \to \mathbb{R}$
<ul>
<li>$f(e, x, y) = - f(e, y, x)$.</li>
<li>$f(v, V) = 0$ for $v \in V \setminus {s, t}$.</li>
<li>$f(\vec{e}) \le c(\vec{e})$.</li>
</ul>
</li>
<li><strong>Def.</strong> Cut $F$ in $G$
<ul>
<li>If there is a partition into sides ${V_1, V_2}$ of $V$ s.t. $F = E(V_1, V_2)$.</li>
</ul>
</li>
<li><strong>Def.</strong> Cut in $N$
<ul>
<li>A pair $(S, \bar{S})$ s.t. $s \in S$ and $t \in \bar{S}$.</li>
</ul>
</li>
<li><strong>Def.</strong> Capacity of a cut
<ul>
<li>$c(S, \bar{S})$.</li>
</ul>
</li>
<li><strong>Def.</strong> Value of $f$
<ul>
<li>$|f| = f(S, \bar{S})$.</li>
</ul>
</li>
</ul>
<h4 id="maximum-flows">Maximum flows</h4>
<ul>
<li><strong>Prob.</strong> (Maximum Flow). Find a flow of maximum value.</li>
<li><strong>Q.</strong> What is the idea behind the Ford-Fulkerson algorithm?
<ul>
<li>Allow a greedy algorithm to undo bad choices.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we think of the residual network?
<ul>
<li>It indicates how much we can undo.</li>
</ul>
</li>
<li><strong>Def.</strong> Forward edge
<ul>
<li>The edge $(e, x, y)$ with capacity $c(\vec{e}) - f(\vec{e})$.</li>
</ul>
</li>
<li><strong>Def.</strong> Backward edge
<ul>
<li>The edge $(e, y, x)$ with capacity $f(\vec{e})$.</li>
</ul>
</li>
<li><strong>Def.</strong> Residual capacities
<ul>
<li>The capacities $c(\vec{e}) - f(\vec{e})$ and $f(\vec{e})$.</li>
</ul>
</li>
<li><strong>Algo.</strong> (Ford-Fulkerson).
<ul>
<li>Find augmenting paths in the residual network.</li>
</ul>
</li>
</ul>
<h4 id="edmonds-karp">Edmonds-Karp</h4>
<ul>
<li><strong>Q.</strong> What is the idea behind the Edmonds-Karp algorithm?
<ul>
<li>Make a better choice of augmenting path.</li>
</ul>
</li>
<li><strong>Thm.</strong> (Edmonds-Karp). If we choose augmenting paths $P$ with the fewest number of edges, only $nm$ iterations are needed.</li>
<li><strong>Prop.</strong> Complexity of Ford-Fulkerson/Edmonds-Karp
<ul>
<li>$O(C(n + m))$ and $O(nm(n + m))$.</li>
</ul>
</li>
<li><strong>Q.</strong> What is strongly polynomial time?
<ul>
<li>If the complexity only depends on the input size. Not on the nature of the input.</li>
</ul>
</li>
</ul>
<h4 id="minimum-cuts">Minimum cuts</h4>
<ul>
<li><strong>Q.</strong> What is another way to think of $s-t$ cuts?
<ul>
<li>A set of edges separating $s$ and $t$.</li>
</ul>
</li>
<li><strong>Def.</strong> Directed cut
<ul>
<li>A set of edges in a directed graph separating $s$ and $t$.</li>
</ul>
</li>
<li><strong>Def.</strong> Size/weight of a cut
<ul>
<li>Number of edges/total weight of edges.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Minimum Cut). Find an edge cut in the network $N$ of minimum capacity.</li>
</ul>
<h4 id="the-max-flow-min-cut-theorem">The Max Flow Min Cut theorem</h4>
<ul>
<li><strong>Thm.</strong> (Max Flow Min Cut). The upper bound $f(S, \bar{S}) \le c(S, \bar{S})$ is attained.</li>
<li><em>Proof.</em> (Max Flow Min Cut). Algorithmic.</li>
</ul>
<h3 id="lecture-7">Lecture 7</h3>
<p>2024-02-12</p>
<h4 id="prelude-3">Prelude</h4>
<ul>
<li><strong>Q.</strong> Why does the Max Flow Min Cut thm. matter?
<ul>
<li>Tells us there is a kind of duality between paths and separators.</li>
<li>Maximum Flow and Minimum Cut are the same problem.</li>
<li>Can be used to give easier proofs of Menger and KÃ¶nig. <a href="https://math.stackexchange.com/questions/4319279/questions-on-proof-of-konigs-theorem">cf.</a></li>
</ul>
</li>
</ul>
<h4 id="vertex-cut">Vertex Cut</h4>
<ul>
<li><strong>Q.</strong> How do we model vertex capacities?
<ul>
<li>Add $v_{\text{in}}$ and $v_{\text{out}}$.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Vertex Cut). Find a minimum vertex separator.</li>
<li><strong>Ex.</strong> Solving Vertex Cut
<ul>
<li>Vertex capacities of $1$ and edge capacities of $\infty$.</li>
</ul>
</li>
</ul>
<h4 id="many-sources-or-sinks">Many sources or sinks</h4>
<ul>
<li><strong>Ex.</strong> Maximum Flow if there are multiple sources/sinks
<ul>
<li>Add a super source/super sink.</li>
</ul>
</li>
</ul>
<h4 id="matching-numbers">Matching numbers</h4>
<ul>
<li><strong>Def.</strong> Matching $M$
<ul>
<li>A set of independent edges.</li>
</ul>
</li>
<li><strong>Def.</strong> Matching number $\nu(G)$
<ul>
<li>The size of a maximum matching.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Matching Number). Find $\nu(G)$.</li>
<li><strong>Ex.</strong> Solving Matching Number
<ul>
<li>Add a super sink and a super source. Set all edge capacities to be $1$.</li>
</ul>
</li>
<li>Compare with KÃ¶nig.</li>
</ul>
<h4 id="disjoint-paths">Disjoint paths</h4>
<ul>
<li><strong>Prob.</strong> (Edge-disjoint Paths). Find the maximum number of edge-disjoint $s$-$t$-paths.</li>
<li><strong>Ex.</strong> Solving Edge-disjoint Paths
<ul>
<li>Set all edge capacities to $1$.</li>
</ul>
</li>
<li><strong>Prob.</strong> (Disjoint Paths). Find the maximum number of disjoint $s$-$t$-paths.</li>
<li><strong>Ex.</strong> Solving Disjoint Paths
<ul>
<li>Set all vertex capacities to $1$.</li>
</ul>
</li>
<li>Compare with Menger.</li>
</ul>
<h4 id="closure">Closure</h4>
<ul>
<li><strong>Prob.</strong> (Closure). A set of projects with values and dependencies. Want to find maximal total value.</li>
</ul>
<h3 id="lecture-8">Lecture 8</h3>
<p>2024-03-18</p>
<h4 id="randomised-algorithms">Randomised algorithms</h4>
<ul>
<li><strong>Q.</strong> What is a randomised algorithm?
<ul>
<li>Uses randomness to make some choices.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a Las Vegas algorithm?
<ul>
<li>Correct answer, random runtime.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a Monte Carlo algorithm?
<ul>
<li>Random correctness, (usually) guarantee on runtime.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the relation between Las Vegas and Monte Carlo algorithms?
<ul>
<li>A Las Vegas algorithm can be turned into a Monte Carlo algorithm.</li>
</ul>
</li>
</ul>
<h4 id="random-number-generators">Random number generators</h4>
<ul>
<li><strong>Q.</strong> What kinds of RNGs are there?
<ul>
<li>True RNGs and pseudo-RNGs.</li>
</ul>
</li>
</ul>
<h4 id="approximation-algorithms">Approximation algorithms</h4>
<ul>
<li><strong>Q.</strong> What is an approximation algorithm?
<ul>
<li>Finds a solution that is guaranteed to be close to optimal.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the relation between approximation algorithms and randomised algorithms?</li>
</ul>
<h4 id="global-minimum-cut">Global Minimum Cut</h4>
<ul>
<li><strong>Prob.</strong> (Global Minimum Cut). Partition the vertices of $G$ into sides s.t. that the size of the cut is minimised.</li>
<li><strong>Ex.</strong> Solving Global Minimum Cut with Ford-Fulkerson
<ul>
<li>View $G$ as a network. Run Ford-Fulkerson for fixed $s$ and let $t$ vary.</li>
</ul>
</li>
<li><strong>Ex.</strong> Approximating Global Minimum Cut
<ul>
<li>Randomly contract edges.</li>
</ul>
</li>
<li><strong>Ex.</strong> Analysing Global Minimum Cut
<ul>
<li>Compute the probability that the $i$:th choice is bad, use $1 - x \le e^{-x}$.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we improve the expected success rate of a randomised algorithm?
<ul>
<li>Rerun the randomised algorithm, take best result.</li>
</ul>
</li>
</ul>
<h4 id="global-maximum-cut">Global Maximum Cut</h4>
<ul>
<li><strong>Prob.</strong> (Global Maximum Cut). Partition the vertices of $G$ into sides s.t. that the size of the cut is maximised.</li>
<li><strong>Ex.</strong> Approximating Global Maximum Cut
<ul>
<li>Place edges in either side with probability $1/2$.</li>
</ul>
</li>
<li><strong>Ex.</strong> Analysing Global Maximum Cut
<ul>
<li>Write $\mathsf{Alg} = \sum_{i} \chi_i$ and use linearity of expectation.</li>
</ul>
</li>
</ul>
<h3 id="lecture-9">Lecture 9</h3>
<p>2024-03-19</p>
<h4 id="prelude-4">Prelude</h4>
<ul>
<li><strong>Q.</strong> What is an algorithm?</li>
</ul>
<h4 id="turing-machines">Turing machines</h4>
<ul>
<li><strong>Q.</strong> What is a Turing machine?
<ul>
<li>A machine that manipulates symbols on a strip of tape according to some rules.</li>
<li><a href="http://turingmachine.io">demo</a>.</li>
</ul>
</li>
<li><strong>Q.</strong> What goes into a Turing machine?
<ul>
<li>An alphabet $\Gamma$, a set of states $Q$, a transition function $\delta$.</li>
</ul>
</li>
<li><strong>Def.</strong> Turing machine $M$</li>
<li><strong>Q.</strong> What is the Church-Turing hypothesis?
<ul>
<li>Anything that can be computed can be computed by a Turing machine.</li>
</ul>
</li>
<li><strong>Q.</strong> Why do we need it?
<ul>
<li>So we can analyse &ldquo;normal&rdquo; algorithms in terms of Turing machines.</li>
</ul>
</li>
</ul>
<h4 id="ram">RAM</h4>
<ul>
<li><strong>Q.</strong> What is the RAM model of computation?
<ul>
<li>Models the way modern computers work with registers and memory cells.</li>
</ul>
</li>
<li><strong>Rmk.</strong> Pseudocode implicitly assumes RAM.</li>
<li><strong>Q.</strong> What kinds of RAM are there?
<ul>
<li>Basic RAM, word RAM, parallel RAM, etc.</li>
</ul>
</li>
<li><strong>Q.</strong> What is word RAM?
<ul>
<li>Registers and memory cells have limited word sizes.</li>
</ul>
</li>
<li><strong>Q.</strong> In the RAM model, how can we think of unit and bit cost?
<ul>
<li>Unit cost is basic RAM, bit cost is word RAM.</li>
</ul>
</li>
</ul>
<h4 id="models-of-computation">Models of computation</h4>
<ul>
<li><strong>Ex.</strong> Our favourite models of computation
<ul>
<li>Turing machines, RAM, etc.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a Turing-complete model of computation?
<ul>
<li>One which can do anything a Turing machine can do.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a computable function?
<ul>
<li>A function whose value can be computed by an effective procedure.</li>
</ul>
</li>
</ul>
<h4 id="restricted-models-of-computation">Restricted models of computation</h4>
<ul>
<li><strong>Q.</strong> Why do we care about restricted models of computation?
<ul>
<li>Hard to reason about all algorithms out there.</li>
</ul>
</li>
<li><strong>Ex.</strong> Our favourite restricted model of computation
<ul>
<li>Limiting what the algorithm can do.</li>
</ul>
</li>
</ul>
<h4 id="comparison-based-sorting">Comparison-based sorting</h4>
<ul>
<li><strong>Ex.</strong> Comparison-based sorting must take $\Omega(n \log n)$ time
<ul>
<li>Need to distinguish between $n!$ permutations.</li>
<li>$k$ comparisons allows us to differentiate between $2^k$ permutations.</li>
</ul>
</li>
<li><strong>Ex.</strong> Lower bound of $\log n!$
<ul>
<li>Estimate with tail terms.</li>
<li>Stirling&rsquo;s formula.</li>
</ul>
</li>
<li><strong>Q.</strong> Is this a natural restriction?
<ul>
<li>It depends on the nature of the input.</li>
</ul>
</li>
<li><strong>Ex.</strong> Sorting in $O(n)$
<ul>
<li>Counting sort.</li>
</ul>
</li>
</ul>
<h3 id="lecture-10">Lecture 10</h3>
<p>2024-03-25</p>
<h4 id="prelude-5">Prelude</h4>
<ul>
<li><strong>Q.</strong> What are the main types of problems?
<ul>
<li>Search, optimisation, decision.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a decidable problem?
<ul>
<li>There exists some algorithm for solving it.</li>
</ul>
</li>
<li><strong>Ex.</strong> Trading is undecidable</li>
</ul>
<h4 id="heuristics-for-transforming-problems">Heuristics for transforming problems</h4>
<ul>
<li><strong>Q.</strong> How can we reduce a search problem to a decision problem?
<ul>
<li>Produce an instance where all choices are determined.</li>
</ul>
</li>
<li><strong>Ex.</strong> Ways to make progress in graph problems
<ul>
<li>Removing or adding edges, contracting vertices, etc.</li>
</ul>
</li>
<li><strong>Ex.</strong> Finding a $k$-colouring
<ul>
<li>All choices are determined in a complete $k$-partite graph.</li>
<li>This suggests adding edges.</li>
</ul>
</li>
<li><strong>Ex.</strong> Finding a Hamilton cycle
<ul>
<li>All choices are determined if there is just one Hamilton cycle.</li>
<li>This suggests removing edges.</li>
</ul>
</li>
</ul>
<h4 id="halting">Halting</h4>
<ul>
<li><strong>Prob.</strong> (Halting). If $M(x)$ halts.</li>
<li><strong>Prop.</strong> Halting is undecidable.</li>
<li><em>Proof.</em> (Halting).
<ul>
<li><code>A(M) = if halts(M, M) enter infinite loop</code>.</li>
</ul>
</li>
<li><strong>Prop.</strong> (Halting on Empty Input).</li>
<li><strong>Prob.</strong> (Halting on All Inputs).</li>
<li><strong>Cor.</strong> Halting on Empty Input and Halting on All Inputs are undecidable.</li>
<li><em>Proof.</em> Let $M&rsquo;$ simulate $M$ on input $x$.</li>
</ul>
<h4 id="undecidable-problems">Undecidable problems</h4>
<ul>
<li><strong>Ex.</strong> Undecidable problems
<ul>
<li>Halting, Diophantine Equations, Post Correspondence Problem.</li>
</ul>
</li>
<li><strong>Q.</strong> What is a recursively enumerable problem?
<ul>
<li>Eventually terminates on yes-instances.</li>
</ul>
</li>
</ul>
<h4 id="turing-reductions">Turing reductions</h4>
<ul>
<li><strong>Q.</strong> What is a Turing/Cook reduction?
<ul>
<li>An algorithm for problem $X$ assuming an algorithm for problem $Y$.</li>
</ul>
</li>
<li><strong>Q.</strong> How can one understand $\le_T$?
<ul>
<li>Larger means harder.</li>
</ul>
</li>
<li><strong>Q.</strong> What are positive and negative reductions?
<ul>
<li>Positive reductions allow us to solve things.</li>
<li>Negative reductions prove that things are unsolvable.</li>
</ul>
</li>
<li><strong>Ex.</strong> Our favourite reduction
<ul>
<li>Minimum Cut to Maximum Flow.</li>
</ul>
</li>
</ul>
<h4 id="examples-of-reductions">Examples of reductions</h4>
<ul>
<li><strong>Ex.</strong> Independent Set $\le_P$ Vertex Cover</li>
<li><strong>Ex.</strong> Vertex Cover $\le_P$ Set Cover</li>
</ul>
<h4 id="3-sat">3-SAT</h4>
<ul>
<li><strong>Prob.</strong> ($3$-SAT). If $\phi$ is satisfiable.</li>
<li><strong>Q.</strong> What are the main ways to think of $3$-SAT?
<ul>
<li>Find a true literal for each clause without picking $x_i$ and $\neg x_i$.</li>
<li>Find an assignment of variables s.t. each clause is true.</li>
</ul>
</li>
<li><strong>Ex.</strong> $3$-SAT $\le_P$ Independent Set
<ul>
<li>Each clause is a $K_3$. Put edges between $x_i$ and $\neg x_i$.</li>
</ul>
</li>
</ul>
<h4 id="karp-reductions">Karp reductions</h4>
<ul>
<li><strong>Q.</strong> What is a Karp reduction?
<ul>
<li>Given an instance $A$ of problem $X$, create an instance $B = f(A)$ of problem $Y$.</li>
</ul>
</li>
</ul>
<h3 id="lecture-11">Lecture 11</h3>
<p>2024-03-27</p>
<h4 id="prelude-6">Prelude</h4>
<ul>
<li>Recall that $\le_T$ indicates hardness, where bigger is harder.</li>
</ul>
<h4 id="problems-as-languages">Problems as languages</h4>
<ul>
<li><strong>Q.</strong> What is a formal language over ${0, 1}$?
<ul>
<li>A set of binary strings with $0$&rsquo;s and $1$&rsquo;s.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the relation between a formal language and a decision problem?
<ul>
<li>$L_p$ consists of the &ldquo;yes&rdquo;-instances.</li>
</ul>
</li>
</ul>
<h4 id="p">P</h4>
<ul>
<li><strong>Def.</strong> The complexity class $\mathsf{P}$
<ul>
<li>A language $L$ is in $\mathsf{P}$ iff there is an $O(n^k)$ algorithm deciding $L$.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the extended Church-Turing thesis?
<ul>
<li>Every effective computation can be carried out effectively by a Turing machine.</li>
</ul>
</li>
<li><strong>Q.</strong> Why do we need the Extended Church-Turing thesis?
<ul>
<li>For the definition of $\mathsf{P}$.</li>
<li>Not only do we care about the existence of an algorithm (computability), but also about computational efficiency.</li>
</ul>
</li>
<li><strong>Rmk.</strong> It is probably false!</li>
</ul>
<h4 id="np">NP</h4>
<ul>
<li><strong>Q.</strong> What is a witness/certificate?
<ul>
<li>&ldquo;Proof by existence&rdquo;.</li>
</ul>
</li>
<li><strong>Q.</strong> What is $\mathsf{NP}$?
<ul>
<li>Problems with efficient verification.</li>
</ul>
</li>
<li><strong>Def.</strong> The complexity class $\mathsf{NP}$
<ul>
<li>A language $L$ is in $\mathsf{NP}$ iff there&rsquo;s a polynomial $p$ and and a polynomial-time algorithm $A : {0, 1}^* \times {0, 1}^* \to {0, 1}$ such that:</li>
<li>If $x \in L$, there&rsquo;s a $y$ satisfying $|y| \le p(|x|)$ s.t. $A(x, y) = 1$.</li>
<li>If $x \notin L$, then $A(x, y) = 0$ for all $y$.</li>
</ul>
</li>
<li><strong>Q.</strong> How should we understand the definition of $\mathsf{NP}$?
<ul>
<li>$y$ is the certificate. There are no fake certificates.</li>
</ul>
</li>
</ul>
<h4 id="p-vs-np">P vs. NP</h4>
<ul>
<li><strong>Q.</strong> Why is $\mathsf{P} \subset \mathsf{NP}$?
<ul>
<li>Run the algorithm. If &ldquo;yes&rdquo;, let any $y$ be witness. If &ldquo;no&rdquo;, there are no witnesses.</li>
</ul>
</li>
<li><strong>Q.</strong> What is another way to put &ldquo;Is $\mathsf{NP} \subset \mathsf{P}$&rdquo;?
<ul>
<li>If we can check it, can we solve it?</li>
</ul>
</li>
</ul>
<h4 id="np-completeness">NP-completeness</h4>
<ul>
<li><strong>Q.</strong> How can we think of $\mathsf{NP}$-hard problems?
<ul>
<li>They&rsquo;re at least as hard as every problem in $\mathsf{NP}$.</li>
</ul>
</li>
<li><strong>Def.</strong> $\mathsf{NP}$-hard problem $Y$
<ul>
<li>If $X \le_p Y$ for all $X \in NP$.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we think of $\mathsf{NP}$-complete problems?
<ul>
<li>The hardest kinds of problem in $\mathsf{NP}$.</li>
</ul>
</li>
<li><strong>Def.</strong> $\mathsf{NP}$-complete problem $Y$
<ul>
<li>If $Y \in NP$ and $Y$ is $\mathsf{NP}$-hard.</li>
</ul>
</li>
<li><strong>Q.</strong> How do you show $Y$ is $\mathsf{NP}$-hard?
<ul>
<li>Reduce an $\mathsf{NP}$-hard problem to $Y$.</li>
</ul>
</li>
</ul>
<h4 id="np-complete-problems">NP-complete problems</h4>
<ul>
<li><strong>Thm.</strong> (Cook-Levin). <em>SAT</em> is $\mathsf{NP}$-hard.</li>
<li><strong>Ex.</strong> Reduction from SAT to 3-SAT
<ul>
<li>Add a &ldquo;chain variable&rdquo; in each clause, e.g. $(x_1 \vee x_2 \vee \neg y_1) \wedge (y_1 \vee x_3 \vee \neg y_2)$.</li>
</ul>
</li>
<li><strong>Q.</strong> How do you show a reduction is correct?
<ul>
<li>It&rsquo;s polynomial.</li>
<li>&ldquo;yes&rdquo; $\mapsto$ &ldquo;yes&rdquo;.</li>
<li>&ldquo;no&rdquo; $\mapsto$ &ldquo;no&rdquo;.</li>
</ul>
</li>
</ul>
<h3 id="lecture-12">Lecture 12</h3>
<p>2024-04-08</p>
<h4 id="prelude-7">Prelude</h4>
<ul>
<li><strong>Q.</strong> What is the difference between Karp reductions and Turing reductions?
<ul>
<li>Turing allows you to call the black box more than once.</li>
</ul>
</li>
<li><strong>Q.</strong> Why do we care more about Karp reductions?
<ul>
<li>Because $\mathsf{NP}$-hardness is defined in terms of Karp reductions.</li>
</ul>
</li>
<li>We will discuss more examples of $\mathsf{NP}$-complete problems.</li>
</ul>
<h4 id="hamiltonian-cycle">Hamiltonian Cycle</h4>
<ul>
<li><strong>Prob.</strong> (Hamiltonian Cycle). Determine whether $G$ has a Hamiltonian cycle.</li>
<li><strong>Thm.</strong> Hamiltonian Cycle is $\mathsf{NP}$-hard.</li>
<li><em>Proof.</em> (Hamiltonian Cycle is $\mathsf{NP}$-hard).
<ul>
<li>Setting $x$ to &ldquo;True&rdquo; is like traversing the $x$ row rightwards.</li>
<li>Add a detour for each clause.</li>
<li>Show it&rsquo;s a valid reduction.</li>
</ul>
</li>
</ul>
<h4 id="travelling-salesman">Travelling salesman</h4>
<ul>
<li><strong>Prob.</strong> (Travelling Salesman). Determine whether there a travelling salesman tour of length at most $k$.</li>
<li><strong>Thm.</strong> Travelling Salesman is $\mathsf{NP}$-hard.</li>
<li><em>Proof.</em> (Travelling Salesman is $\mathsf{NP}$-hard).
<ul>
<li>Reduction from Hamiltonian Cycle, with $$d(u, v) = \begin{cases} 1, &amp; (u, v) \in E, \ \infty.\end{cases}$$</li>
</ul>
</li>
</ul>
<h4 id="graph-colourings">Graph colourings</h4>
<ul>
<li><strong>Prob.</strong> ($k$-colouring). Determine whether $G$ is $k$-colourable.</li>
<li><strong>Thm.</strong> $3$-colouring is $\mathsf{NP}$-hard.</li>
<li><strong>Rmk.</strong> $2$-colouring is easy &ndash; run DFS.</li>
<li><strong>Thm.</strong> $k$-colouring is $\mathsf{NP}$-hard.</li>
</ul>
<h4 id="subset-sum">Subset sum</h4>
<ul>
<li><strong>Prob.</strong> (Subset sum). Determine if there is a subset $S \subset [n]$ s.t. $\sum_{i \in S} x_i = t$.</li>
<li><strong>Thm.</strong> Subset sum is $\mathsf{NP}$-hard.</li>
</ul>
<h4 id="knapsack-1">Knapsack</h4>
<ul>
<li><strong>Prob.</strong> (Knapsack). Determine if there is a subset $S \subset [n]$ s.t. $\sum_{i \in S} w_i \le C$ and $\sum_{i \in S} v_i \ge k$.</li>
<li><strong>Thm.</strong> Knapsack is $\mathsf{NP}$-hard.</li>
<li><em>Proof.</em> (Knapsack is $\mathsf{NP}$-hard).
<ul>
<li>Reduction from Subset sum to Knapsack.</li>
<li>Take $x_i = v_i = w_i$ and $C = k = t$.</li>
</ul>
</li>
<li><strong>Rmk.</strong> We used DP to find a $O(nC)$ algorithm.</li>
<li><strong>Q.</strong> Why is $O(nC)$ not polynomial?
<ul>
<li>If each of the $2n + 1$ variables has $\log C$ bits, the input size is $O(n \log C)$.</li>
<li>Set $m = n \log C$. The runtime $O(f(m))$ is not polynomial.</li>
</ul>
</li>
</ul>
<h4 id="summary-np-completeness">Summary NP-completeness</h4>
<ul>
<li><strong>Q.</strong> How can we visualise Cook-Levin in a Venn diagram?
<ul>
<li>Draw arrows from all problems in $\mathsf{NP}$ to Sat.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we visualise the $\mathsf{NP}$-complete problems?
<ul>
<li>A complete graph.</li>
</ul>
</li>
</ul>
<h4 id="understanding-the-definition-of-np">Understanding the definition of NP</h4>
<ul>
<li><strong>Q.</strong> How can we understand Turing machines?
<ul>
<li>As an algorithm, or a computer program in our favourite programming language.</li>
</ul>
</li>
<li><strong>Q.</strong> How can we understand the definition of $\mathsf{NP}$, without $A$?
<ul>
<li>$X$ is in $\mathsf{NP}$ iff there&rsquo;s an algorithm $M$ in time $p(|x|)$ s.t.
<ul>
<li>If $x \in X$, there&rsquo;s a short $y$ s.t. $M(x, y)$ accepts.</li>
<li>If $x \notin X$, then $M(x, y)$ rejects all $y$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="proof-sketch-of-cook-levin">Proof sketch of Cook-Levin</h4>
<ul>
<li><em>Proof sketch.</em> (Cook-Levin).
<ul>
<li>Construct a SAT formula $\phi$ satisfiable iff there&rsquo;s a witness $y$ s.t. $M(x, y)$ accepts in time $p(|x|)$.</li>
<li>Add Boolean variables that specify the state of the machine.</li>
<li>Add constraints to ensure the states are valid.</li>
</ul>
</li>
</ul>
<h3 id="lecture-13">Lecture 13</h3>
<p>2024-05-07</p>
<h4 id="conp">CoNP</h4>
<ul>
<li><strong>Q.</strong> What is the first way to think of $\mathsf{CoNP}$?
<ul>
<li>&ldquo;No&rdquo;-instances can easily be verified.</li>
</ul>
</li>
<li><strong>Q.</strong> What is the second way to think of $\mathsf{CoNP}$?
<ul>
<li>$X^c \in \mathsf{NP}$.</li>
</ul>
</li>
<li><strong>Def.</strong> Co-NP</li>
<li><strong>Ex.</strong> Problem in $\mathsf{CoNP}$
<ul>
<li>Tautology, primality testing.</li>
</ul>
</li>
</ul>
<h4 id="conp-hardness">CoNP-hardness</h4>
<ul>
<li><strong>Def.</strong> $\mathsf{CoNP}$-hard, $\mathsf{CoNP}$-complete</li>
<li><strong>Q.</strong> If $X \le Y$, what is the relation between $X^c$ and $Y^c$?</li>
<li><strong>Q.</strong> What is the relation between $\mathsf{NP}$-hardness and $\mathsf{CoNP}$-hardness?
<ul>
<li>$X$ is $\mathsf{CoNP}$-hard iff $X^C$ is $\mathsf{NP}$-hard.</li>
</ul>
</li>
<li><strong>Ex.</strong> $\mathsf{CoNP}$-complete problem
<ul>
<li>No Hamiltonian Cycle</li>
</ul>
</li>
</ul>
<h4 id="p-np--conp">P, NP &amp; CoNP</h4>
<ul>
<li><strong>Q.</strong> Is $\mathsf{NP} = \mathsf{CoNP}$?
<ul>
<li>Maybe not.</li>
</ul>
</li>
<li><strong>Q.</strong> Is $\mathsf{P} = \mathsf{NP} \cap \mathsf{CoNP}$?
<ul>
<li>Maybe not.</li>
</ul>
</li>
</ul>
<h4 id="pspace">PSPACE</h4>
<ul>
<li><strong>Def.</strong> $\mathsf{PSPACE}$</li>
<li><strong>Q.</strong> How does $\mathsf{PSPACE}$ relate to $\mathsf{P}$ and $\mathsf{NP}$?
<ul>
<li>It contains them.</li>
</ul>
</li>
<li><strong>Ex.</strong> Problem in $\mathsf{PSPACE}$ but not $\mathsf{NP}$
<ul>
<li>Generalised chess.</li>
</ul>
</li>
<li><strong>Q.</strong> Why is generalised chess in $\mathsf{PSPACE} - \mathsf{P}$?</li>
</ul>
<h4 id="pspace-hardness">PSPACE-hardness</h4>
<ul>
<li><strong>Def.</strong> $\mathsf{PSPACE}$-hard, $\mathsf{PSPACE}$-complete</li>
<li><strong>Ex.</strong> $\mathsf{PSPACE}$-complete problem
<ul>
<li>QBF.</li>
</ul>
</li>
</ul>
<h4 id="venn-diagram">Venn diagram</h4>
<ul>
<li><strong>Q.</strong> How do we think the complexity classes relate?</li>
</ul>
<h4 id="bpp--zpp">BPP &amp; ZPP</h4>
<ul>
<li><strong>Def.</strong> $\mathsf{BPP}$</li>
<li><strong>Q.</strong> Why $2/3$?
<ul>
<li>Immaterial, just as long as it&rsquo;s better than $1/2$.</li>
</ul>
</li>
<li><strong>Def.</strong> $\mathsf{ZPP}$</li>
<li><strong>Q.</strong> What is the relation between $\mathsf{P}$, $\mathsf{ZPP}$ and $\mathsf{BPP}$?</li>
</ul>
</article>

        </main><footer id="footer">
    Copyright Â© 2025 Isabel Dahlgren
</footer>
</body>
</html>
