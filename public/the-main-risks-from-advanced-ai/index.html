<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/the-main-risks-from-advanced-ai/" />
    <title>The main risks from advanced AI</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>The main risks from advanced AI</h1>
        <div>
                <time>March 16, 2025</time>
            </div>
    </header><p>As with any new technology, advanced AI entails certain risks. While we&rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.</p>
<p>The other week, I had an insightful discussion with people in <a href="https://www.zurich-ai-alignment.com">Zürich AI Alignment (ZAIA)</a> about the risks from AI. Afterwards, I began jotting down my thoughts, and they somehow ballooned into this blog post. Here are what I currently consider to be the most prominent risks:</p>
<ul>
<li><strong>Powerful technology in bad hands</strong>: First, there are the risks which all fall under the label &ldquo;bad guys using powerful technology to further their own interests&rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. Presumably, most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.</li>
<li><strong>Concentration of power</strong>: AI is a transformative technology - a message that shouldn&rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&rsquo;s likely to have a profound impact on our economy<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and healthcare systems<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. As a result, the main players, OpenAI, DeepMind and Anthropic, are going to have an enormous influence on our lives. Unless AI policy guidelines are put in place very soon, the engineers at these companies are going to have an outsized impact on society. And this is somewhat problematic. While these engineers tend to be lovely people, they aren&rsquo;t democratically elected. Moreover, they probably aren&rsquo;t in the best position to decide what&rsquo;s best for society at large.</li>
<li><strong>Misaligned AI models</strong>: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&rsquo;ve already seen plenty of examples of this. For example, pointed out during the ZAIA discussion, it&rsquo;s still quite easy jailbreaking LLMs like ChatGPT<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Moreover, there are plenty of instances of algorithms encoding our own biases, having been fed biased data during training<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</li>
<li><strong>Power-seeking AI</strong>: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&rsquo;ll briefly outline what I understand to be the core argument.
<ol>
<li>In the future, we might see the emergence of power-seeking AI models. AI models are trained to minimise some loss function, and power could be instrumental in achieving this.</li>
<li>At the same time, we might develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. Such AI agents could replace a greater portion of the workforce, and would therefore be immensely profitable. The economic incentives are there, so it&rsquo;s quite plausible that we&rsquo;ll see more agentic AI models in the future.</li>
<li>All in all, it&rsquo;s possible we&rsquo;ll develop power-seeking, agentic AI models. The key point now is that such an AI might view humans as obstacles to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly:  &ldquo;You can&rsquo;t fetch the coffee if you&rsquo;re dead&rdquo;. So, the argument goes, the AI might try to disempower humanity.</li>
</ol>
</li>
<li>While points (1) and (2) seem quite reasonable, I&rsquo;m more sceptical about point (3). A more conservative version of point (3) would be that power-seeking agentic AI models can do a lot of harm, despite not being mistrustful of all humans. I think the key word here is &ldquo;agency&rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &ldquo;agentic&rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage. In light of this, I would lower-bound the risk of existential threat from AI by 1%. I think the stakes are too high for us to dismiss this kind of threat.</li>
</ul>
<p>Of course, there&rsquo;s much more to be said about each topic. Each bullet could easily be broken into a bunch of sub-bullets. But broadly speaking, I think most risks fall into either one of the above categories. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate, or at least mitigate, some of these risks. See e.g. <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary">this</a> article for a more exhaustive survey of the risks from AI, along with suggested measures. I&rsquo;ve also left links for further resources in the footnotes.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://www.pewresearch.org/short-reads/2024/03/26/americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information">Americans increasingly using ChatGPT, but few trust its 2024 election information | Pew Research Center</a>, <a href="https://www.pewresearch.org/short-reads/2025/01/15/about-a-quarter-of-us-teens-have-used-chatgpt-for-schoolwork-double-the-share-in-2023">Share of teens using ChatGPT for schoolwork doubled from 2023 to 2024 | Pew Research Center</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://www.weforum.org/stories/2025/03/ai-transforming-global-health/"># 6 ways AI is transforming healthcare</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://www.youtube.com/watch?list=PLj62-wQeg_DhmYphxg70DhPEJcjfmnVEt&amp;v=UG_X_7g63rY&amp;embeds_referring_euri=https%3A%2F%2Fwww.media.mit.edu%2F&amp;source_ve_path=MjM4NTE">How I'm fighting bias in algorithms | Joy Buolamwini - YouTube</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples">AI Bias Examples | IBM</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai">Agentic AI vs. Generative AI | IBM</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright © 2025 Isabel Dahlgren
</footer>
</body>
</html>
