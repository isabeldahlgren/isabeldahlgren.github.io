<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=55424&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - http://localhost:55424/">
  <title>Understanding the AI alignment problem | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Understanding the AI alignment problem">
  <meta name="twitter:description" content="Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic here:
“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”
The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it’s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.">

<meta property="og:url" content="http://localhost:55424/understanding-the-ai-alignment-problem/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="Understanding the AI alignment problem">
  <meta property="og:description" content="Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic here:
“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”
The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it’s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-25T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">
    <meta property="article:tag" content="Ai">


  <meta itemprop="name" content="Understanding the AI alignment problem">
  <meta itemprop="description" content="Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic here:
“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”
The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it’s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.">
  <meta itemprop="datePublished" content="2025-05-25T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-25T00:00:00+00:00">
  <meta itemprop="wordCount" content="792">
  <meta itemprop="keywords" content="Ai-Alignment,Ai">
  <link rel="canonical" href="http://localhost:55424/understanding-the-ai-alignment-problem/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="http://localhost:55424/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="http://localhost:55424/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "Understanding the AI alignment problem",
    "headline": "Understanding the AI alignment problem",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eBroadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic \u003ca href=\u0022https:\/\/www.anthropic.com\/news\/core-views-on-ai-safety#:~:text=build%20safe%2C%20reliable%2C%20and%20steerable%20systems%20when%20those%20systems%20are%20starting%20to%20become%20as%20intelligent%20and%20as%20aware%20of%20their%20surroundings%20as%20their%20designers\u0022\u003ehere\u003c\/a\u003e:\u003c\/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”\u003c\/p\u003e\u003c\/blockquote\u003e\n\u003cp\u003eThe general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it\u0026rsquo;s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http:\/\/localhost:55424\/understanding-the-ai-alignment-problem\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-05-25T00:00:00.00Z",
    "datePublished": "2025-05-25T00:00:00.00Z",
    "dateModified": "2025-05-25T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "http://localhost:55424/",
        "logo": {
            "@type": "ImageObject",
            "url": "http:\/\/localhost:55424\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "http://localhost:55424/",
    "url" : "http:\/\/localhost:55424\/understanding-the-ai-alignment-problem\/",
    "wordCount" : "792",
    "genre" : [ "ai-alignment" , "ai" ],
    "keywords" : [ "ai-alignment" , "ai" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">Understanding the AI alignment problem</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-05-25"> May 25, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>, <a href='/tags/ai'>ai</a>
        </p>
      

      <span itemprop="articleBody">
      <p>Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic <a href="https://www.anthropic.com/news/core-views-on-ai-safety#:~:text=build%20safe%2C%20reliable%2C%20and%20steerable%20systems%20when%20those%20systems%20are%20starting%20to%20become%20as%20intelligent%20and%20as%20aware%20of%20their%20surroundings%20as%20their%20designers">here</a>:</p>
<blockquote>
<p>“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”</p></blockquote>
<p>The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it&rsquo;s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.</p>
<h3 id="the-eight-year-old-ceo">The eight-year-old CEO <a href="#the-eight-year-old-ceo" class="hash">#</a></h3>
<p>In this excellent <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">blog post</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, Ajeya Cotra asks you to imagine the following scenario:</p>
<blockquote>
<p>Imagine you are an eight-year-old whose parents left you a $1 trillion company and no trusted adult to serve as your guide to the world. You must hire a smart adult to run your company as CEO, handle your life the way that a parent would (e.g. decide your school, where you’ll live, when you need to go to the dentist), and administer your vast wealth (e.g. decide where you’ll invest your money). You have to hire these grownups based on a work trial or interview you come up with &ndash; you don&rsquo;t get to see any resumes, don&rsquo;t get to do reference checks, etc. Because you&rsquo;re so rich, tons of people apply for all sorts of reasons.</p>
<p>Your candidate pool includes:</p>
<p><strong>Saints</strong> &ndash; people who genuinely just want to help you manage your estate well and look out for your long-term interests.</p>
<p><strong>Sycophants</strong> &ndash; people who just want to do whatever it takes to make you short-term happy or satisfy the letter of your instructions regardless of long-term consequences.</p>
<p><strong>Schemers</strong> &ndash; people with their own agendas who want to get access to your company and all its wealth and power so they can use it however they want.</p></blockquote>
<p>Deciding whom to hire is extremely difficult - you&rsquo;re just eight! In this analogy, humanity is the eight-year-old CEO. Hiring a candidate is like training the superhuman AI model which will best serve our interests.</p>
<h3 id="building-planes">Building planes <a href="#building-planes" class="hash">#</a></h3>
<p>Suppose aerospace engineers have developed a new plane model. It&rsquo;s energy-efficient, cheap to produce and has increased passenger comfort. However, the engineers don&rsquo;t fully understand the internal workings of the engine. During testing, the engine seems to work alright. The engineers identified a few issues, but these could all be fixed quite easily. Would you be comfortable with this plane being produced for commercial use?</p>
<p>Here, the AI models are like the engines. We know how to build AI models capable of writing poetry and conducting PhD-level research, but our understanding of how these models learn is relatively limited<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. So, we should probably reflect more carefully on how we&rsquo;re deploying LLMs.</p>
<h3 id="drug-regulation">Drug regulation <a href="#drug-regulation" class="hash">#</a></h3>
<p>It takes years for newly developed drugs to reach consumers. First, you need preclinical trials. Then you carry out clinical trials in three distinct phases. This done, you need the approval of a regulatory agency, such as the FDA. It&rsquo;s not uncommon for the entire process to take 10-15 years. Given that we subject drugs to such rigorous testing, why not do the same for LLMs?</p>
<p>I first heard this analogy in <a href="https://open.spotify.com/episode/38R2p5TG0uO02q3xybxsvR?si=7f8fa707ea174823">this podcast</a> (Swedish, sorry), where Olle Häggström makes the case for AI slowdown. I think the above analogy is quite compelling, although I don&rsquo;t fully share his views.</p>
<h3 id="the-hustler">The hustler <a href="#the-hustler" class="hash">#</a></h3>
<p>Imagine a person trying to learn a new skill, say playing Go. He has memorised all textbooks on Go ever published by heart, as well as all the games played by professional Go players. Moreover, he&rsquo;s extremely hardworking: he plays roughly 1.5 million games against himself per day. (He doesn&rsquo;t need any sleep, and he happens to think very quickly.) Given the amount of practise he gets, how can normal humans hope to defeat him?</p>
<p>Here, the hustler is similar to an RL system. To me, this analogy makes the prospect of an intelligence explosion seem much more plausible<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and less sci-fi-ish.</p>
<h3 id="final-thoughts">Final thoughts <a href="#final-thoughts" class="hash">#</a></h3>
<p>Finally, to understand the alignment problem, I think it&rsquo;s also worth appreciating the potential impact of superhuman AGI. To me it seems like superhuman AI could be about as transformative as the industrial revolution. At the very least, I&rsquo;d expect it to be as impactful as electricity. So, the ensuring the development of AGI &ldquo;goes well&rdquo; seems like a key problem of our time.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This is one of my all-time favourite pieces on AI alignment.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I don&rsquo;t count &ldquo;just backpropagate&rdquo; as a satisfactory answer!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>This analogy was inspired by a conversation with Samuel Ratnam at EAG.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
