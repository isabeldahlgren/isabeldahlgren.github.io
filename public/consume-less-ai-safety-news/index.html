<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/consume-less-ai-safety-news/" />
    <title>Consume less AI safety news</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>Consume less AI safety news</h1>
        <div>
                <time>June 15, 2025</time>
            </div>
    </header><p>It&rsquo;s hard staying on top of all the AI safety news. Some people, like <a href="https://thezvi.wordpress.com/about/">Zvi</a>, have basically made this their full-time job.</p>
<p>A common failure mode for forming views on AI safety is consuming too much information. It&rsquo;s a tendency I&rsquo;ve observed in myself, as well as in others in the AI safety community.</p>
<p>I think it comes from the urge to solve the AI alignment problem quickly. It&rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there&rsquo;s a lot of general excitement surrounding the area.</p>
<p>So we might imagine someone who starts following all the leading researchers on Twitter, listening to <em>The Cognitive Revolution</em> while commuting and reading LessWrong posts before going to bed. Or maybe they&rsquo;ll have lengthy discussions about AI governance in WhatsApp groups and watch Robert Miles&rsquo; YouTube videos over meals.</p>
<p>But if you&rsquo;re looking to gain a deeper understanding of the AI safety landscape, this isn&rsquo;t enough. You&rsquo;d have to engage more with the material, shifting your creation-to-consumption ratio towards more creation. And you&rsquo;d have to discuss your views with people in real life and, if possible, engage in a local community<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. A common piece of advice for people aiming to become experts is to focus on <a href="https://forum.effectivealtruism.org/posts/ckj6Moau9qpYArHWc/want-to-be-an-expert-build-deep-models">building deep models</a>.</p>
<p>Even if your goal is just to get the big picture, the above approach seems needlessly high effort. There are many excellent resources summarising the main ideas in AI safety, such as the the 80,000 hours <a href="https://80000hours.org/problem-profiles/risks-from-power-seeking-ai/">problem profile</a>. It&rsquo;s a 60 min read with a very high signal-to-noise ratio.</p>
<p>Either way, binging AI safety-related material to won&rsquo;t help fix the AI alignment problem. It&rsquo;s stressful, and perhaps self-defeating. Given that the AI safety landscape is changing so rapidly, much of what we&rsquo;re seeing is noise. To gain conceptual clarity, perhaps you&rsquo;ll benefit from consuming less AI-safety related news. Unless you&rsquo;re working full-time on AI safety, either as a policy-maker or a researcher, this probably won&rsquo;t negatively influence your ability to do good work.</p>
<p>Hard problem require careful reflection. Although AI advances fast, we must think slowly about how to ensure things go well.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>My friend Miles Kodama put it well: &ldquo;It is easy to BS to a screen&rdquo;.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>, <a href="/tags/ai">ai</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright Â© 2025 Isabel Dahlgren
</footer>
</body>
</html>
