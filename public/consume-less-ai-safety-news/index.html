<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - https://isabeldahlgren.github.io/">
  <title>Consume less AI safety news | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Consume less AI safety news">
  <meta name="twitter:description" content="It’s hard staying on top of all the AI safety news. Some people, like Zvi, have basically made this their full-time job.
A common failure mode for forming views on AI safety is consuming too much information. It’s a tendency I’ve observed in myself, as well as in others in the AI safety community.
I think it comes from the urge to solve the AI alignment problem quickly. It’s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there’s a lot of general excitement surrounding the area.">

<meta property="og:url" content="https://isabeldahlgren.github.io/consume-less-ai-safety-news/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="Consume less AI safety news">
  <meta property="og:description" content="It’s hard staying on top of all the AI safety news. Some people, like Zvi, have basically made this their full-time job.
A common failure mode for forming views on AI safety is consuming too much information. It’s a tendency I’ve observed in myself, as well as in others in the AI safety community.
I think it comes from the urge to solve the AI alignment problem quickly. It’s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there’s a lot of general excitement surrounding the area.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-06-01T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">
    <meta property="article:tag" content="Ai">


  <meta itemprop="name" content="Consume less AI safety news">
  <meta itemprop="description" content="It’s hard staying on top of all the AI safety news. Some people, like Zvi, have basically made this their full-time job.
A common failure mode for forming views on AI safety is consuming too much information. It’s a tendency I’ve observed in myself, as well as in others in the AI safety community.
I think it comes from the urge to solve the AI alignment problem quickly. It’s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there’s a lot of general excitement surrounding the area.">
  <meta itemprop="datePublished" content="2025-06-01T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-06-01T00:00:00+00:00">
  <meta itemprop="wordCount" content="378">
  <meta itemprop="keywords" content="Ai-Alignment,Ai">
  <link rel="canonical" href="https://isabeldahlgren.github.io/consume-less-ai-safety-news/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "Consume less AI safety news",
    "headline": "Consume less AI safety news",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eIt\u0026rsquo;s hard staying on top of all the AI safety news. Some people, like \u003ca href=\u0022https:\/\/thezvi.wordpress.com\/about\/\u0022\u003eZvi\u003c\/a\u003e, have basically made this their full-time job.\u003c\/p\u003e\n\u003cp\u003eA common failure mode for forming views on AI safety is consuming too much information. It\u0026rsquo;s a tendency I\u0026rsquo;ve observed in myself, as well as in others in the AI safety community.\u003c\/p\u003e\n\u003cp\u003eI think it comes from the urge to solve the AI alignment problem quickly. It\u0026rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there\u0026rsquo;s a lot of general excitement surrounding the area.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https:\/\/isabeldahlgren.github.io\/consume-less-ai-safety-news\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-06-01T00:00:00.00Z",
    "datePublished": "2025-06-01T00:00:00.00Z",
    "dateModified": "2025-06-01T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "https://isabeldahlgren.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https:\/\/isabeldahlgren.github.io\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "https://isabeldahlgren.github.io/",
    "url" : "https:\/\/isabeldahlgren.github.io\/consume-less-ai-safety-news\/",
    "wordCount" : "378",
    "genre" : [ "ai-alignment" , "ai" ],
    "keywords" : [ "ai-alignment" , "ai" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">Consume less AI safety news</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-06-01"> June 01, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>, <a href='/tags/ai'>ai</a>
        </p>
      

      <span itemprop="articleBody">
      <p>It&rsquo;s hard staying on top of all the AI safety news. Some people, like <a href="https://thezvi.wordpress.com/about/">Zvi</a>, have basically made this their full-time job.</p>
<p>A common failure mode for forming views on AI safety is consuming too much information. It&rsquo;s a tendency I&rsquo;ve observed in myself, as well as in others in the AI safety community.</p>
<p>I think it comes from the urge to solve the AI alignment problem quickly. It&rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there&rsquo;s a lot of general excitement surrounding the area.</p>
<p>So we might imagine someone who starts following all the leading researchers on Twitter, listening to <em>The Cognitive Revolution</em> while commuting and reading LessWrong posts before going to bed. Or maybe they&rsquo;ll have lengthy discussions about AI governance in WhatsApp groups and watch Robert Miles&rsquo; YouTube videos over meals.</p>
<p>But if you&rsquo;re looking to gain a deeper understanding of the AI safety landscape, this isn&rsquo;t enough. You&rsquo;d have to engage more with the material, shifting your creation-to-consumption ratio towards more creation. And you&rsquo;d have to discuss your views with people in real life and, if possible, engage in a local community<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. A common piece of advice for people aiming to become experts is to focus on <a href="https://forum.effectivealtruism.org/posts/ckj6Moau9qpYArHWc/want-to-be-an-expert-build-deep-models">building deep models</a>.</p>
<p>Even if your goal is just to get the big picture, the above approach seems needlessly high effort. There are many excellent resources summarising the main ideas in AI safety, such as the the 80,000 hours <a href="https://80000hours.org/problem-profiles/risks-from-power-seeking-ai/">problem profile</a>. It&rsquo;s a 60 min read with a very high signal-to-noise ratio.</p>
<p>Either way, binging AI safety-related material to won&rsquo;t help fix the AI alignment problem. It&rsquo;s stressful, and perhaps self-defeating. Given that the AI safety landscape is changing so rapidly, much of what we&rsquo;re seeing is noise. To gain conceptual clarity, perhaps you&rsquo;ll benefit from consuming less AI-safety related news. Unless you&rsquo;re working full-time on AI safety, either as a policy-maker or a researcher, this probably won&rsquo;t negatively influence your ability to do good work.</p>
<p>Hard problem require careful reflection. Although AI advances fast, we must think slowly about how to ensure things go well.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>My friend <a href="https://mkodama.org/">Miles Kodama</a> put it well: &ldquo;It is easy to BS to a screen&rdquo;.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
