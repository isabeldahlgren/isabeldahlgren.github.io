<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/how-ai-might-go-wrong/" />
    <title>How AI might go wrong</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>How AI might go wrong</h1>
        <div>
                <time>March 16, 2025</time>
            </div>
    </header><p>As with any new technology, advanced AI entails certain risks. While we&rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.</p>
<p>The other week, I had an insightful discussion with people in <a href="https://www.zurich-ai-alignment.com">Zürich AI Alignment (ZAIA)</a> about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:</p>
<ul>
<li><strong>Powerful technology in bad hands</strong>: First, there are the risks which all fall under the label &ldquo;bad guys using powerful technology to further their own interests&rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I&rsquo;d guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.</li>
<li><strong>Concentration of power</strong>: AI is a transformative technology - a message that shouldn&rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&rsquo;s likely to have a profound impact on things like our economy and healthcare system; see e.g. <a href="https://www.weforum.org/stories/2025/03/ai-transforming-global-health/">here</a>. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren&rsquo;t democratically elected.</li>
<li><strong>Misaligned AI models</strong>: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&rsquo;ve already seen plenty of examples of this. For example, as highlighted in <a href="https://www.youtube.com/watch?v=zn2ukSnDqSg">this video</a>, it&rsquo;s still quite easy jailbreaking LLMs like ChatGPT. Moreover, there are <a href="https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples">striking examples</a>
) of algorithms encoding our own biases.</li>
<li><strong>Power-seeking AI</strong>: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&rsquo;ll briefly outline what I understand to be the core argument. The first premise is that AIs might end up with power-seeking behaviour. If power is instrumental in achieving one of the AI&rsquo;s goals, the AI might learn to optimise for power. At the same time, we&rsquo;re likely to develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities. Now the punchline is that such an AI might view humans as obstructions to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly: &ldquo;You can&rsquo;t fetch the coffee if you&rsquo;re dead&rdquo;. So, the argument goes, the AI might try to disempower humanity.</li>
</ul>
<p>The argument about power-seeking AI is quite subtle, so I&rsquo;ll elaborate a bit. I think the keyword here is &ldquo;agency&rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &ldquo;agentic&rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage.</p>
<p>Of course, there&rsquo;s much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. For a more fine-grained taxonomy, see <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary">this article</a>. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate or mitigate some of these risks. If we take adequate measures, I believe we can make AI &ldquo;go well&rdquo; with very high probability - something like 90%. Just take a moment to imagine what that would be like.</p>
<p><em>Thanks to Isaia Gisler for feedback on an earlier version of this text.</em></p>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright © 2025 Isabel Dahlgren
</footer>
</body>
</html>
