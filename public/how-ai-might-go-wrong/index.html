<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=55424&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - http://localhost:55424/">
  <title>How AI might go wrong | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="How AI might go wrong">
  <meta name="twitter:description" content="As with any new technology, advanced AI entails certain risks. While we’re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.
The other week, I had an insightful discussion with people in Zürich AI Alignment (ZAIA) about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:">

<meta property="og:url" content="http://localhost:55424/how-ai-might-go-wrong/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="How AI might go wrong">
  <meta property="og:description" content="As with any new technology, advanced AI entails certain risks. While we’re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.
The other week, I had an insightful discussion with people in Zürich AI Alignment (ZAIA) about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-03-09T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">


  <meta itemprop="name" content="How AI might go wrong">
  <meta itemprop="description" content="As with any new technology, advanced AI entails certain risks. While we’re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.
The other week, I had an insightful discussion with people in Zürich AI Alignment (ZAIA) about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:">
  <meta itemprop="datePublished" content="2025-03-09T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-03-09T00:00:00+00:00">
  <meta itemprop="wordCount" content="726">
  <meta itemprop="keywords" content="Ai-Alignment">
  <link rel="canonical" href="http://localhost:55424/how-ai-might-go-wrong/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="http://localhost:55424/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="http://localhost:55424/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "How AI might go wrong",
    "headline": "How AI might go wrong",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eAs with any new technology, advanced AI entails certain risks. While we\u0026rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.\u003c\/p\u003e\n\u003cp\u003eThe other week, I had an insightful discussion with people in \u003ca href=\u0022https:\/\/www.zurich-ai-alignment.com\u0022\u003eZürich AI Alignment (ZAIA)\u003c\/a\u003e about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http:\/\/localhost:55424\/how-ai-might-go-wrong\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-03-09T00:00:00.00Z",
    "datePublished": "2025-03-09T00:00:00.00Z",
    "dateModified": "2025-03-09T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "http://localhost:55424/",
        "logo": {
            "@type": "ImageObject",
            "url": "http:\/\/localhost:55424\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "http://localhost:55424/",
    "url" : "http:\/\/localhost:55424\/how-ai-might-go-wrong\/",
    "wordCount" : "726",
    "genre" : [ "ai-alignment" ],
    "keywords" : [ "ai-alignment" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">How AI might go wrong</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-03-09"> March 09, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>
        </p>
      

      <span itemprop="articleBody">
      <p>As with any new technology, advanced AI entails certain risks. While we&rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.</p>
<p>The other week, I had an insightful discussion with people in <a href="https://www.zurich-ai-alignment.com">Zürich AI Alignment (ZAIA)</a> about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:</p>
<ul>
<li><strong>Powerful technology in bad hands</strong>: First, there are the risks which all fall under the label &ldquo;bad guys using powerful technology to further their own interests&rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I&rsquo;d guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.</li>
<li><strong>Concentration of power</strong>: AI is a transformative technology - a message that shouldn&rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&rsquo;s likely to have a profound impact on things like our economy and healthcare system; see e.g. <a href="https://www.weforum.org/stories/2025/03/ai-transforming-global-health/">here</a>. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren&rsquo;t democratically elected.</li>
<li><strong>Misaligned AI models</strong>: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&rsquo;ve already seen plenty of examples of this. For example, as highlighted in <a href="https://www.youtube.com/watch?v=zn2ukSnDqSg">this video</a>, it&rsquo;s still quite easy jailbreaking LLMs like ChatGPT. Moreover, there are <a href="https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples">striking examples</a>
) of algorithms encoding our own biases.</li>
<li><strong>Power-seeking AI</strong>: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&rsquo;ll briefly outline what I understand to be the core argument. The first premise is that AIs might end up with power-seeking behaviour. If power is instrumental in achieving one of the AI&rsquo;s goals, the AI might learn to optimise for power. At the same time, we&rsquo;re likely to develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities. Now the punchline is that such an AI might view humans as obstructions to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly: &ldquo;You can&rsquo;t fetch the coffee if you&rsquo;re dead&rdquo;. So, the argument goes, the AI might try to disempower humanity.</li>
</ul>
<p>The argument about power-seeking AI is quite subtle, so I&rsquo;ll elaborate a bit. I think the keyword here is &ldquo;agency&rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &ldquo;agentic&rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage.</p>
<p>Of course, there&rsquo;s much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. For a more fine-grained taxonomy, see <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary">this article</a>. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate or mitigate some of these risks. If we take adequate measures, I believe we can make AI &ldquo;go well&rdquo; with very high probability - something like 90%. Just take a moment to imagine what that would be like.</p>
<p><em>Thanks to Isaia Gisler for feedback on an earlier version of this text.</em></p>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
