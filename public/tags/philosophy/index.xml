<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Philosophy on Isabel Dahlgren</title>
    <link>http://localhost:1313/tags/philosophy/</link>
    <description>Recent content in Philosophy on Isabel Dahlgren</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 09 Mar 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/philosophy/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>How I use LLMs</title>
      <link>http://localhost:1313/how-i-use-llms/</link>
      <pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/how-i-use-llms/</guid>
      <description>&lt;p&gt;&lt;em&gt;Related: &lt;a href=&#34;https://isabeldahlgren.github.io/will-ai-replace-mathematicians/&#34;&gt;Will AI replace mathematicians?&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;About 40% of students in the ETH main library always have a Chat-GPT tab open. I soon decided to try using LLMs for my own studies (because the wisdom of the crowd is a real thing). I haven&amp;rsquo;t figured out how to best use LLMs for my coursework, but I&amp;rsquo;m experimenting with various approaches.&lt;/p&gt;
&lt;h3 id=&#34;getting-unstuck&#34;&gt;Getting unstuck&lt;/h3&gt;
&lt;p&gt;For me, a big time sink is getting stuck on details. I usually go over the lecture notes after lectures, trying to work out the steps I didn&amp;rsquo;t follow with pen and paper. Ideally, I&amp;rsquo;d do this sitting next to a friend - it&amp;rsquo;s very convenient having someone whom to ask nearby. As Nate Soares put it&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I&amp;rsquo;ve either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words. &amp;lsquo;Dude. Disjunction. &lt;em&gt;Dis&lt;/em&gt;junction.&amp;rsquo;&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But studying with a friend isn&amp;rsquo;t always possible. If there&amp;rsquo;s a point of confusion I cannot resolve myself after making a reasonable effort, asking an LLM might help. Formulating a good question is always an instructive exercise. Moreover, nine times out of ten, the response is useful. Even if the LLM doesn&amp;rsquo;t entirely solve my problem, it might reference relevant concepts or serve as a sanity check. Sometimes I&amp;rsquo;ll learn that my approach was completely mistaken - and that&amp;rsquo;s certainly useful too!&lt;/p&gt;
&lt;h3 id=&#34;hints&#34;&gt;Hints&lt;/h3&gt;
&lt;p&gt;In the classic &lt;em&gt;How to Solve It&lt;/em&gt;, George Pólya famously noted that mathematics isn&amp;rsquo;t a spectator sport. By generating hints, LLMs can be an aid in the problem-solving process too. Just have an honest attempt at the problem before consulting an LLM, and tell the LLM to not give away the entire solution. But it&amp;rsquo;s important to notice when one is stuck and ask for help. For someone like me who usually waits too long before taking hints, the ease of generating hints with Chat-GPT makes a huge difference.&lt;/p&gt;
&lt;h3 id=&#34;the-big-picture&#34;&gt;The big picture&lt;/h3&gt;
&lt;p&gt;LLMs are terrific at explaining high-level ideas. I&amp;rsquo;m a big fan of learning concepts &amp;ldquo;top-down&amp;rdquo;, starting with the big picture before getting into the details. While having more context doesn&amp;rsquo;t necessarily mean the material sticks better, I find this approach much more enjoyable. I usually ask Chat-GPT to give me the key idea before I look into the details. Apart from this, I regularly prompt Chat-GPT to give me the intuition for something or to motivate concepts. If a lecturer is pressed on time, they&amp;rsquo;ll cut the motivation bit, rather than leaving out a definition or theorem statement. For this reason, an AI-generated introduction can complement the lectured material.&lt;/p&gt;
&lt;p&gt;Some of my favourite prompts include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Why do we care about X?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What is the main idea behind the proof of X?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What&amp;rsquo;s the intuition for this definition?&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also find it helpful trying to explain a concept in my own words and asking Chat-GPT to elaborate or check if my explanation is accurate.&lt;/p&gt;
&lt;h3 id=&#34;caveats&#34;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;All this said, I&amp;rsquo;d like to add a few caveats.&lt;/p&gt;
&lt;p&gt;A friend or teaching assistant could help with the above tasks better than today&amp;rsquo;s LLMs. They know about your mathematical background and what conventions you&amp;rsquo;re using. When I interact with chatbots, explaining conventions and providing context adds a lot of overhead. However, this problem seems fixable. Many AI labs are already working on ways to provide more personalised responses by having the chatbot remember information across chat sessions&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Students could e.g. upload lecture notes and indicate which parts they&amp;rsquo;d covered.&lt;/p&gt;
&lt;p&gt;AI systems also make mistakes. But this isn&amp;rsquo;t that big of an issue. Most mistakes are easy to spot, especially if you ask the AI to explain steps that seem fishy. If you point out what went wrong, it will modify the argument. With human guidance, AI systems can get quite far. Also, LLMs don&amp;rsquo;t need to get all the details right in order to be useful. As Terry Tao noted&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Strangely, even nonsensical LLM-generated math often references relevant concepts. With effort, human experts can modify ideas that do not work as presented into a correct and original argument. The 2023-level AI can already generate suggestive hints and promising leads to a working mathematician and participate actively in the decision-making process.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Of course, if you want to be dead certain the AI-generated argument is correct, have the AI output a formal proof in Lean.&lt;/p&gt;
&lt;p&gt;Another fear of mine, perhaps ungrounded, is basically that LLMs will make us lazy. Learning requires a certain amount of effort, while writing a good LLM prompt is relatively easy. If we use LLMs more and more, will we remove the friction needed for learning? I don&amp;rsquo;t know whether this fear is valid or if it&amp;rsquo;s just an instance of &amp;ldquo;tech panic&amp;rdquo;. But as long as we set boundaries for our LLM usage, we need not spoil the learning experience.&lt;/p&gt;
&lt;h3 id=&#34;where-does-this-leave-us&#34;&gt;Where does this leave us?&lt;/h3&gt;
&lt;p&gt;It seems, then, as if we could overcome the problems I&amp;rsquo;ve encountered when tinkering with AIs. My experience with using LLMs as part of my studies has been positive, so I&amp;rsquo;ll continue exploring ways in which AI can assist. The one thing that LLMs can&amp;rsquo;t provide, however, is the social aspect of doing maths. Solving problems with others is infinitely more fun than coming up with LLM prompts. If anything, I think the above use cases highlight the importance of doing maths together with others.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things&#34;&gt;On learning difficult things | LessWrong&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://help.openai.com/en/articles/8983136-what-is-memory&#34;&gt;What is memory? | OpenAI&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://unlocked.microsoft.com/ai-anthology/terence-tao/&#34;&gt;Embracing change and resetting expectations | Microsoft Unlocked&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Will AI replace mathematicians?</title>
      <link>http://localhost:1313/will-ai-replace-mathematicians/</link>
      <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/will-ai-replace-mathematicians/</guid>
      <description>&lt;p&gt;I used to think of maths as the one thing LLMs couldn&amp;rsquo;t do well. While GPT 3.0 would excel in language-based tasks, it would struggle to solve elementary maths problems. But a lot has happened since then. Over the last year, I&amp;rsquo;ve come to take the idea of using AI as an aid for doing maths more seriously. In fact, I now believe LLMs might prove new theorems with no human guidance within just 3 years.&lt;/p&gt;
&lt;p&gt;First, there was the silver medal at International Mathematics Olympiad (IMO)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Although AlphaProof and AlphaGeometry 2 took well over 9h, the time given contestants are given, it&amp;rsquo;s quite a feat: IMO problems require an element of creativity. Not only that - it was able to formalise its solution in Lean. Lean is still a fairly new programming language, and there isn&amp;rsquo;t nearly as much training data as for other programming languages. The work of the DeepMind team shows two things: firstly, lots of clever people are trying to build AI systems for doing maths; secondly, apparently their current approach works pretty well. However, as of April 2025, you don&amp;rsquo;t need an AI specifically trained to do maths in order to solve tricky problems: the new o3 and o4 mini models achieve impressing performance in the American Invitational Mathematics Examination (AIME)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. What if we use RL to build AI systems specialised in more advanced topics? Perhaps these AIs might prove new theorems. Even if they don&amp;rsquo;t, they might provide researchers with insights.&lt;/p&gt;
&lt;p&gt;Next, several top-gun mathematicians think AI might transform maths research in the next decade. Most notably, Terence Tao has highlighted ways in which machines can help human mathematicians. Here&amp;rsquo;s Tao&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I could feed GPT-4 the first few PDF pages of a recent math preprint and get it to generate a half-dozen intelligent questions that an expert attending a talk on the preprint could ask. I plan to use variants of such prompts to prepare my future presentations or to begin reading a technically complex paper.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Richard Borcherds seems equally optimistic about the possibilities of using, predicting that AI might even surpass human mathematicians within 10 years&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Overall, I think we&amp;rsquo;re starting to see a cultural shift in the maths community. People are recognising that AI is a huge deal.&lt;/p&gt;
&lt;p&gt;Finally, I observed that 40% of students in the library seem to have a Chat-GPT tab open at all times. These are students doing STEM subjects, such as maths and physics. This seems like an important data point (and this isn&amp;rsquo;t just because I&amp;rsquo;m giving more weight to first-hand experience). LLMs are transforming the way students learn, and these are the people who will go on to do research in a couple of years. Chances are we won&amp;rsquo;t stop using LLMs just because the material becomes more niche. Even if you receive a hallucinatory answer, the LLM might reference a relevant concept, helping you get unstuck. I&amp;rsquo;m using Chat-GPT for my own studies, and I&amp;rsquo;m impressed by its explaining abilities. Basically, it can easily handle any concept you&amp;rsquo;ll come across in a master degree in mathematics. I&amp;rsquo;ve also prompted Chat-GPT to distill the key ideas from more recent papers and found its responses very helpful.&lt;/p&gt;
&lt;p&gt;All in all, I&amp;rsquo;ve come to shorten my AI timelines quite a bit. But rather than thinking &amp;ldquo;Will I ever find a job?&amp;rdquo;, I find myself thinking &amp;ldquo;What a time to be alive!&amp;rdquo;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level&#34;&gt;AI achieves silver-medal standard solving International Mathematical Olympiad problems - Google DeepMind&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://openai.com/index/introducing-o3-and-o4-mini&#34;&gt;Introducing OpenAI o3 and o4-mini | OpenAI&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://unlocked.microsoft.com/ai-anthology/terence-tao&#34;&gt;Embracing change and resetting expectations; Microsoft Unlocked&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://epoch.ai/frontiermath/expert-perspectives&#34;&gt;AI and the future of Math: Interviews with Top Mathematicians | Epoch AI&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>&#34;Just switch it off&#34;</title>
      <link>http://localhost:1313/just-switch-it-off/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/just-switch-it-off/</guid>
      <description>&lt;p&gt;If we develop a rogue AI, couldn&amp;rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &amp;ldquo;switching off&amp;rdquo; would mean deleting a model&amp;rsquo;s weights, so it can&amp;rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?&lt;/p&gt;
&lt;p&gt;First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&amp;rsquo;t &amp;ldquo;want&amp;rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&amp;rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &amp;ldquo;alignment faking&amp;rdquo;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Second, and perhaps more worryingly, is the question of whether we as a society want to &amp;ldquo;press delete&amp;rdquo;. Turning off sandboxed AI - AI developed in a secure training environment - isn&amp;rsquo;t a big deal. The negative consequences, if any, are limited to the few people who can access the model. But a leading AI company has strong incentives against withdrawing potentially unsafe models from the market. Doing this would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&amp;rsquo;s the geopolitical aspect. As highlighted in the AI 2027 report&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the fear of falling behind in the AI arms race might lead us to deploy even misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, despite poor performance on safety benchmarks.&lt;/p&gt;
&lt;p&gt;Switching off an AI isn&amp;rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&amp;rsquo;s likely to become more difficult with more capable models. Then there&amp;rsquo;s the human factor. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;Alignment Faking in Large Language Models&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ai-2027.com/&#34;&gt;AI 2027&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>From a YouTube alumni</title>
      <link>http://localhost:1313/from-a-youtube-alumni/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/from-a-youtube-alumni/</guid>
      <description>&lt;p&gt;Random people on the Internet have played a huge role in my education. I&amp;rsquo;m not just referring to my coursework at university, but also to &amp;ldquo;Bildung&amp;rdquo; more generally. I&amp;rsquo;ve learned a ton by browsing StackOverflow threads and reading Medium articles. However, I&amp;rsquo;ve probably learned the most from watching YouTube.&lt;/p&gt;
&lt;h3 id=&#34;learning-by-watching&#34;&gt;Learning by watching&lt;/h3&gt;
&lt;p&gt;Above all, there&amp;rsquo;s some really high-quality content out there. Nowadays there are full-time YouTubers, working on creating professional, meticulously edited videos. And some channels, such as &lt;a href=&#34;https://www.youtube.com/channel/UCsXVk37bltHxD1rDPwtNM8Q&#34;&gt;Kurzgesagt&lt;/a&gt;, are even ran by entire teams of illustrators and script-writers. Moreover, because anyone can record themselves and upload it to YouTube, we have world-class experts sharing their knowledge in YouTube lectures&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This means there are some truly remarkable YouTube videos. For example, here are comments from some &lt;a href=&#34;https://www.youtube.com/@3blue1brown&#34;&gt;3Blue1Brown&lt;/a&gt; videos:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I dropped out in 10th grade 25 years ago and your videos have inspired me to go back to school.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You sir truly deserve an honorary doctorate - just for this video. Your impact to generations of confused engineering and math students will forever ripple through our society.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I have a master&amp;rsquo;s degree in mechanical engineering and I&amp;rsquo;m starting to think I should redo my whole education from ground up searching for this kind of intuitive knowledge. It&amp;rsquo;s absurd that I find out explanations which are as intuitive as this one so late in my life. I&amp;rsquo;m blown away completely! I mean how many bits of information have we stumbled upon during our formal education failing to see how they elegantly relate to each other and form a bigger picture&amp;hellip;oh my!&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, videos tend to be more attention-grabbing than articles. Although most people are unable to read while cooking or brushing their teeth, they can watch videos. So getting started learning has never required less willpower: just search &amp;ldquo;Introduction to&amp;hellip;&amp;rdquo; on YouTube.&lt;/p&gt;
&lt;h3 id=&#34;learning-to-watch&#34;&gt;Learning to watch&lt;/h3&gt;
&lt;p&gt;Of course, YouTube isn&amp;rsquo;t designed to be a learning platform. But there ways of optimising for a better learning experience.&lt;/p&gt;
&lt;p&gt;The first step is to recognise that YouTube tries maximising user retention. This is a feature, not a bug. It means we can design our YouTube interface such that we end up binge watching informative videos about topics we care about. Here are some ways of achieving this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Train your algorithm&lt;/strong&gt;: In your YouTube feed, take 2 seconds to press &amp;ldquo;Not interested&amp;rdquo; whenever something irrelevant pops up. It pays off - I find the YouTube algorithm to be surprisingly sensitive to my feedback.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keep separate accounts&lt;/strong&gt;: On a similar note, I have two Google accounts: my main account, and my secondary account. I&amp;rsquo;ll use my main account for watching &amp;ldquo;useful&amp;rdquo; content, while I&amp;rsquo;ll log onto my secondary account for, well, everything else.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Block channels&lt;/strong&gt;: One can use &lt;a href=&#34;https://getcoldturkey.com/support/how-to/allow-youtube-channel/&#34;&gt;ColdTurkey&lt;/a&gt; to block certain YouTube channels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second step is to recognise the limits of just watching videos. There&amp;rsquo;s a reason we don&amp;rsquo;t abandon more traditional media altogether. When reading a book, I find it much easier recognising when I&amp;rsquo;m confused. However, after finishing a video, I sometimes find myself completely lost and unable to tell where I stopped following. Moreover, whenever I have a physical textbook, I&amp;rsquo;ll often refer back to chapters I&amp;rsquo;ve finished, just to refresh my memory. Here are two partial fixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Media notes plugin&lt;/strong&gt;: The &lt;a href=&#34;obsidian://show-plugin?id=media-notes&#34;&gt;Media Notes&lt;/a&gt; plugin for Obsidian is a real game-changer. It allows you to watch YouTube videos from inside Obsidian and take notes with timestamps. When doing this, I seem to engage more with the material. Taking physical notes while watching YouTube is a bit overkill, so this seems like a good compromise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rewatch your favourite videos&lt;/strong&gt;: It&amp;rsquo;s easy ending up only consuming new content, just because the YouTube landing page is filled with new videos. But it&amp;rsquo;s worth saving your favourite videos to playlists and rewatching them later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I discussed above, there are some hacks for a better learning experience. However, YouTube could also design their platform differently. They could e.g. develop distraction-free mode, enabling the user to remove shorts, ads or sponsored content. From a technical perspective, it&amp;rsquo;s doable. But we could also go beyond ordinary videos. For example, Andy Matuschak and Michael Nielsen have explored ways of incorporating an element of spaced repetition in videos, making for a more interactive learning experience&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. In general, I&amp;rsquo;m excited about integrating modern technology with education. And from someone,&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;See e.g. &lt;a href=&#34;https://www.youtube.com/@AndrejKarpathy&#34;&gt;Andrej Karpathy&amp;rsquo;s channel&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;My friend Åke Lindblom first told me about this. He apparently has an insanely good algorithm.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://numinous.productions/ttft/#mnemonic-video&#34;&gt;How can we develop transformative tools for thought?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Look for opinions</title>
      <link>http://localhost:1313/look-for-opinions/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/look-for-opinions/</guid>
      <description>&lt;p&gt;Opinionated people can be really annoying. Wherever they go, they try convincing you of their ideas. If you have an opinionated uncle, the Christmas dinner might be ruined by a bitter argument. I&amp;rsquo;ve certainly had bad experiences with a dinner-table conversations turning into feuds. For this reason, I used to try having fewer opinions. I somehow assumed this meant being more open-minded and mature. Well, no.&lt;/p&gt;
&lt;h3 id=&#34;welcoming-opinions&#34;&gt;Welcoming opinions&lt;/h3&gt;
&lt;p&gt;Actually, there are plenty of benefits of actively trying to form more opinions. Even about topics you don&amp;rsquo;t know particularly well. If you learn with a view towards arguing, then you&amp;rsquo;ll pay closer attention to the material. I think this has to do with anchoring. If you pick a stance, even at random, you&amp;rsquo;ll be more emotionally invested. Holden Karnofsky summarised it neatly&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;By doing this [trying to have a hypothesis and rearticulating it whenever it changes], I try to &lt;strong&gt;continually focus my reading on the goal of forming a bottom-line view, rather than just “gathering information.”&lt;/strong&gt; I think this makes my investigations more focused and directed, and the results easier to retain. I consider this approach to be &lt;strong&gt;probably the single biggest difference-maker between &amp;ldquo;reading a ton about lots of things, but retaining little&amp;rdquo; and &amp;ldquo;efficiently developing a set of views on key topics and retaining the reasoning behind them.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, chatting with people with strong opinions can also be fun. Say you&amp;rsquo;re at a cocktail party. Small talk can be quite tiring, at least after a couple of hours. In this situation, I&amp;rsquo;ll gladly talk to people trying to persuade me of their ideas. Or say you&amp;rsquo;re hosting friends for dinner, and a friend explains her take on a topic you all like.&lt;/p&gt;
&lt;p&gt;Back in school, we were encouraged to form more opinions. Teachers made us write argumentative essays about topics we hardly knew anything about. They know that most 14 year-olds don&amp;rsquo;t care the slightest about whether fathers should be given two additional weeks of paternity leave, or if the capital income tax should be raised by 1%. But I don&amp;rsquo;t think it was only meant as an exercise in communicating effectively. It felt as if teachers were saying &amp;ldquo;Go out there in the big wild world, and look for opinions!&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;chasing-opinions&#34;&gt;Chasing opinions&lt;/h3&gt;
&lt;p&gt;So why doesn&amp;rsquo;t everyone have, like, a lot opinions?&lt;/p&gt;
&lt;p&gt;I think many people, whether they recognise it or not, resort to some kind of agnosticism for fear of being wrong. However, recognising you&amp;rsquo;re wrong just means you&amp;rsquo;re updating your beliefs. It&amp;rsquo;s not that big of a deal. Also, many people think they aren&amp;rsquo;t entitled to hold an opinion since they aren&amp;rsquo;t &amp;ldquo;qualified&amp;rdquo;. This is true for areas in which there&amp;rsquo;s a clear distinction between experts and non-experts. But I have a hunch that we sometimes use this as an excuse for not looking into certain issues. For example, as highlighted in &lt;em&gt;Superforecasting&lt;/em&gt;, normal people can form well-informed predictions on certain issues with a bit of practise&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Anyway, I think there&amp;rsquo;s a middle ground here: if you don&amp;rsquo;t know all the technical details, just adjust your confidence levels.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth emphasising that forming opinions can be very difficult and time-consuming. For example, predicting technological progress is notoriously hard. It requires you to do your homework, researching which factors influence scientific advancements and so on. This goes against the idea of opinions coming to us &amp;ldquo;naturally&amp;rdquo;, as if by chance. Yet, this isn&amp;rsquo;t a good reason not to actively seek out opinions. Sometimes, we have to make our minds up in order to take action.&lt;/p&gt;
&lt;p&gt;As long as one has some &amp;ldquo;epistemic etiquette&amp;rdquo; - being prepared to change your beliefs in the light of new evidence, and not taking everything so personally - having more opinions seems like a good thing. I&amp;rsquo;m currently trying to build the habit of always having a working hypothesis whenever I learn something new. Having more opinions makes you feel more like part of the world, rather than as a bystander.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cold-takes.com/learning-by-writing&#34;&gt;Learning By Writing&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Tetlock, Philip E., and Dan Gardner. &lt;em&gt;Superforecasting: The Art and Science of Prediction.&lt;/em&gt; Crown Publishers, 2015.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>How AI might go wrong</title>
      <link>http://localhost:1313/how-ai-might-go-wrong/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/how-ai-might-go-wrong/</guid>
      <description>&lt;p&gt;As with any new technology, advanced AI entails certain risks. While we&amp;rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.&lt;/p&gt;
&lt;p&gt;The other week, I had an insightful discussion with people in &lt;a href=&#34;https://www.zurich-ai-alignment.com&#34;&gt;Zürich AI Alignment (ZAIA)&lt;/a&gt; about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this think piece. Here are what I currently consider to be the most relevant risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Powerful technology in bad hands&lt;/strong&gt;: First, there are the risks which all fall under the label &amp;ldquo;bad guys using powerful technology to further their own interests&amp;rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I&amp;rsquo;d guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concentration of power&lt;/strong&gt;: AI is a transformative technology - a message that shouldn&amp;rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&amp;rsquo;s likely to have a profound impact on things like our economy and healthcare system&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren&amp;rsquo;t democratically elected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misaligned AI models&lt;/strong&gt;: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&amp;rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&amp;rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&amp;rsquo;ve already seen plenty of examples of this. For example, as pointed out during the ZAIA discussion, it&amp;rsquo;s still quite easy jailbreaking LLMs like ChatGPT&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Moreover, there are striking examples of algorithms encoding our own biases&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power-seeking AI&lt;/strong&gt;: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&amp;rsquo;ll briefly outline what I understand to be the core argument. The first premise is that AIs might end up with power-seeking behaviour. If power is instrumental in achieving one of the AI&amp;rsquo;s goals, the AI might learn to optimise for power. At the same time, we&amp;rsquo;re likely to develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Now the punchline is that such an AI might view humans as obstructions to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly: &amp;ldquo;You can&amp;rsquo;t fetch the coffee if you&amp;rsquo;re dead&amp;rdquo;. So, the argument goes, the AI might try to disempower humanity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The argument about power-seeking AI is quite subtle, so I&amp;rsquo;ll elaborate a bit. I think the keyword here is &amp;ldquo;agency&amp;rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &amp;ldquo;agentic&amp;rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage.&lt;/p&gt;
&lt;p&gt;Of course, there&amp;rsquo;s much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate or mitigate some of these risks&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. If we take adequate measures, I believe we can make AI &amp;ldquo;go well&amp;rdquo; with very high probability - something like 90%. Just take a moment to imagine what that would be like.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Isaia Gisler for feedback on an earlier version of this text.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.weforum.org/stories/2025/03/ai-transforming-global-health/&#34;&gt;6 ways AI is transforming healthcare | World Economic Forum&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zn2ukSnDqSg&#34;&gt;ChatGPT Jailbreak - Computerphile - YouTube&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples&#34;&gt;AI Bias Examples | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai&#34;&gt;Agentic AI vs. Generative AI | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary&#34;&gt;Risks from AI Overview | LessWrong&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Thought experiments for normal people</title>
      <link>http://localhost:1313/thought-experiments-for-normal-people/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/thought-experiments-for-normal-people/</guid>
      <description>&lt;p&gt;Philosophers love thought experiments. Thought experiments are hypothetical scenarios meant to tease out our intuitions about an argument or theory. For example, here&amp;rsquo;s a classic thought experiment, due to Robert Nozick:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose there were an experience machine that would give you any experience you desired. Superduper neuropsychologists could stimulate your brain so that you would think and feel you were writing a great novel, or making a friend, or reading an interesting book. All the time you would be floating in a tank, with electrodes attached to your brain. Should you plug into this machine for life, preprogramming your life&amp;rsquo;s experiences? &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Many thought experiments seem kind of cooked up, so it&amp;rsquo;s easy to believe thought experiments have no practical use. However, one of my key takeaways from &lt;em&gt;The Scout Mindset&lt;/em&gt; by Julia Galef was that thought experiments aren&amp;rsquo;t just diversion for people with too much spare time. In fact, she argues, certain thought experiments can help us think more clearly about decisions we face in everyday life&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. In this essay, I&amp;rsquo;ll go over three interesting thought experiments from her book, and then describe two personal faves.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Outsider Test&lt;/strong&gt;: This all comes down to having an outsider&amp;rsquo;s perspective, as if one were calling a friend. How would an outsider evaluate the situation? For example, say John Doe has been given two job offers: one at company X, the other at company Y. Company X will look better on his CV, but he&amp;rsquo;s unlikely to enjoy the day-to-day tasks. In contrast, the job at company Y, although not quite as prestigious, seems more fun. Here, an outsider may say something like &amp;ldquo;If prestige weren&amp;rsquo;t a consideration, which option would you pick?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Conformity Test&lt;/strong&gt;: This is a good one. We&amp;rsquo;re often quick in adopting the beliefs of people we respect. And this is usually a good thing: life is to short to overthink everything, and we&amp;rsquo;ve got to form opinions somehow. (For fans of &lt;em&gt;Thinking, Fast and Slow&lt;/em&gt;, this is System 1 in action.) However, when it comes to more delicate subjects, this mental short-cut might fail. In the Conformity Test, Julia Galef asks you to imagine that people no longer hold your view. (To all contrarians out there, just imagine that the people in your community suddenly become just as everyone else.) There&amp;rsquo;s a particularly interesting spin-off here: what if one of the main proponents of your view, perhaps the person who helped shape your beliefs about the subject, would change their mind? I think the EA/rationalist community provides a good use case. What if Will MacAskill would say he was completely mistaken about longtermism, rejecting the idea altogether? Or if Eliezer Yudowsky would declare that AI after all isn&amp;rsquo;t that big of a threat?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Status Quo Bias Test&lt;/strong&gt;: The underlying idea here is that humans have a bias towards the status quo. If you were to start from scratch, would you actively choose your current situation? For example, imagine a medical student who realises halfway through second year of med school that medicine isn&amp;rsquo;t for her. Although she cannot imagine herself as a doctor, she&amp;rsquo;s still hesitant to switch subjects. Here the Status Quo Bias Test might come in handy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading &lt;em&gt;The Scout Mindset&lt;/em&gt;, I soon realised that some of the advice I&amp;rsquo;ve received over the years can be rephrased as thought experiments. Here are two such thought experiments which I&amp;rsquo;ve found particularly useful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Worst Case Scenario&lt;/strong&gt;: This just involves asking oneself about the worst-case scenario. Is it really that bad? If yes, well, then you know. At least your fear isn&amp;rsquo;t  some sort of vague, scary illusion. And if not, good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast forward a decade&lt;/strong&gt;: In ten years from now, which decision is one more likely to regret? Humans are famously bad at long-term planning. We&amp;rsquo;ll often fail to take the route of action with benefits in a distant future. (Hence climate change and the fact that most adults don&amp;rsquo;t get enough sleep.) So it might be a good idea doing some kind of Outsider test, where the outsider is one&amp;rsquo;s future self.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the above thought experiments might sound familiar. Perhaps you&amp;rsquo;ve already used some of them yourself. After all, these seem like obvious tricks for seeing this for what they are. But perhaps it&amp;rsquo;s helpful having names for these tricks. It&amp;rsquo;s a bit like building a toolkit for better decision-making. I&amp;rsquo;ve applied something like Worst Case Scenario a bunch of times, but only after spending a couple of days of dwelling on the issue in a very unproductive way. Thinking in terms of thought experiments would have spared me a lot of headache.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Nozick, Robert, and Thomas Nagel. &lt;em&gt;Anarchy, state, and utopia&lt;/em&gt;. Vol. 5038. New York: Basic books, 1974.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Galef, Julia. &lt;em&gt;The scout mindset: Why some people see things clearly and others don&amp;rsquo;t&lt;/em&gt;. Penguin, 2021.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Hunting dependencies</title>
      <link>http://localhost:1313/hunting-dependencies/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hunting-dependencies/</guid>
      <description>&lt;p&gt;One of the benefits of attending lectures is that lecturers tend to give some really good unsolicited advice every now and then. Last semester, a professor of ours digressed to talk about the importance of identifying the key ingredients of a result. He&amp;rsquo;d just concluded a rather long proof, and was about to clean the blackboard as he said that the result was elementary. See, you only need this one lemma from point-set topology (admittedly a bit niche, but easy to prove), and then the definition of the Fourier transform. Put that way, sure, maybe it&amp;rsquo;s elementary. Explicitly writing down the dependencies of an idea, he said, was a good exercise.&lt;/p&gt;
&lt;h3 id=&#34;an-example&#34;&gt;An example&lt;/h3&gt;
&lt;p&gt;I tried doing this a couple of times for definitions, theorem statements and proofs. At first, it felt a bit silly. Once I finished writing down the main components of a result, it seemed trivial. I overdid it at first, writing down dependencies even for minor lemmas. But for more complicated theorems or involved definitions, it proved quite useful.&lt;/p&gt;
&lt;p&gt;Now for a concrete example. Suppose we&amp;rsquo;re trying to understand martingales. Martingales can be thought of as sequences of random variables representing fair games: if we&amp;rsquo;re given the value of the $n$:th random variable, we expect the value of the $(n+1)$:th random variable to stay the same. There&amp;rsquo;s no predictable up- or downward trend. Here&amp;rsquo;s the definition from LeGall&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let $(X_n)_{n \in \mathbb{Z}_+}$ be an adapted, real-valued random process, such that $E(|X_n|) &amp;lt; \infty$ for every $n \in \mathbb{Z}_+$. We say that the process $(X_n)_{n \in \mathbb{Z}_+}$ is a martingale if, for every $n \in \mathbb{Z}_+$, $$E(X_{n+1}|\mathcal{F}_n) = X_n.$$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;First, we need some terminology related to random processes, understanding what&amp;rsquo;s meant by an &amp;ldquo;adapted process&amp;rdquo; and a &amp;ldquo;filtration&amp;rdquo;. Apart from that, we need a solid grasp of conditional expectations. The definition of a conditional expectation with respect to a $\sigma$-algebra, as well as the underlying intuition. And that&amp;rsquo;s about it.&lt;/p&gt;
&lt;h3 id=&#34;should-i-bother&#34;&gt;Should I bother?&lt;/h3&gt;
&lt;p&gt;If you want to learn something thoroughly, then yes. Tracking down dependencies makes you engage more with the material. Learning a concept isn&amp;rsquo;t just a matter of being able to regurgitate the contents of the lecture notes - it requires you to build your own mental model of what&amp;rsquo;s going on. As I see it, the purpose of exercises, quizzes or review questions is to make us think more carefully about a given topic. Otherwise, because humans (or at least maths students) are lazy, chances are we&amp;rsquo;ll go through the material too quickly. Tracking down dependencies is a bit like doing more problems, in that it prompts us to revisit the material.&lt;/p&gt;
&lt;p&gt;Another benefit of nailing down dependencies is that it makes the concept seem relatively simple. Some results can seem quite daunting at first. But working out the dependencies can make the concept seem deceptively simple - almost to the point where you&amp;rsquo;re struck with the curse of knowledge. Moreover, if I struggle to understand an idea but am clear about the dependencies, I know what to do: I&amp;rsquo;ll just read up on each of the topics involved. In this way, the dependencies translate into an a checklist for my learning process.&lt;/p&gt;
&lt;p&gt;Lastly, I found it satisfying seeing how different notions tied into one another. I also put it all into one mindmap which, apart from having a high aestethic value, gave me the big picture of the subject.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Le Gall, J. F. (2022). &lt;em&gt;Measure theory, probability, and stochastic processes&lt;/em&gt;. Cham: Springer.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    
    
    
  </channel>
</rss>
