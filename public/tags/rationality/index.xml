<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rationality on Isabel Dahlgren</title>
    <link>http://localhost:1313/tags/rationality/</link>
    <description>Recent content in Rationality on Isabel Dahlgren</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 30 Mar 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/rationality/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>Will AI replace mathematicians?</title>
      <link>http://localhost:1313/will-ai-replace-mathematicians/</link>
      <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/will-ai-replace-mathematicians/</guid>
      <description>&lt;p&gt;I used to think of maths as the one thing LLMs couldn&amp;rsquo;t do well. While GPT 3.0 would excel in language-based tasks, it would struggle to solve elementary maths problems. But a lot has happened since then. Over the last year, I&amp;rsquo;ve come to take the idea of using AI as an aid for doing maths more seriously. In fact, I now believe LLMs might prove new theorems with no human guidance within just 3 years.&lt;/p&gt;
&lt;p&gt;First, there was the silver medal at IMO&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Although AlphaProof and AlphaGeometry 2 took well over 3h, the time given contestants are given, it&amp;rsquo;s quite a feat: IMO problems require an element of creativity. Not only that - it was able to formalise its solution in Lean. Lean is still a fairly new programming language, and there isn&amp;rsquo;t nearly as much training data as for other programming languages. The work of the DeepMind team shows two things: firstly, lots of clever people are building AI systems for maths; secondly, apparently their approach works pretty well. However, as of April 2025, you don&amp;rsquo;t need an AI specifically trained to do maths in order to solve tricky problems: the new o3 and o4 mini models do quite well in the AIME&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. What if we use RL to build AI systems specialised in more advanced topics? Perhaps these AIs might prove new theorems. Even if they don&amp;rsquo;t, they might provide researchers with insights.&lt;/p&gt;
&lt;p&gt;Next, several top-gun mathematicians think AI might transform maths research in the next decade. Most notably, Terence Tao has highlighted ways in which machines can help human mathematicians. Here&amp;rsquo;s Tao&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I could feed GPT-4 the first few PDF pages of a recent math preprint and get it to generate a half-dozen intelligent questions that an expert attending a talk on the preprint could ask. I plan to use variants of such prompts to prepare my future presentations or to begin reading a technically complex paper.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Richard Borcherds seems equally optimistic about the possibilities of using, predicting that AI might even surpass human mathematicians within 10 years&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Overall, I think we&amp;rsquo;re starting to see a cultural shift in the maths community. People are recognising that AI is a huge deal.&lt;/p&gt;
&lt;p&gt;Finally, I observed that 50% of students in the library seem to have a Chat-GPT tab open at all times. These are students doing STEM subjects, such as maths and physics. This seems like an important data point (and this is not just because I&amp;rsquo;m giving more weight to first-hand experience). LLMs are transforming the way we learn, and these are the people who will go on to do research in a couple of years. Chances are they won&amp;rsquo;t stop using LLMs just because the material becomes more niche. Even if you receive a hallucinatory answer, the LLM might reference a relevant concept, helping you get unstuck. I&amp;rsquo;m using Chat-GPT for my own studies, and I&amp;rsquo;m genuinely impressed by its explaining abilities. Basically, it can easily handle any concept you&amp;rsquo;ll come across in a master degree in mathematics. I&amp;rsquo;ve also prompted Chat-GPT to distill the key ideas from more recent papers, and it does this pretty well.&lt;/p&gt;
&lt;p&gt;All in all, I&amp;rsquo;ve come to shorten my AI timelines quite a bit. But rather than thinking &amp;ldquo;Will I ever find a job?&amp;rdquo;, I find myself thinking &amp;ldquo;What a time to be alive!&amp;rdquo;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level&#34;&gt;AI achieves silver-medal standard solving International Mathematical Olympiad problems - Google DeepMind&lt;/a&gt;/&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://openai.com/index/introducing-o3-and-o4-mini&#34;&gt;Introducing OpenAI o3 and o4-mini | OpenAI&lt;/a&gt;/&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://unlocked.microsoft.com/ai-anthology/terence-tao&#34;&gt;Embracing change and resetting expectations; Microsoft Unlocked&lt;/a&gt;/&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://epoch.ai/frontiermath/expert-perspectives&#34;&gt;AI and the future of Math: Interviews with Top Mathematicians | Epoch AI&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>&#34;Just switch it off&#34;</title>
      <link>http://localhost:1313/just-switch-it-off/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/just-switch-it-off/</guid>
      <description>&lt;p&gt;If we develop a rogue AI, couldn&amp;rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &amp;ldquo;switching off&amp;rdquo; would mean deleting a model&amp;rsquo;s weights, so it can&amp;rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?&lt;/p&gt;
&lt;p&gt;First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&amp;rsquo;t &amp;ldquo;want&amp;rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&amp;rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &amp;ldquo;alignment faking&amp;rdquo;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Second, and perhaps more worryingly, is the question of whether we as a society want to press &amp;ldquo;delete&amp;rdquo;. Turning off sandboxed AI - AI developed in a secure training environment - isn&amp;rsquo;t a big deal. The negative consequences, if any, are limited to the few people who can access the model. But a leading AI company has strong incentives against withdrawing potentially unsafe models from the market. Doing this would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&amp;rsquo;s the geopolitical aspect. As highlighted in the AI 2027 report&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the fear of falling behind in the AI arms race might lead us to deploy even misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, despite poor performance on safety benchmarks.&lt;/p&gt;
&lt;p&gt;Switching off an AI isn&amp;rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&amp;rsquo;s likely to become more difficult with more capable models. Then there&amp;rsquo;s the human factor. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;Alignment Faking in Large Language Models&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ai-2027.com/&#34;&gt;AI 2027&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>From a YouTube alumni</title>
      <link>http://localhost:1313/from-a-youtube-alumni/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/from-a-youtube-alumni/</guid>
      <description>&lt;p&gt;Random people on the Internet have played a huge role in my education. I&amp;rsquo;m not just referring to my coursework at university, but also to &amp;ldquo;Bildung&amp;rdquo; more generally. I&amp;rsquo;ve learned a ton by browsing StackOverflow threads and reading Medium articles. However, I&amp;rsquo;ve probably learned the most from watching YouTube.&lt;/p&gt;
&lt;p&gt;Above all, there&amp;rsquo;s some really high-quality content out there. Nowadays there are full-time YouTubers, working on creating professional, meticulously edited videos. And some channels, such as &lt;a href=&#34;https://www.youtube.com/channel/UCsXVk37bltHxD1rDPwtNM8Q&#34;&gt;Kurzgesagt&lt;/a&gt;, are even ran by entire teams of illustrators and script-writers. Moreover, because anyone can record themselves and upload it to YouTube, we have world-class experts sharing their knowledge in YouTube lectures&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This means there are some truly remarkable YouTube videos. For example, here are comments from some &lt;a href=&#34;https://www.youtube.com/@3blue1brown&#34;&gt;3Blue1Brown&lt;/a&gt; videos:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I dropped out in 10th grade 25 years ago and your videos have inspired me to go back to school.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You sir truly deserve an honorary doctorate - just for this video. Your impact to generations of confused engineering and math students will forever ripple through our society.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I have a master&amp;rsquo;s degree in mechanical engineering and I&amp;rsquo;m starting to think I should redo my whole education from ground up searching for this kind of intuitive knowledge. It&amp;rsquo;s absurd that I find out explanations which are as intuitive as this one so late in my life. I&amp;rsquo;m blown away completely! I mean how many bits of information have we stumbled upon during our formal education failing to see how they elegantly relate to each other and form a bigger picture&amp;hellip;oh my!&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, videos tend to be more attention-grabbing than articles. Although most people are unable to read while cooking or brushing their teeth, they can watch videos. So getting started learning has never required less willpower: just search &amp;ldquo;Introduction to&amp;hellip;&amp;rdquo; on YouTube.&lt;/p&gt;
&lt;p&gt;Of course, YouTube isn&amp;rsquo;t designed to be a learning platform. But there ways of optimising for a better learning experience.&lt;/p&gt;
&lt;p&gt;The first step is to recognise that YouTube tries maximising user retention. This is a feature, not a bug. It means we can design our YouTube interface such that we end up binge watching informative videos about topics we care about. Here are some ways of achieving this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Train your algorithm&lt;/strong&gt;: In your YouTube feed, take 2 seconds to press &amp;ldquo;Not interested&amp;rdquo; whenever something irrelevant pops up. It pays off - I find the YouTube algorithm to be surprisingly sensitive to my feedback.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keep separate accounts&lt;/strong&gt;: On a similar note, I have two Google accounts: my main account, and my secondary account. I&amp;rsquo;ll use my main account for watching &amp;ldquo;useful&amp;rdquo; content, while I&amp;rsquo;ll log onto my secondary account for, well, everything else.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Block channels&lt;/strong&gt;: One can use &lt;a href=&#34;https://getcoldturkey.com/support/how-to/allow-youtube-channel/&#34;&gt;ColdTurkey&lt;/a&gt; to block certain YouTube channels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second step is to recognise the limits of just watching videos. There&amp;rsquo;s a reason we don&amp;rsquo;t abandon more traditional media altogether. When reading a book, I find it much easier recognising when I&amp;rsquo;m confused. However, after finishing a video, I sometimes find myself completely lost and unable to tell where I stopped following. Moreover, whenever I have a physical textbook, I&amp;rsquo;ll often refer back to chapters I&amp;rsquo;ve finished, just to refresh my memory. Here are two partial fixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Media notes plugin&lt;/strong&gt;: The &lt;a href=&#34;obsidian://show-plugin?id=media-notes&#34;&gt;Media Notes&lt;/a&gt; plugin for Obsidian is a real game-changer. It allows you to watch YouTube videos from inside Obsidian and take notes with timestamps. When doing this, I seem to engage more with the material. Taking physical notes while watching YouTube is a bit overkill, so this seems like a good compromise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rewatch your favourite videos&lt;/strong&gt;: It&amp;rsquo;s easy ending up only consuming new content, just because the YouTube landing page is filled with new videos. But it&amp;rsquo;s worth saving your favourite videos to playlists and rewatching them later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I discussed above, there are some hacks for a better learning experience. However, YouTube could also design their platform differently. They could e.g. develop distraction-free mode, enabling the user to remove shorts, ads or sponsored content. From a technical perspective, it&amp;rsquo;s doable. But we could also go beyond ordinary videos. For example, Andy Matuschak and Michael Nielsen have explored ways of incorporating an element of spaced repetition in videos, making for a more interactive learning experience&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. In general, I&amp;rsquo;m excited about integrating modern technology with education.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;See e.g. &lt;a href=&#34;https://www.youtube.com/@AndrejKarpathy&#34;&gt;Andrej Karpathy&amp;rsquo;s channel&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;My friend Åke Lindblom first told me about this. He apparently has an insanely good algorithm.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://numinous.productions/ttft/#mnemonic-video&#34;&gt;How can we develop transformative tools for thought?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Look for opinions</title>
      <link>http://localhost:1313/look-for-opinions/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/look-for-opinions/</guid>
      <description>&lt;p&gt;Opinionated people can be really annoying. Wherever they go, they try convincing you of their ideas. If you have an opinionated uncle, the Christmas dinner might be ruined by a bitter argument. I&amp;rsquo;ve certainly had bad experiences with a dinner-table conversations turning into feuds. For this reason, I used to try having fewer opinions. I somehow assumed this meant being more liberal and open-minded. Well, no.&lt;/p&gt;
&lt;p&gt;Actually, there are plenty of benefits of actively trying to form more opinions. Even about topics you don&amp;rsquo;t know particularly well. If you learn with a view towards arguing, then you&amp;rsquo;ll pay closer attention to the material. I think this has to do with anchoring. If you pick a stance, even at random, you&amp;rsquo;ll be more emotionally invested. Holden Karnofsky summarised it neatly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;By doing this [trying to have a hypothesis and rearticulating it whenever it changes], I try to &lt;strong&gt;continually focus my reading on the goal of forming a bottom-line view, rather than just “gathering information.”&lt;/strong&gt; I think this makes my investigations more focused and directed, and the results easier to retain. I consider this approach to be &lt;strong&gt;probably the single biggest difference-maker between &amp;ldquo;reading a ton about lots of things, but retaining little&amp;rdquo; and &amp;ldquo;efficiently developing a set of views on key topics and retaining the reasoning behind them.&amp;rdquo;&lt;/strong&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moreover, chatting with people with strong opinions can also be fun. Say you&amp;rsquo;re at a cocktail party. Small talk can be quite tiring, at least after a couple of hours. In this situation, I&amp;rsquo;ll gladly talk to people trying to persuade me of their ideas. Or say you&amp;rsquo;re hosting friends for dinner, and a friend explains her take on a topic you all like.&lt;/p&gt;
&lt;p&gt;Back in school, we were encouraged to form more opinions. Teachers made us write argumentative essays about topics we hardly knew anything about. They know that most 14 year-olds don&amp;rsquo;t care the slightest about whether fathers should be given two additional weeks of paternity leave, or if the capital income tax should be raised by 1%. But I don&amp;rsquo;t think it was only meant as an exercise in communicating effectively. It felt as if teachers were saying &amp;ldquo;Go out there in the big wild world, and look for opinions!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;So why doesn&amp;rsquo;t everyone have, like, a lot opinions? Here are the reasons that stand out to me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fear of being wrong&lt;/strong&gt;: I think many people, whether they recognise it or not, resort to some kind of agnosticism for fear of being wrong. However, recognising you&amp;rsquo;re wrong just means you&amp;rsquo;re updating your beliefs. It&amp;rsquo;s not that big of a deal. So stick your neck out.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of confidence&lt;/strong&gt;: Many people think they aren&amp;rsquo;t entitled to hold an opinion since they aren&amp;rsquo;t &amp;ldquo;qualified&amp;rdquo;. This is true for areas in which there&amp;rsquo;s a clear distinction between experts and non-experts. But I have a hunch that we sometimes use this as an excuse for not looking into an issue. I&amp;rsquo;ve certainly been guilty of doing this. Anyway, I think there&amp;rsquo;s a middle ground here: if you don&amp;rsquo;t know all the technical details, just adjust your confidence levels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Uncertainty&lt;/strong&gt;: Forming opinions about some topics might be really, really hard. For example, predicting technological progress is notoriously difficult. However, uncertainty isn&amp;rsquo;t a good reason to not make up one&amp;rsquo;s mind, especially if you need to take action.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As long as one has some &amp;ldquo;epistemic etiquette&amp;rdquo; - being prepared to change your beliefs in the light of new evidence, and not taking everything so personally - having more opinions seems like a good thing. I&amp;rsquo;m currently trying to build the habit of always having a working hypothesis whenever I learn something new. Having more opinions makes you feel more like part of the world, rather than as a bystander.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cold-takes.com/learning-by-writing&#34;&gt;Learning By Writing&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>How AI might go wrong</title>
      <link>http://localhost:1313/how-ai-might-go-wrong/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/how-ai-might-go-wrong/</guid>
      <description>&lt;p&gt;As with any new technology, advanced AI entails certain risks. While we&amp;rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.&lt;/p&gt;
&lt;p&gt;The other week, I had an insightful discussion with people in &lt;a href=&#34;https://www.zurich-ai-alignment.com&#34;&gt;Zürich AI Alignment (ZAIA)&lt;/a&gt; about the risks from AI. Afterwards, I began writing down my thoughts, and they somehow ballooned into this essay. Here are what I currently consider to be the most relevant risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Powerful technology in bad hands&lt;/strong&gt;: First, there are the risks which all fall under the label &amp;ldquo;bad guys using powerful technology to further their own interests&amp;rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. I&amp;rsquo;d guess most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concentration of power&lt;/strong&gt;: AI is a transformative technology - a message that shouldn&amp;rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&amp;rsquo;s likely to have a profound impact on things like our economy and healthcare system&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. So the main players, like OpenAI, DeepMind and Anthropic, will be able to shape the trajectory of our society in many ways. And this is kind of problematic. While the engineers at these companies tend to be lovely people, they aren&amp;rsquo;t democratically elected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misaligned AI models&lt;/strong&gt;: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&amp;rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&amp;rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&amp;rsquo;ve already seen plenty of examples of this. For example, as pointed out during the ZAIA discussion, it&amp;rsquo;s still quite easy jailbreaking LLMs like ChatGPT&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Moreover, there are striking examples of algorithms encoding our own biases&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power-seeking AI&lt;/strong&gt;: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&amp;rsquo;ll briefly outline what I understand to be the core argument.
&lt;ol&gt;
&lt;li&gt;In the future, we might see the emergence of power-seeking AI models. AI models are trained to minimise some loss function, and power could be instrumental in achieving this.&lt;/li&gt;
&lt;li&gt;At the same time, we might develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Such AI agents could replace a greater portion of the workforce, and would therefore be very profitable.&lt;/li&gt;
&lt;li&gt;All in all, it&amp;rsquo;s possible we&amp;rsquo;ll develop power-seeking, agentic AI models. The key point now is that such an AI might view humans as obstacles to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly:  &amp;ldquo;You can&amp;rsquo;t fetch the coffee if you&amp;rsquo;re dead&amp;rdquo;. So, the argument goes, the AI might try to disempower humanity.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;While points (1) and (2) seem quite reasonable, I&amp;rsquo;m more sceptical about point (3). A more conservative version of point (3) would be that power-seeking agentic AI models can do a lot of harm, despite not being mistrustful of all humans. I think the key word here is &amp;ldquo;agency&amp;rdquo;. History is full of examples of goal-oriented men with strong persuasion skills who made a great deal of harm, without necessarily wanting to exterminate all of humanity. These people knew how to pursue their goals and were good at strategic thinking. These people were doers, or, if you will, very &amp;ldquo;agentic&amp;rdquo;. Endowed with AI-like capabilities, these men would probably have caused much greater damage. In light of this, I&amp;rsquo;d lower-bound the risk of existential threat from AI by 1%. It seems like the stakes are too high for us to dismiss this kind of threat.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, there&amp;rsquo;s much more to be said about each topic. But broadly speaking, I think most risks fall into either one of the above categories. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate, or at least mitigate, some of these risks. See e.g. &lt;a href=&#34;https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary&#34;&gt;this&lt;/a&gt; article for a more exhaustive survey of the risks from AI, along with suggested measures. I&amp;rsquo;ve also left links to further resources in the footnotes.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.weforum.org/stories/2025/03/ai-transforming-global-health/&#34;&gt;6 ways AI is transforming healthcare | World Economic Forum&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zn2ukSnDqSg&#34;&gt;ChatGPT Jailbreak - Computerphile - YouTube&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples&#34;&gt;AI Bias Examples | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai&#34;&gt;Agentic AI vs. Generative AI | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Thought experiments for non-philosophers</title>
      <link>http://localhost:1313/thought-experiments-for-non-philosophers/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/thought-experiments-for-non-philosophers/</guid>
      <description>&lt;p&gt;Philosophers love thought experiments. Thought experiments are hypothetical scenarios meant to tease out our intuitions about an argument or theory. For example, here&amp;rsquo;s a classic thought experiment, due to Robert Nozick:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose there were an experience machine that would give you any experience you desired. Superduper neuropsychologists could stimulate your brain so that you would think and feel you were writing a great novel, or making a friend, or reading an interesting book. All the time you would be floating in a tank, with electrodes attached to your brain. Should you plug into this machine for life, preprogramming your life&amp;rsquo;s experiences? &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Many thought experiments seem kind of cooked up, so it&amp;rsquo;s easy to believe thought experiments have no practical use. However, one of my key takeaways from &lt;em&gt;The Scout Mindset&lt;/em&gt; by Julia Galef was that thought experiments aren&amp;rsquo;t just diversion for people with too much spare time. In fact, she argues, certain thought experiments can help us think more clearly about decisions we face in everyday life&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. In this essay, I&amp;rsquo;ll go over three interesting thought experiments from her book, and then describe two personal faves.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Outsider Test&lt;/strong&gt;: This all comes down to having an outsider&amp;rsquo;s perspective, as if one were calling a friend. How would an outsider evaluate the situation? For example, say John Doe has been given two job offers: one at company X, the other at company Y. Company X will look better on his CV, but he&amp;rsquo;s unlikely to enjoy the day-to-day tasks. In contrast, the job at company Y, although not quite as prestigious, seems more fun. Here, an outsider may say something like &amp;ldquo;If prestige weren&amp;rsquo;t a consideration, which option would you pick?&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Conformity Test&lt;/strong&gt;: This is a good one. We&amp;rsquo;re often quick in adopting the beliefs of people we respect. And this is usually a good thing: life is to short to overthink everything, and we&amp;rsquo;ve got to form opinions somehow. (For fans of &lt;em&gt;Thinking, Fast and Slow&lt;/em&gt;, this is System 1 in action.) However, when it comes to more delicate subjects, this mental short-cut might fail. In the Conformity Test, Julia Galef asks you to imagine that people no longer hold your view. (To all contrarians out there, just imagine that the people in your community suddenly become just as everyone else.) There&amp;rsquo;s a particularly interesting spin-off here: what if one of the main proponents of your view, perhaps the person who helped shape your beliefs about the subject, would change their mind? I think the EA/rationalist community provides a good use case. What if Will MacAskill would say he was completely mistaken about longtermism, rejecting the idea altogether? Or if Eliezer Yudowsky would declare that AI after all isn&amp;rsquo;t that big of a threat?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Status Quo Bias Test&lt;/strong&gt;: The underlying idea here is that humans have a bias towards the status quo. If you were to start from scratch, would you actively choose your current situation? For example, imagine a medical student who realises halfway through second year of med school that medicine isn&amp;rsquo;t for her. Although she cannot imagine herself as a doctor, she&amp;rsquo;s still hesitant to switch subjects. Here the Status Quo Bias Test might come in handy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading &lt;em&gt;The Scout Mindset&lt;/em&gt;, I soon realised that some of the advice I&amp;rsquo;ve received over the years can be rephrased as thought experiments. Here are two such thought experiments which I&amp;rsquo;ve found particularly useful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Worst Case Scenario&lt;/strong&gt;: This just involves asking oneself about the worst-case scenario. Is it really that bad? If yes, well, then you know. At least your fear isn&amp;rsquo;t  some sort of vague, scary illusion. And if not, good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast forward a decade&lt;/strong&gt;: In ten years from now, which decision is one more likely to regret? Humans are famously bad at long-term planning. We&amp;rsquo;ll often fail to take the route of action with benefits in a distant future. (Hence climate change and the fact that most adults don&amp;rsquo;t get enough sleep.) So it might be a good idea doing some kind of Outsider test, where the outsider is one&amp;rsquo;s future self.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the above thought experiments might sound familiar. Perhaps you&amp;rsquo;ve already used some of them yourself. After all, these seem like obvious tricks for seeing this for what they are. But perhaps it&amp;rsquo;s helpful having names for these tricks. It&amp;rsquo;s a bit like building a toolkit for better decision-making. I&amp;rsquo;ve applied something like Worst Case Scenario a bunch of times, but only after spending a couple of days of dwelling on the issue in a very unproductive way. Thinking in terms of thought experiments would have spared me a lot of headache.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Nozick, Robert, and Thomas Nagel. &lt;em&gt;Anarchy, state, and utopia&lt;/em&gt;. Vol. 5038. New York: Basic books, 1974.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Galef, Julia. &lt;em&gt;The scout mindset: Why some people see things clearly and others don&amp;rsquo;t&lt;/em&gt;. Penguin, 2021.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Tracking down dependencies</title>
      <link>http://localhost:1313/tracking-down-dependencies/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/tracking-down-dependencies/</guid>
      <description>&lt;p&gt;One of the benefits of attending lectures is that lecturers tend to give some really good unsolicited advice every now and then. Last semester, a professor of ours digressed to talk about the importance of identifying the key ingredients of a result. He&amp;rsquo;d just concluded a rather long proof, and was about to clean the blackboard as he said that the result was elementary. See, you only need this one lemma from point-set topology (admittedly a bit niche, but easy to prove), and then the definition of the Fourier transform. Put that way, sure, maybe it&amp;rsquo;s elementary. Explicitly writing down the dependencies of an idea, he said, was a good exercise.&lt;/p&gt;
&lt;p&gt;I tried doing this a couple of times for definitions, theorem statements and proofs. At first, it felt a bit silly. Once I finished writing down the main components of a result, it seemed trivial. For bite-sized lemmas and propositions, it felt like a waste of time. But for more complicated theorems or involved definitions, it proved quite useful.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s do a concrete example. Suppose we&amp;rsquo;re trying to understand martingales. Martingales can be thought of as sequences of random variables representing fair games: if we&amp;rsquo;re given the value of the $n$:th random variable, we expect the value of the $(n+1)$:th random variable to stay the same. In other words, there&amp;rsquo;s no predictable up- or downward trend. To be precise:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Def.&lt;/strong&gt; Let $(X_n)_{n \in \mathbb{Z}_+}$ be an adapted, real-valued random process, such that $E(|X_n|) &amp;lt; \infty$ for every $n \in \mathbb{Z}_+$. We say that the process $(X_n)_{n \in \mathbb{Z}_+}$ is a martingale if, for every $n \in \mathbb{Z}_+$, $$E(X_{n+1}|\mathcal{F}_n) = X_n.$$&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;We can recursively work out the dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General terminology related to random processes
&lt;ul&gt;
&lt;li&gt;Adapted processes&lt;/li&gt;
&lt;li&gt;Filtrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conditional expectation
&lt;ul&gt;
&lt;li&gt;The definition of the conditional expectation with respect to a $\sigma$-algebra&lt;/li&gt;
&lt;li&gt;What $E(X_{n+1}|\mathcal{F}_n)$ means, intuitively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that&amp;rsquo;s it. Once one has a solid grasp of the above notions, one is well on one&amp;rsquo;s way to understanding martingales.&lt;/p&gt;
&lt;p&gt;Here are the main benefits I&amp;rsquo;ve identified with this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;It makes you engage more with the material.&lt;/strong&gt; This is pretty self-explanatory. Learning a concept isn&amp;rsquo;t just a matter of being able to regurgitate the contents of the lecture notes; it requires you to build some sort of inner mental model. The purpose of exercises, quizzes or review questions is to make you think more carefully about a given topic. Otherwise, because humans (or at least mathematicians) are lazy, chances are you&amp;rsquo;ll go through the material too quickly. Tracking down dependencies is a bit like doing more problems, in that it prompts you to revisit the material.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It&amp;rsquo;s easier memorising a short list of bullet points rather than big chunks of information.&lt;/strong&gt; This mostly applies when deconstructing proofs. For the most part, you do get by memorising just the dependencies. You can usually fill in the details yourself.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Once you&amp;rsquo;ve nailed down the dependencies, the concept seems much simpler.&lt;/strong&gt; Some results can seem quite daunting at first. But working out the dependencies makes the concept seem deceptively simple, thereby making the learning process much more enjoyable. (So, if you want the curse of knowledge, you know what to do.) Moreover, if you struggle to understand an idea but you&amp;rsquo;re clear about the dependencies, you know what to do: read up on each of the topics involved. In this way, the dependencies translate into a checklist for your learning process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, tracking down dependencies for each concept one encounters is too time-consuming. But in some cases, it&amp;rsquo;s definitely worthwhile.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Le Gall, J. F. (2022). &lt;em&gt;Measure theory, probability, and stochastic processes&lt;/em&gt;. Cham: Springer.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>German learning resources</title>
      <link>http://localhost:1313/german-learning-resources/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/german-learning-resources/</guid>
      <description>&lt;p&gt;Here are some of my favourite German learning resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learngerman.dw.com/de/nicos-weg/c-36519718&#34;&gt;&lt;em&gt;Nicos Weg&lt;/em&gt;&lt;/a&gt;: An engaging, comprehensive German course, all for free.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;NE:s lilla tyska ordbok&lt;/em&gt;: Because everyone needs a good physical dictionary.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Den tyska grammatiken&lt;/em&gt;: Very very helpful, in that it contrasts Swedish grammar with German grammar.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Unfassbar&lt;/em&gt;: A documantary-style podcast. Very bingeable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wild Wild web&lt;/em&gt;: Stories about, well, the Internet. A mixture of true crime, personal stories and trivia.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Einschlafen mit Wikipedia&lt;/em&gt;: The low (sleep inducing) speaking pace is perfect for language learners.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Easy German&lt;/em&gt;: Excellent for A1-B1 learners.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    
    
    
    
    
  </channel>
</rss>
