<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Languages on Isabel Dahlgren</title>
    <link>http://localhost:1313/tags/languages/</link>
    <description>Recent content in Languages on Isabel Dahlgren</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 16 Feb 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/languages/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>The main risks from advanced AI</title>
      <link>http://localhost:1313/the-main-risks-from-advanced-ai/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/the-main-risks-from-advanced-ai/</guid>
      <description>&lt;p&gt;As with any new technology, advanced AI entails certain risks. While we&amp;rsquo;re investing crazy amounts of money in developing more capable AI models, only a fraction gets channeled towards AI safety research.&lt;/p&gt;
&lt;p&gt;The other week, I had an insightful discussion with people in &lt;a href=&#34;https://www.zurich-ai-alignment.com&#34;&gt;ZÃ¼rich AI Alignment (ZAIA)&lt;/a&gt; about the risks from AI. Afterwards, I began jotting down my thoughts, and they somehow ballooned into this blog post. Here are what I consider to be the most prominent risks, as of now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Powerful technology in bad hands&lt;/strong&gt;: First, there are the risks which all fall under the label &amp;ldquo;bad guys using powerful technology to further their own interests&amp;rdquo;. For example, AI can be used for mass surveillance technology, cyber warfare or in autonomous lethal weapons. Presumably, most people are uncomfortable with China having nukes. Similarly, China developing cutting-edge AI is a cause for concern.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concentration of power&lt;/strong&gt;: AI is a transformative technology - a message that shouldn&amp;rsquo;t have escaped anyone, given the current AI hype. In the years to come, it&amp;rsquo;s likely to have a profound impact on our economy&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and healthcare systems&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. As a result, the main players, OpenAI, DeepMind and Anthropic, are going to have an enormous influence on our lives. Unless AI policy guidelines are put in place very soon, the engineers at these companies are going to have an outsized impact on society. And this is somewhat problematic. While these engineers tend to be lovely people, they aren&amp;rsquo;t democratically elected. Moreover, they probably aren&amp;rsquo;t in the best position to decide what&amp;rsquo;s best for society at large.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misaligned AI models&lt;/strong&gt;: But we could also end up building AI models doing more harm than good, i.e. AI models whose incentives aren&amp;rsquo;t aligned with our values. Our understanding of the inner workings of certain AI models is very limited. While aerospace engineers know exactly what goes on inside, say, a combustion engine, the exact details of how neural networks learn remain fuzzy. If the aerospace engineers only have a vague idea of how the cooling system works, how confident can they be that the combustion engine will work as intended? In the process of developing more capable AI, we&amp;rsquo;ll probably engineer some AI models that are thrash; useless, at best, harmful at worst. We&amp;rsquo;ve already seen plenty of examples of this. For example, pointed out during the ZAIA discussion, it&amp;rsquo;s still quite easy jailbreaking LLMs like ChatGPT&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Moreover, there are plenty of instances of algorithms encoding our own biases, having been fed biased data during training&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power-seeking AI&lt;/strong&gt;: Some people push this argument further, arguing that we might see very, very misaligned AI models - AI models posing an existential threat to humanity. I&amp;rsquo;ll briefly outline what I understand to be the core argument.
&lt;ol&gt;
&lt;li&gt;In the future, we might see the emergence of power-seeking AI models. AI models are trained to minimise some loss function, and power could be instrumental in achieving this.&lt;/li&gt;
&lt;li&gt;At the same time, we might develop more agentic AI, that is, AI capable of pursuing independent goals and with more advanced planning capabilities&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. Such AI agents could replace a greater portion of the workforce, and would therefore be immensely profitable. The economic incentives are there, so it&amp;rsquo;s quite plausible that we&amp;rsquo;ll see more agentic AI models in the future.&lt;/li&gt;
&lt;li&gt;All in all, it&amp;rsquo;s possible we&amp;rsquo;ll develop power-seeking, agentic AI models. The key point now is that such an AI might view humans as obstacles to pursuing its goals. Humans can modify the learning algorithm, or try switching off the AI - things which would prevent the AI from minimising the loss function. Stuart Russell put it neatly:  &amp;ldquo;You can&amp;rsquo;t fetch the coffee if you&amp;rsquo;re dead&amp;rdquo;. So, the argument goes, the AI might try to disempower humanity.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;While points (1) and (2) seem quite reasonable, I&amp;rsquo;m more sceptical about point (3). A more conservative version of point (3) would be that power-seeking agentic AI models can do a lot of harm, despite not being misanthropic. I think the key word here is &amp;ldquo;agency&amp;rdquo;. History is full of examples of stubborn men with a bit of business savvy who made a great deal of harm, without necessarily wanting to exterminate all of humanity. Endowed with AI-like capabilities, these people would probably have caused much greater damage. At any rate, I would lower-bound the risk of things going badly by 1%. I think the stakes are too high for us to dismiss this kind of threat.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, there&amp;rsquo;s much more to be said about each topic. Each bullet could easily be broken into a bunch of sub-bullets. But broadly speaking, I think most risks fall into either one of the above categories. Finally, if this essay seems a bit doom-laden, remember that there are plenty of things we can do to eliminate, or at least mitigate, some of these risks. See e.g. &lt;a href=&#34;https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/risks-from-ai-overview-summary&#34;&gt;this&lt;/a&gt; article for a more exhaustive survey of the risks from AI, along with suggested measures. I&amp;rsquo;ve also left links for further resources in the footnotes.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pewresearch.org/short-reads/2024/03/26/americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information&#34;&gt;Americans increasingly using ChatGPT, but few trust its 2024 election information | Pew Research Center&lt;/a&gt;, &lt;a href=&#34;https://www.pewresearch.org/short-reads/2025/01/15/about-a-quarter-of-us-teens-have-used-chatgpt-for-schoolwork-double-the-share-in-2023&#34;&gt;Share of teens using ChatGPT for schoolwork doubled from 2023 to 2024 | Pew Research Center&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.weforum.org/stories/2025/03/ai-transforming-global-health/&#34;&gt;# 6 ways AI is transforming healthcare&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?list=PLj62-wQeg_DhmYphxg70DhPEJcjfmnVEt&amp;amp;v=UG_X_7g63rY&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.media.mit.edu%2F&amp;amp;source_ve_path=MjM4NTE&#34;&gt;How I&#39;m fighting bias in algorithms | Joy Buolamwini - YouTube&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples&#34;&gt;AI Bias Examples | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai&#34;&gt;Agentic AI vs. Generative AI | IBM&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Tracking down dependencies</title>
      <link>http://localhost:1313/tracking-down-dependencies/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/tracking-down-dependencies/</guid>
      <description>&lt;p&gt;One of the benefits of attending lectures is that lecturers tend to give some really good unsolicited advice every now and then. Last semester, a professor of ours digressed to talk about the importance of identifying the key ingredients of a result. He&amp;rsquo;d just concluded a rather long proof, and was about to clean the blackboard as he said that the result was elementary. See, you only need this one lemma from point-set topology (admittedly a bit niche, but easy to prove), and then the definition of the Fourier transform. Put that way, sure, maybe it&amp;rsquo;s elementary. Explicitly writing down the dependencies of an idea, he said, was a good exercise.&lt;/p&gt;
&lt;p&gt;I tried doing this a couple of times for definitions, theorem statements and proofs. At first, it felt a bit silly. Once I finished writing down the main components of a result, it seemed trivial. For bite-sized lemmas and propositions, it felt like a waste of time. But for more complicated theorems or involved definitions, it proved quite useful.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s do a concrete example. Suppose we&amp;rsquo;re trying to understand martingales. Martingales can be thought of as sequences of random variables representing fair games: if we&amp;rsquo;re given the value of the $n$:th random variable, we expect the value of the $(n+1)$:th random variable to stay the same. In other words, there&amp;rsquo;s no predictable up- or downward trend. To be precise:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Def.&lt;/strong&gt; Let $(X_n)_{n \in \mathbb{Z}_+}$ be an adapted, real-valued random process, such that $E(|X_n|) &amp;lt; \infty$ for every $n \in \mathbb{Z}_+$. We say that the process $(X_n)_{n \in \mathbb{Z}_+}$ is a martingale if, for every $n \in \mathbb{Z}_+$, $$E(X_{n+1}|\mathcal{F}_n) = X_n.$$&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;We can recursively work out the dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General terminology related to random processes
&lt;ul&gt;
&lt;li&gt;Adapted processes&lt;/li&gt;
&lt;li&gt;Filtrations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conditional expectation
&lt;ul&gt;
&lt;li&gt;The definition of the conditional expectation with respect to a $\sigma$-algebra&lt;/li&gt;
&lt;li&gt;What $E(X_{n+1}|\mathcal{F}_n)$ means, intuitively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that&amp;rsquo;s it. Once one has a solid grasp of the above notions, one is well on one&amp;rsquo;s way to understanding martingales.&lt;/p&gt;
&lt;p&gt;Here are the main benefits I&amp;rsquo;ve identified with this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;It makes you engage more with the material.&lt;/strong&gt; This is pretty self-explanatory. Learning a concept isn&amp;rsquo;t just a matter of being able to regurgitate the contents of the lecture notes; it requires you to build some sort of inner mental model. The purpose of exercises, quizzes or review questions is to make you think more carefully about a given topic. Otherwise, because humans (or at least mathematicians) are lazy, chances are you&amp;rsquo;ll go through the material too quickly. Tracking down dependencies is a bit like doing more problems, in that it prompts you to revisit the material.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It&amp;rsquo;s easier memorising a short list of bullet points rather than big chunks of information.&lt;/strong&gt; This mostly applies when deconstructing proofs. For the most part, you do get by memorising just the dependencies. You can usually fill in the details yourself.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Once you&amp;rsquo;ve nailed down the dependencies, the concept seems much simpler.&lt;/strong&gt; Some results can seem quite daunting at first. But working out the dependencies makes the concept seem deceptively simple, thereby making the learning process much more enjoyable. (So, if you want the curse of knowledge, you know what to do.) Moreover, if you struggle to understand an idea but you&amp;rsquo;re clear about the dependencies, you know what to do: read up on each of the topics involved. In this way, the dependencies translate into a checklist for your learning process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, tracking down dependencies for each concept one encounters is too time-consuming. But in some cases, it&amp;rsquo;s definitely worthwhile.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Le Gall, J. F. (2022). &lt;em&gt;Measure theory, probability, and stochastic processes&lt;/em&gt;. Cham: Springer.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>German learning resources</title>
      <link>http://localhost:1313/german-learning-resources/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/german-learning-resources/</guid>
      <description>&lt;p&gt;Here is a quick rundown of some of my favourite German learning resources.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learngerman.dw.com/de/nicos-weg/c-36519718&#34;&gt;Nicos Weg&lt;/a&gt;: An engaging, comprehensive German course, all for free.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;NE:s lilla tyska ordbok&lt;/em&gt;: Because everyone needs a good physical dictionary.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Den tyska grammatiken&lt;/em&gt;: Very very helpful, in that it contrasts Swedish grammar with German grammar.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Unfassbar&lt;/em&gt;: A documantary-style podcast. Very bingeable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wild Wild web&lt;/em&gt;: Stories about, well, the Internet. A mixture of true crime, personal stories and trivia.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Einschlafen mit Wikipedia&lt;/em&gt;: The low (sleep inducing) speaking pace is perfect for language learners.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Easy German&lt;/em&gt;: Excellent for A1-B1 learners.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    
    
    
    
    
  </channel>
</rss>
