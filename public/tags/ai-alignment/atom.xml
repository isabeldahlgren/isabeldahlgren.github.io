<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai-Alignment on Isabel Dahlgren</title>
    <link>https://isabeldahlgren.github.io/tags/ai-alignment/</link>
    <description>Recent content in Ai-Alignment on Isabel Dahlgren</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 21 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://isabeldahlgren.github.io/tags/ai-alignment/atom.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI safety lingo</title>
      <link>https://isabeldahlgren.github.io/ai-safety-lingo/</link>
      <pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/ai-safety-lingo/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I&amp;rsquo;ll omit the exact definitions. I&amp;rsquo;ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I&amp;rsquo;ll focus on intuitions.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents&#34;&gt;Embedded agency&lt;/a&gt;: Playing the Sims is very different from living IRL. When playing a video game, you&amp;rsquo;re not an agent embedded in the game environment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction&#34;&gt;Base optimizer vs. mesa-optimizer&lt;/a&gt;: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1&#34;&gt;Inner alignment vs. outer alignment&lt;/a&gt;: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/deceptive-alignment&#34;&gt;Deceptive alignment&lt;/a&gt;: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/treacherous-turn&#34;&gt;Treacherous turn&lt;/a&gt;: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to&amp;hellip; I don&amp;rsquo;t know, but people imagine something bad.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/corrigibility-1&#34;&gt;Corrigibility&lt;/a&gt;: Ever been in the library when someone&amp;rsquo;s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we&amp;rsquo;d speak of a non-corrigible phone: a phone that resists attempts to &amp;ldquo;correct&amp;rdquo; its behaviour and resists attempts to be shut down.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.01790&#34;&gt;Goal misgeneralization&lt;/a&gt;: A pianist might practise Bach to please her pianist friends; however, when she&amp;rsquo;s at normal parties, people just want her to play &lt;em&gt;Let it Be&lt;/em&gt;. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/instrumental-convergence&#34;&gt;Edge instantiation&lt;/a&gt;: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Goodhart%27s_law&#34;&gt;Goodhart&amp;rsquo;s law&lt;/a&gt;: When a measure becomes a target, it ceases to be a good measure&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The phenomenon of studying for the test is an example of Goodhart&amp;rsquo;s law.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc&#34;&gt;Value learning&lt;/a&gt;: This is the broad project of teaching AIs human values. AI-rearing, essentially.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qo355ALvLRI&#34;&gt;Inverse reinforcement learning (IRL)&lt;/a&gt;: If you&amp;rsquo;re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2024-11-28-reward-hacking/&#34;&gt;Reward hacking/specification gaming&lt;/a&gt;: Or, finding legal loopholes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/instrumental-convergence&#34;&gt;Instrumental convergence&lt;/a&gt;: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_model_specification&#34;&gt;Model misspecification&lt;/a&gt;: If you think the colour of Alice&amp;rsquo;s shirt determines whether she&amp;rsquo;ll win over Bob in a game of pingpong, your model is misspecified. You&amp;rsquo;re making the wrong assumptions about the data generation mechanism.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/sAJnZY8pp2W3DR4mx/breaking-down-the-training-deployment-dichotomy&#34;&gt;Training distribution vs. deployment distribution&lt;/a&gt;: Regardless of how much a soccer player practises taking penalty kicks, she&amp;rsquo;ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/distributional-shifts&#34;&gt;Distributional shift&lt;/a&gt;: The shift from training conditions to a real match is a distributional shift.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1&#34;&gt;Reward model splintering&lt;/a&gt;: A strategy can fail when you switch to a more general setting. Student insider jokes won&amp;rsquo;t work on the average person on the street.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf&#34;&gt;Constitutional AI (CAI)&lt;/a&gt;: Tell, don&amp;rsquo;t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. The idea behind constitutional AI is to give AIs a &amp;ldquo;constitution&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&#34;&gt;Human feedback (RLHF)&lt;/a&gt;: A process for producing ideal leaders. The ideal leader studies people&amp;rsquo;s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://newsletter.safe.ai/p/ai-safety-newsletter-13&#34;&gt;Proxy gaming&lt;/a&gt;: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game &amp;ndash; that of recommending addictive content &amp;ndash; has been gamed.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/w/value-drift&#34;&gt;Value drift&lt;/a&gt;: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;Thanks to Atharva Nihalani for inspiring me to write this post.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Let LLMs be LLMs</title>
      <link>https://isabeldahlgren.github.io/let-llms-be-llms/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/let-llms-be-llms/</guid>
      <description>&lt;h3 id=&#34;thought-experiment&#34;&gt;Thought experiment &lt;a href=&#34;#thought-experiment&#34; class=&#34;hash&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Imagine a very unusual kind of chatbot. In the user interface (UI), the text &amp;ldquo;This is a large language model. Its outputs are an aggregate of the text on the internet.&amp;rdquo; is displayed in bold, black letters, like the warning label on a pack of cigarettes.&lt;/p&gt;&#xA;&lt;p&gt;When the user inputs a string of text, the text &amp;ldquo;Matrix multiplications&amp;hellip;&amp;rdquo; flashes onto the screen for a split second. Next, ten blocks of text appear. The blocks aren&amp;rsquo;t rendered incrementally, as if the LLM were writing a text; all text appears at once. Beneath each block is a number between $0$ and $1$. The numbers sum to about $1$. Then the label &amp;ldquo;Highest logit: output $7$&amp;rdquo; appears on the screen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Finding research influences</title>
      <link>https://isabeldahlgren.github.io/finding-research-influences/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/finding-research-influences/</guid>
      <description>&lt;p&gt;There isn&amp;rsquo;t any expert consensus on many &lt;a href=&#34;https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/&#34;&gt;key questions related to AI safety&lt;/a&gt;. For example, estimates of when we&amp;rsquo;ll have transformative AI range from a few years to a century. There are also many wild opinions in the AI safety space. While some of these wild opinions seem justifiable, many people seem to exaggerate the risks from AI in an attempt to move policy-makers.&lt;/p&gt;&#xA;&lt;p&gt;I think there are a few researchers which seem to have an unusual degree of conceptual clarity, though. A few names that come to mind are &lt;a href=&#34;https://substack.com/@redwoodresearch?utm_source=about-page&#34;&gt;Buck Shleregis&lt;/a&gt;, &lt;a href=&#34;https://www.cold-takes.com/cold-takes-on-ai/&#34;&gt;Holden Karnofsky&lt;/a&gt; and &lt;a href=&#34;https://www.lesswrong.com/users/jan_kulveit?from=search_autocomplete&#34;&gt;Jan Kulveit&lt;/a&gt;. While I don&amp;rsquo;t endorse all their views, they seem to raise good questions. For lack of a better word, you could call them my research influences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The spectrum of views on AI safety</title>
      <link>https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/</link>
      <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/the-spectrum-of-views-on-ai-safety/</guid>
      <description>&lt;p&gt;I agree the concept of &lt;a href=&#34;https://en.wikipedia.org/wiki/P(doom)&#34;&gt;P(doom)&lt;/a&gt; is problematic. First, &amp;ldquo;doom&amp;rdquo; can mean a variety of things: human extinction, existential catastrophe or gradual disempowerment. Also, P(doom) - condition on present-day regulations or AI slowdown? Furthermore, the timeframe matters, as P(doom within the next $X$ years) increases with $X$.&lt;/p&gt;&#xA;&lt;p&gt;But perhaps we&amp;rsquo;re missing the point of the P(doom) question. If someone asks you for P(doom) at a cocktail party, it usually means they&amp;rsquo;re just interested in hearing general takes on AI safety, at least in my experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Consume less AI safety news</title>
      <link>https://isabeldahlgren.github.io/consume-less-ai-safety-news/</link>
      <pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/consume-less-ai-safety-news/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s hard staying on top of all the AI safety news. Some people, like &lt;a href=&#34;https://thezvi.wordpress.com/about/&#34;&gt;Zvi&lt;/a&gt;, have basically made this their full-time job.&lt;/p&gt;&#xA;&lt;p&gt;A common failure mode for forming views on AI safety is consuming too much information. It&amp;rsquo;s a tendency I&amp;rsquo;ve observed in myself, as well as in others in the AI safety community.&lt;/p&gt;&#xA;&lt;p&gt;I think it comes from the urge to solve the AI alignment problem quickly. It&amp;rsquo;s also an exciting time to be working in AI safety, with many rapid advancements being made. Also, since AI safety is something AI, there&amp;rsquo;s a lot of general excitement surrounding the area.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding the AI alignment problem</title>
      <link>https://isabeldahlgren.github.io/understanding-the-ai-alignment-problem/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/understanding-the-ai-alignment-problem/</guid>
      <description>&lt;p&gt;Broadly speaking, the AI alignment problem refers to the problem of ensuring AI systems do what we want them to do. I like the definition used by Anthropic &lt;a href=&#34;https://www.anthropic.com/news/core-views-on-ai-safety#:~:text=build%20safe%2C%20reliable%2C%20and%20steerable%20systems%20when%20those%20systems%20are%20starting%20to%20become%20as%20intelligent%20and%20as%20aware%20of%20their%20surroundings%20as%20their%20designers&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;“build safe, reliable, and steerable systems when those systems are starting to become as intelligent and as aware of their surroundings as their designers”&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;The general idea is pretty simple. But there are many things to unpack here. Understanding what this means in practise is hard. First, why might we end up training unsafe AI systems? Even if you think this is possible, it&amp;rsquo;s not clear what regulations might be appropriate. Here are some metaphors I found particularly useful for gaining a deeper understanding of some aspects of AI alignment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>&#34;Just switch it off&#34;</title>
      <link>https://isabeldahlgren.github.io/just-switch-it-off/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://isabeldahlgren.github.io/just-switch-it-off/</guid>
      <description>&lt;p&gt;If we develop a rogue AI, couldn&amp;rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &amp;ldquo;switching off&amp;rdquo; would mean deleting a model&amp;rsquo;s weights, so it can&amp;rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?&lt;/p&gt;&#xA;&lt;p&gt;First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&amp;rsquo;t &amp;ldquo;want&amp;rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&amp;rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;&amp;ldquo;alignment faking&amp;rdquo;&lt;/a&gt;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
