<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - https://isabeldahlgren.github.io/">
  <title>AI safety lingo | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AI safety lingo">
  <meta name="twitter:description" content="There’s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I’ll omit the exact definitions. I’ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I’ll focus on intuitions.
Embedded agency: Playing the Sims is very different from living IRL. When playing a video game, you’re not an agent embedded in the game environment. Base optimizer vs. mesa-optimizer: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal. Inner alignment vs. outer alignment: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance. Deceptive alignment: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned. Treacherous turn: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to… I don’t know, but people imagine something bad. Corrigibility: Ever been in the library when someone’s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we’d speak of a non-corrigible phone: a phone that resists attempts to “correct” its behaviour and resists attempts to be shut down. Goal misgeneralization: A pianist might practise Bach to please her pianist friends; however, when she’s at normal parties, people just want her to play Let it Be. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations. Edge instantiation: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative. Goodhart’s law: When a measure becomes a target, it ceases to be a good measure1. The phenomenon of studying for the test is an example of Goodhart’s law. Value learning: This is the broad project of teaching AIs human values. AI-rearing, essentially. Inverse reinforcement learning (IRL): If you’re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems. Reward hacking/specification gaming: Or, finding legal loopholes. Instrumental convergence: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first. Model misspecification: If you think the colour of Alice’s shirt determines whether she’ll win over Bob in a game of pingpong, your model is misspecified. You’re making the wrong assumptions about the data generation mechanism. Training distribution vs. deployment distribution: Regardless of how much a soccer player practises taking penalty kicks, she’ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions. Distributional shift: The shift from training conditions to a real match is a distributional shift. Reward model splintering: A strategy can fail when you switch to a more general setting. Student insider jokes won’t work on the average person on the street. Constitutional AI (CAI): Tell, don’t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow2. The idea behind constitutional AI is to give AIs a “constitution”. Human feedback (RLHF): A process for producing ideal leaders. The ideal leader studies people’s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles. Proxy gaming: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game – that of recommending addictive content – has been gamed. Value drift: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory. Thanks to Atharva Nihalani for inspiring me to write this post.">

<meta property="og:url" content="https://isabeldahlgren.github.io/ai-safety-lingo/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="AI safety lingo">
  <meta property="og:description" content="There’s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I’ll omit the exact definitions. I’ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I’ll focus on intuitions.
Embedded agency: Playing the Sims is very different from living IRL. When playing a video game, you’re not an agent embedded in the game environment. Base optimizer vs. mesa-optimizer: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal. Inner alignment vs. outer alignment: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance. Deceptive alignment: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned. Treacherous turn: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to… I don’t know, but people imagine something bad. Corrigibility: Ever been in the library when someone’s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we’d speak of a non-corrigible phone: a phone that resists attempts to “correct” its behaviour and resists attempts to be shut down. Goal misgeneralization: A pianist might practise Bach to please her pianist friends; however, when she’s at normal parties, people just want her to play Let it Be. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations. Edge instantiation: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative. Goodhart’s law: When a measure becomes a target, it ceases to be a good measure1. The phenomenon of studying for the test is an example of Goodhart’s law. Value learning: This is the broad project of teaching AIs human values. AI-rearing, essentially. Inverse reinforcement learning (IRL): If you’re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems. Reward hacking/specification gaming: Or, finding legal loopholes. Instrumental convergence: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first. Model misspecification: If you think the colour of Alice’s shirt determines whether she’ll win over Bob in a game of pingpong, your model is misspecified. You’re making the wrong assumptions about the data generation mechanism. Training distribution vs. deployment distribution: Regardless of how much a soccer player practises taking penalty kicks, she’ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions. Distributional shift: The shift from training conditions to a real match is a distributional shift. Reward model splintering: A strategy can fail when you switch to a more general setting. Student insider jokes won’t work on the average person on the street. Constitutional AI (CAI): Tell, don’t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow2. The idea behind constitutional AI is to give AIs a “constitution”. Human feedback (RLHF): A process for producing ideal leaders. The ideal leader studies people’s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles. Proxy gaming: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game – that of recommending addictive content – has been gamed. Value drift: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory. Thanks to Atharva Nihalani for inspiring me to write this post.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-21T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-21T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">


  <meta itemprop="name" content="AI safety lingo">
  <meta itemprop="description" content="There’s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I’ll omit the exact definitions. I’ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I’ll focus on intuitions.
Embedded agency: Playing the Sims is very different from living IRL. When playing a video game, you’re not an agent embedded in the game environment. Base optimizer vs. mesa-optimizer: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal. Inner alignment vs. outer alignment: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance. Deceptive alignment: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned. Treacherous turn: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to… I don’t know, but people imagine something bad. Corrigibility: Ever been in the library when someone’s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we’d speak of a non-corrigible phone: a phone that resists attempts to “correct” its behaviour and resists attempts to be shut down. Goal misgeneralization: A pianist might practise Bach to please her pianist friends; however, when she’s at normal parties, people just want her to play Let it Be. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations. Edge instantiation: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative. Goodhart’s law: When a measure becomes a target, it ceases to be a good measure1. The phenomenon of studying for the test is an example of Goodhart’s law. Value learning: This is the broad project of teaching AIs human values. AI-rearing, essentially. Inverse reinforcement learning (IRL): If you’re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems. Reward hacking/specification gaming: Or, finding legal loopholes. Instrumental convergence: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first. Model misspecification: If you think the colour of Alice’s shirt determines whether she’ll win over Bob in a game of pingpong, your model is misspecified. You’re making the wrong assumptions about the data generation mechanism. Training distribution vs. deployment distribution: Regardless of how much a soccer player practises taking penalty kicks, she’ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions. Distributional shift: The shift from training conditions to a real match is a distributional shift. Reward model splintering: A strategy can fail when you switch to a more general setting. Student insider jokes won’t work on the average person on the street. Constitutional AI (CAI): Tell, don’t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow2. The idea behind constitutional AI is to give AIs a “constitution”. Human feedback (RLHF): A process for producing ideal leaders. The ideal leader studies people’s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles. Proxy gaming: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game – that of recommending addictive content – has been gamed. Value drift: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory. Thanks to Atharva Nihalani for inspiring me to write this post.">
  <meta itemprop="datePublished" content="2025-09-21T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-09-21T00:00:00+00:00">
  <meta itemprop="wordCount" content="793">
  <meta itemprop="keywords" content="Ai-Alignment">
  <link rel="canonical" href="https://isabeldahlgren.github.io/ai-safety-lingo/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="https://isabeldahlgren.github.io/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "AI safety lingo",
    "headline": "AI safety lingo",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eThere\u0026rsquo;s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I\u0026rsquo;ll omit the exact definitions. I\u0026rsquo;ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I\u0026rsquo;ll focus on intuitions.\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/posts\/p7x32SEt43ZMC9r7r\/embedded-agents\u0022\u003eEmbedded agency\u003c\/a\u003e: Playing the Sims is very different from living IRL. When playing a video game, you\u0026rsquo;re not an agent embedded in the game environment.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/posts\/FkgsxrGf3QxhfLWHG\/risks-from-learned-optimization-introduction\u0022\u003eBase optimizer vs. mesa-optimizer\u003c\/a\u003e: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/posts\/poyshiMEhJsAuifKt\/outer-vs-inner-misalignment-three-framings-1\u0022\u003eInner alignment vs. outer alignment\u003c\/a\u003e: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/deceptive-alignment\u0022\u003eDeceptive alignment\u003c\/a\u003e: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/treacherous-turn\u0022\u003eTreacherous turn\u003c\/a\u003e: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to\u0026hellip; I don\u0026rsquo;t know, but people imagine something bad.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/corrigibility-1\u0022\u003eCorrigibility\u003c\/a\u003e: Ever been in the library when someone\u0026rsquo;s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we\u0026rsquo;d speak of a non-corrigible phone: a phone that resists attempts to \u0026ldquo;correct\u0026rdquo; its behaviour and resists attempts to be shut down.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/arxiv.org\/abs\/2210.01790\u0022\u003eGoal misgeneralization\u003c\/a\u003e: A pianist might practise Bach to please her pianist friends; however, when she\u0026rsquo;s at normal parties, people just want her to play \u003cem\u003eLet it Be\u003c\/em\u003e. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/instrumental-convergence\u0022\u003eEdge instantiation\u003c\/a\u003e: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Goodhart%27s_law\u0022\u003eGoodhart\u0026rsquo;s law\u003c\/a\u003e: When a measure becomes a target, it ceases to be a good measure\u003csup id=\u0022fnref:1\u0022\u003e\u003ca href=\u0022#fn:1\u0022 class=\u0022footnote-ref\u0022 role=\u0022doc-noteref\u0022\u003e1\u003c\/a\u003e\u003c\/sup\u003e. The phenomenon of studying for the test is an example of Goodhart\u0026rsquo;s law.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.alignmentforum.org\/s\/4dHMdK5TLN6xcqtyc\u0022\u003eValue learning\u003c\/a\u003e: This is the broad project of teaching AIs human values. AI-rearing, essentially.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.youtube.com\/watch?v=qo355ALvLRI\u0022\u003eInverse reinforcement learning (IRL)\u003c\/a\u003e: If you\u0026rsquo;re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/lilianweng.github.io\/posts\/2024-11-28-reward-hacking\/\u0022\u003eReward hacking\/specification gaming\u003c\/a\u003e: Or, finding legal loopholes.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/instrumental-convergence\u0022\u003eInstrumental convergence\u003c\/a\u003e: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Statistical_model_specification\u0022\u003eModel misspecification\u003c\/a\u003e: If you think the colour of Alice\u0026rsquo;s shirt determines whether she\u0026rsquo;ll win over Bob in a game of pingpong, your model is misspecified. You\u0026rsquo;re making the wrong assumptions about the data generation mechanism.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/posts\/sAJnZY8pp2W3DR4mx\/breaking-down-the-training-deployment-dichotomy\u0022\u003eTraining distribution vs. deployment distribution\u003c\/a\u003e: Regardless of how much a soccer player practises taking penalty kicks, she\u0026rsquo;ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/distributional-shifts\u0022\u003eDistributional shift\u003c\/a\u003e: The shift from training conditions to a real match is a distributional shift.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/posts\/k54rgSg7GcjtXnMHX\/model-splintering-moving-from-one-imperfect-model-to-another-1\u0022\u003eReward model splintering\u003c\/a\u003e: A strategy can fail when you switch to a more general setting. Student insider jokes won\u0026rsquo;t work on the average person on the street.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www-cdn.anthropic.com\/7512771452629584566b6303311496c262da1006\/Anthropic_ConstitutionalAI_v2.pdf\u0022\u003eConstitutional AI (CAI)\u003c\/a\u003e: Tell, don\u0026rsquo;t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow\u003csup id=\u0022fnref:2\u0022\u003e\u003ca href=\u0022#fn:2\u0022 class=\u0022footnote-ref\u0022 role=\u0022doc-noteref\u0022\u003e2\u003c\/a\u003e\u003c\/sup\u003e. The idea behind constitutional AI is to give AIs a \u0026ldquo;constitution\u0026rdquo;.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning_from_human_feedback\u0022\u003eHuman feedback (RLHF)\u003c\/a\u003e: A process for producing ideal leaders. The ideal leader studies people\u0026rsquo;s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/newsletter.safe.ai\/p\/ai-safety-newsletter-13\u0022\u003eProxy gaming\u003c\/a\u003e: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game \u0026ndash; that of recommending addictive content \u0026ndash; has been gamed.\u003c\/li\u003e\n\u003cli\u003e\u003ca href=\u0022https:\/\/www.lesswrong.com\/w\/value-drift\u0022\u003eValue drift\u003c\/a\u003e: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003e\u003cem\u003eThanks to Atharva Nihalani for inspiring me to write this post.\u003c\/em\u003e\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https:\/\/isabeldahlgren.github.io\/ai-safety-lingo\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-09-21T00:00:00.00Z",
    "datePublished": "2025-09-21T00:00:00.00Z",
    "dateModified": "2025-09-21T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "https://isabeldahlgren.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https:\/\/isabeldahlgren.github.io\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "https://isabeldahlgren.github.io/",
    "url" : "https:\/\/isabeldahlgren.github.io\/ai-safety-lingo\/",
    "wordCount" : "793",
    "genre" : [ "ai-alignment" ],
    "keywords" : [ "ai-alignment" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">AI safety lingo</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-09-21"> September 21, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>
        </p>
      

      <span itemprop="articleBody">
      <p>There&rsquo;s a lot of jargon within AI safety. Here are analogies for 20 AI safety terms. I assume some familiarity with these terms; I&rsquo;ll omit the exact definitions. I&rsquo;ll give references to appropriate resources rather than trying and failing to define these concepts precisely in a couple of lines. Instead, I&rsquo;ll focus on intuitions.</p>
<ul>
<li><a href="https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents">Embedded agency</a>: Playing the Sims is very different from living IRL. When playing a video game, you&rsquo;re not an agent embedded in the game environment.</li>
<li><a href="https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction">Base optimizer vs. mesa-optimizer</a>: A base optimizer is a process for achieving some goal (cooking a good risotto). The base optimizer (the chef) soon learns that the seasoning makes a huge difference. A process for perfecting the seasoning is an example of a mesa-optimiser: a process for achieving a learned subgoal.</li>
<li><a href="https://www.lesswrong.com/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1">Inner alignment vs. outer alignment</a>: If a government wants to reduce unemployment, it has to design efficient regulations and and ensure citizens comply. Outer alignment is the problem of specifying the right incentive structure; inner alignment the problem of compliance.</li>
<li><a href="https://www.lesswrong.com/w/deceptive-alignment">Deceptive alignment</a>: This is much like an intelligent bully will pretend being nice when the grown-ups are watching. A misaligned AI system might benefit from appearing more aligned.</li>
<li><a href="https://www.lesswrong.com/w/treacherous-turn">Treacherous turn</a>: The moment when the opposition seizes power through a coup. The hypothetical moment when a misaligned, highly capable AI decides to&hellip; I don&rsquo;t know, but people imagine something bad.</li>
<li><a href="https://www.lesswrong.com/w/corrigibility-1">Corrigibility</a>: Ever been in the library when someone&rsquo;s phone continues ringing, despite their best efforts to silence it? They might try putting the phone on silent, then turning the volume to zero, then turning it off. Maybe their phone seems to be frozen or something? In this case, we&rsquo;d speak of a non-corrigible phone: a phone that resists attempts to &ldquo;correct&rdquo; its behaviour and resists attempts to be shut down.</li>
<li><a href="https://arxiv.org/abs/2210.01790">Goal misgeneralization</a>: A pianist might practise Bach to please her pianist friends; however, when she&rsquo;s at normal parties, people just want her to play <em>Let it Be</em>. Likewise, an AI system might competently pursue one goal which leads to good performance in training situations but poorly in novel test situations.</li>
<li><a href="https://www.lesswrong.com/w/instrumental-convergence">Edge instantiation</a>: An AI agent instructed to fill a cauldron of water might flood the entire room. Task accomplished, technically. And for AI agents, only the technicalities matter. AIs can be annoyingly creative.</li>
<li><a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart&rsquo;s law</a>: When a measure becomes a target, it ceases to be a good measure<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. The phenomenon of studying for the test is an example of Goodhart&rsquo;s law.</li>
<li><a href="https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc">Value learning</a>: This is the broad project of teaching AIs human values. AI-rearing, essentially.</li>
<li><a href="https://www.youtube.com/watch?v=qo355ALvLRI">Inverse reinforcement learning (IRL)</a>: If you&rsquo;re trying to schedule a time to meet with a passive aggressive friend, you have to infer their preferences based on their wordings and emoji usage. Inverse reinforcement learning is an ML approach for inferring preferences of AI systems.</li>
<li><a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward hacking/specification gaming</a>: Or, finding legal loopholes.</li>
<li><a href="https://www.lesswrong.com/w/instrumental-convergence">Instrumental convergence</a>: Great minds think alike. In particular, great minds with different goals might pursue similar subgoals. For example, two high school students wanting to become an aerospace engineer and a medical doctor respectively might infer that they should get university degrees first.</li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_model_specification">Model misspecification</a>: If you think the colour of Alice&rsquo;s shirt determines whether she&rsquo;ll win over Bob in a game of pingpong, your model is misspecified. You&rsquo;re making the wrong assumptions about the data generation mechanism.</li>
<li><a href="https://www.lesswrong.com/posts/sAJnZY8pp2W3DR4mx/breaking-down-the-training-deployment-dichotomy">Training distribution vs. deployment distribution</a>: Regardless of how much a soccer player practises taking penalty kicks, she&rsquo;ll find it different taking a penalty kick in a real match. You cannot perfectly simulate the test conditions.</li>
<li><a href="https://www.lesswrong.com/w/distributional-shifts">Distributional shift</a>: The shift from training conditions to a real match is a distributional shift.</li>
<li><a href="https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1">Reward model splintering</a>: A strategy can fail when you switch to a more general setting. Student insider jokes won&rsquo;t work on the average person on the street.</li>
<li><a href="https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf">Constitutional AI (CAI)</a>: Tell, don&rsquo;t show. Rather than showing kids examples of good and bad behaviour, tell them which ethical principles to follow<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The idea behind constitutional AI is to give AIs a &ldquo;constitution&rdquo;.</li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Human feedback (RLHF)</a>: A process for producing ideal leaders. The ideal leader studies people&rsquo;s opinions closely, tries inferring principles explaining the data, and aspires to act according to these principles.</li>
<li><a href="https://newsletter.safe.ai/p/ai-safety-newsletter-13">Proxy gaming</a>: Social media companies take the time a user spends on their platform as a proxy for the quality of the content recommended. Thus the recommender algorithms might favour polarising content. The proxy game &ndash; that of recommending addictive content &ndash; has been gamed.</li>
<li><a href="https://www.lesswrong.com/w/value-drift">Value drift</a>: Values of individuals and communities change over time. Nowadays, almost everyone thinks slavery is indefensible. Similarly, the values implicit in an AI model might change as it accumulates more memory.</li>
</ul>
<p><em>Thanks to Atharva Nihalani for inspiring me to write this post.</em></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Yet another reason that <a href="https://isabeldahlgren.github.io/policy-making-is-complicated/">policy is complicated</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Maybe something like <a href="https://www.poetryfoundation.org/poems/46473/if---">If</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
