<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/just-switch-it-off/" />
    <title>&#34;Just switch it off&#34;</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>&#34;Just switch it off&#34;</h1>
        <div>
                <time>April 13, 2025</time>
            </div>
    </header><p>If we develop a rogue AI, couldn&rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &ldquo;switching off&rdquo; would mean deleting a model&rsquo;s weights, so it can&rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?</p>
<p>First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&rsquo;t &ldquo;want&rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &ldquo;alignment faking&rdquo;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Second, and perhaps more worryingly, is the question of whether we as a society want to press &ldquo;delete&rdquo;. Turning off sandboxed AI - AI developed in a secure training environment - isn&rsquo;t a big deal. The negative consequences, if any, are limited to the few people who can access the model. But a leading AI company has strong incentives against withdrawing potentially unsafe models from the market. Doing this would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&rsquo;s the geopolitical aspect. As highlighted in the AI 2027 report<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, the fear of falling behind in the AI arms race might lead us to deploy even misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, despite poor performance on safety benchmarks.</p>
<p>Switching off an AI isn&rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&rsquo;s likely to become more difficult with more capable models. Then there&rsquo;s the human factor. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://arxiv.org/pdf/2412.14093">Alignment Faking in Large Language Models</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://ai-2027.com/">AI 2027</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright Â© 2025 Isabel Dahlgren
</footer>
</body>
</html>
