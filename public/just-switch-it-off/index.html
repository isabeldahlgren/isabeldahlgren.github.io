<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                
                
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                
                throwOnError : false
            });
        });
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Isabel Dahlgren">
    
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/just-switch-it-off/" />
    <title>&#34;Just switch it off&#34;</title>
</head>
<body><header id="banner">
    <h2><a href="http://localhost:1313/">Isabel Dahlgren</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/library/" title="library">library</a>
            </li><li>
                <a href="/resources/" title="resources">resources</a>
            </li><li>
                <a href="/tags/" title="">Tags</a>
            </li>
        </ul>
    </nav>
</header>

<main id="content">
<article>
    <header id="post-header">
        <h1>&#34;Just switch it off&#34;</h1>
        <div>
                <time>April 13, 2025</time>
            </div>
    </header><p>If we develop a rogue AI, couldn&rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &ldquo;switching off&rdquo; would mean deleting a model&rsquo;s weights, so it can&rsquo;t be deployed. Deleting files is easy enough, so surely we should be able to switch off a misaligned AI? But there are two catches here.</p>
<p>First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&rsquo;t &ldquo;want&rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&rsquo;s misaligned, it might try appearing safe during training. This is commonly known as &ldquo;alignment faking&rdquo;. Although this sounds a bit far-fetched, this phenomenon has been observed in some models<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Second, and perhaps more worryingly, is the question of whether we as a society want to press &ldquo;delete&rdquo;. Turning off sandboxed AI, AI developed in a secure training environment, isn&rsquo;t a big deal. Researchers don&rsquo;t have strong incentives to keep using an unsafe model. But if a leading AI lab were to revoke their flagship model from the market, this would have huge implications. It would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&rsquo;s the geopolitical aspect. As highlighted in the AI 2027 report<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, the fear of falling behind in the AI arms race might lead us to deploy even potentially misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, even though they perform poorly on safety benchmarks.</p>
<p>Switching off an AI isn&rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&rsquo;s likely to become more difficult with more capable models. Then there&rsquo;s the human aspect to it. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://arxiv.org/pdf/2412.14093">Alignment Faking in Large Language Models</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://ai-2027.com/">AI 2027</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    <p>Tags:
        <a href="/tags/ai-alignment">ai-alignment</a>
    </p>
    
</article>

        </main><footer id="footer">
    Copyright Â© 2025 Isabel Dahlgren
</footer>
</body>
</html>
