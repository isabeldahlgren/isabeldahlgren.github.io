<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Ronalds Vilcins - http://localhost:1313/">
  <title>&#34;Just switch it off&#34; | Isabel Dahlgren</title>
  <meta name="description" content="Isabel Dahlgren">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="&#34;Just switch it off&#34;">
  <meta name="twitter:description" content="If we develop a rogue AI, couldn’t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here “switching off” would mean deleting a model’s weights, so it can’t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?
First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don’t “want” to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it’s misaligned, it might try appearing safe during training. This is commonly known as “alignment faking”. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.">

<meta property="og:url" content="http://localhost:1313/just-switch-it-off/">
  <meta property="og:site_name" content="Isabel Dahlgren">
  <meta property="og:title" content="&#34;Just switch it off&#34;">
  <meta property="og:description" content="If we develop a rogue AI, couldn’t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here “switching off” would mean deleting a model’s weights, so it can’t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?
First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don’t “want” to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it’s misaligned, it might try appearing safe during training. This is commonly known as “alignment faking”. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-30T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-03-30T00:00:00+00:00">
    <meta property="article:tag" content="Ai-Alignment">
    <meta property="article:tag" content="Ai">


  <meta itemprop="name" content="&#34;Just switch it off&#34;">
  <meta itemprop="description" content="If we develop a rogue AI, couldn’t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here “switching off” would mean deleting a model’s weights, so it can’t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?
First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don’t “want” to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it’s misaligned, it might try appearing safe during training. This is commonly known as “alignment faking”. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.">
  <meta itemprop="datePublished" content="2025-03-30T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-03-30T00:00:00+00:00">
  <meta itemprop="wordCount" content="369">
  <meta itemprop="keywords" content="Ai-Alignment,Ai">
  <link rel="canonical" href="http://localhost:1313/just-switch-it-off/">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
  <link rel="alternate" type="application/atom+xml" title="Isabel Dahlgren" href="http://localhost:1313/atom.xml" />
  <link rel="alternate" type="application/json" title="Isabel Dahlgren" href="http://localhost:1313/feed.json" />
  <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">

  
  <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline}header a,footer a{text-decoration:none}header ul,footer ul{display:flex;gap:15px}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              throwOnError : false
          });
      });
  </script>

  


<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "articleSection": "posts",
    "name": "\u0022Just switch it off\u0022",
    "headline": "\u0022Just switch it off\u0022",
    "alternativeHeadline": "",
    "description": "\u003cp\u003eIf we develop a rogue AI, couldn\u0026rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here \u0026ldquo;switching off\u0026rdquo; would mean deleting a model\u0026rsquo;s weights, so it can\u0026rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?\u003c\/p\u003e\n\u003cp\u003eFirst, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don\u0026rsquo;t \u0026ldquo;want\u0026rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it\u0026rsquo;s misaligned, it might try appearing safe during training. This is commonly known as \u003ca href=\u0022https:\/\/arxiv.org\/pdf\/2412.14093\u0022\u003e\u0026ldquo;alignment faking\u0026rdquo;\u003c\/a\u003e. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.\u003c\/p\u003e",
    "inLanguage": "en-US",
    "isFamilyFriendly": "true",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http:\/\/localhost:1313\/just-switch-it-off\/"
    },
    "author" : {
        "@type": "Person",
        "name": ""
    },
    "creator" : {
        "@type": "Person",
        "name": ""
    },
    "accountablePerson" : {
        "@type": "Person",
        "name": ""
    },
    "copyrightHolder" : "Isabel Dahlgren",
    "copyrightYear" : "2025",
    "dateCreated": "2025-03-30T00:00:00.00Z",
    "datePublished": "2025-03-30T00:00:00.00Z",
    "dateModified": "2025-03-30T00:00:00.00Z",
    "publisher":{
        "@type":"Organization",
        "name": "Isabel Dahlgren",
        "url": "http://localhost:1313/",
        "logo": {
            "@type": "ImageObject",
            "url": "http:\/\/localhost:1313\/",
            "width":"32",
            "height":"32"
        }
    },
    "image": "http://localhost:1313/",
    "url" : "http:\/\/localhost:1313\/just-switch-it-off\/",
    "wordCount" : "369",
    "genre" : [ "ai-alignment" , "ai" ],
    "keywords" : [ "ai-alignment" , "ai" ]
}
</script>


</head>
<body>
<main>
      <header>
    <nav>
      
  <ul>
    <li>
      
      
      
      
      <a href="/" >Isabel Dahlgren</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/about/" >About</a>
    </li>
  
    <li>
      
      
      
      
      <a href="/tags/" >Tags</a>
    </li>
  
  </ul>
    </nav>
  </header>

  <section>

      
      <h2 itemprop="name headline">&#34;Just switch it off&#34;</h2>
      <p class="meta">
        <time itemprop="datePublished" datetime="2025-03-30"> March 30, 2025</time> &bull; 
        <a href='/tags/ai-alignment'>ai-alignment</a>, <a href='/tags/ai'>ai</a>
        </p>
      

      <span itemprop="articleBody">
      <p>If we develop a rogue AI, couldn&rsquo;t we just switch it off? This is the obvious objection to the idea that AI could be dangerous. Here &ldquo;switching off&rdquo; would mean deleting a model&rsquo;s weights, so it can&rsquo;t be deployed. Deleting files is easy enough, so what might prevent us from switching off a misaligned AI?</p>
<p>First, the users need to realise that the model is dangerous. This can be challenging, especially for more advanced models. The key premise here is that AIs will try preserving themselves. That is, they don&rsquo;t &ldquo;want&rdquo; to be turned off - this would prevent them from pursuing their goals. If an AI knows that it will be turned off if it&rsquo;s misaligned, it might try appearing safe during training. This is commonly known as <a href="https://arxiv.org/pdf/2412.14093">&ldquo;alignment faking&rdquo;</a>. Although this sounds a bit far-fetched, this phenomenon has been observed in some models.</p>
<p>Second, and perhaps more worryingly, is the question of whether we as a society want to &ldquo;press delete&rdquo;. Turning off sandboxed AI - AI developed in a secure training environment - isn&rsquo;t a big deal. The negative consequences, if any, are limited to the few people who can access the model. But a leading AI company has strong incentives against withdrawing potentially unsafe models from the market. Doing this would mean less profit, bad PR and giving away market share to competitors. Besides these economical considerations, there&rsquo;s the geopolitical aspect. As highlighted in the <a href="https://ai-2027.com/">AI 2027 report</a>, the fear of falling behind in the AI arms race might lead us to deploy even misaligned AI. Even if the people behind the AI wanted to switch off their models because of safety considerations, what would the general public think? Most people would probably be reluctant to stop their favourite LLMs, despite poor performance on safety benchmarks.</p>
<p>Switching off an AI isn&rsquo;t just a matter of deleting files. It requires us to detect unsafe behaviour, a task that&rsquo;s likely to become more difficult with more capable models. Then there&rsquo;s the human factor. Asking that AI companies delete models showing signs of misalignment is asking for a lot. In the future, turning off an AI in a broader sense would require turning off parts of our society.</p>

      </span>
       

    </section>
    <footer>
	  <nav>
  <ul>
    <li>
      © 2025
    </li>
  
  </ul>
    </nav>
</footer>

  </main>
</body>
</html>
